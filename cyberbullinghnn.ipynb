{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification with Quanvolutional layer + Attention + HQLSTM"
      ],
      "metadata": {
        "id": "IOw7Xm_A-Msu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "yXYmXj3x_Z3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for computations"
      ],
      "metadata": {
        "id": "fklFBxOt_n0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install custatevec_cu12\n",
        "!pip install pennylane pennylane-lightning-gpu\n",
        "import numpy as np\n",
        "import pennylane as qml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "9KMjztfO_iLH",
        "outputId": "8e38b1e7-0a90-478a-bb57-02a1e3d2d709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: custatevec_cu12 in /usr/local/lib/python3.10/dist-packages (1.6.0.post1)\n",
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.38.0)\n",
            "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.10/dist-packages (0.38.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.15.1)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.7.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\n",
            "Requirement already satisfied: pennylane-lightning>=0.38 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For plotting data"
      ],
      "metadata": {
        "id": "pNda1wZNADCt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "0HzKYqfvkO5s",
        "outputId": "0ea785f1-1051-445f-8220-3c97769b00c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "# plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For data preprocessing and text cleaning"
      ],
      "metadata": {
        "id": "qEByIz_FAyoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install contractions\n",
        "!pip install emoji==1.4.1\n",
        "!pip install nltk\n",
        "\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from langdetect import detect, LangDetectException\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "5pFgXpHkJ1fd",
        "outputId": "844767a6-c574-45d1-aa61-fce66b9b166f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
            "Requirement already satisfied: emoji==1.4.1 in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set seed for reproductivity"
      ],
      "metadata": {
        "id": "Z-O5RGApBB0S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0Ud4hzR0kO5v"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmjJoJyZkO5v",
        "outputId": "7b4e3d4e-8dea-42de-cd51-3add9ba05a99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('cuda', <lightning.qubit device (wires=4) at 0x7cb244067eb0>)"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ],
      "source": [
        "num_qubits = 4\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dev = qml.device(\"lightning.qubit\" if device == \"cuda\" else \"default.qubit\", wires=range(num_qubits))\n",
        "device, dev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data, data preprocessing and analysis"
      ],
      "metadata": {
        "id": "wh5ts37oBzek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use \"Cyberbullying Classification\" dataset from Kaggle. You can acquire more information about the data by the following link: https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification/data"
      ],
      "metadata": {
        "id": "NZHah9RUDkV1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PN22QbhmkO5v"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('cyberbullying_tweets.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "rCNEZ-XakO5v",
        "outputId": "580251ae-df8f-4f3e-e444-89c2e9abb83a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              tweet_text cyberbullying_type\n",
              "0      In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1      Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2      @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3      @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4      @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
              "...                                                  ...                ...\n",
              "47687  Black ppl aren't expected to do anything, depe...          ethnicity\n",
              "47688  Turner did not withhold his disappointment. Tu...          ethnicity\n",
              "47689  I swear to God. This dumb nigger bitch. I have...          ethnicity\n",
              "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...          ethnicity\n",
              "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...          ethnicity\n",
              "\n",
              "[47692 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68a354c3-166f-4dd0-ac68-53634c67b2cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>cyberbullying_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47687</th>\n",
              "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47688</th>\n",
              "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47689</th>\n",
              "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47690</th>\n",
              "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47691</th>\n",
              "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
              "      <td>ethnicity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47692 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68a354c3-166f-4dd0-ac68-53634c67b2cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68a354c3-166f-4dd0-ac68-53634c67b2cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68a354c3-166f-4dd0-ac68-53634c67b2cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-340dc48b-3307-4a02-adb8-256080a236ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-340dc48b-3307-4a02-adb8-256080a236ba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-340dc48b-3307-4a02-adb8-256080a236ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 47692,\n  \"fields\": [\n    {\n      \"column\": \"tweet_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 46017,\n        \"samples\": [\n          \"@AndyEaston85 Love how we are teaching the Bullshitters a lesson in football. Miss Bully and his message board posts.\",\n          \"GYUK | Anti-feminist YouTuber doubles down on vile Jess Phillips rape joke while leaping to the defence ...: In a video uploaded Thursday (April 23), former UKIP candidate Benjamin jumped to the defence of retired gay porn actor turned men's rights\\u2026 http://dlvr.it/RVRS8h\",\n          \"@Truth_Haqq Islam declared war on all mankind 1400 years ago. Now we return the favor. http://t.co/av4B4yCQzY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cyberbullying_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"not_cyberbullying\",\n          \"gender\",\n          \"ethnicity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})"
      ],
      "metadata": {
        "id": "92KibKseK0C7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "NoSlde7eLDdp",
        "outputId": "de27136a-c730-4a06-e641-fc02b767c0b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text          sentiment\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-216d6714-2910-4d96-b6b7-bce701de495c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-216d6714-2910-4d96-b6b7-bce701de495c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-216d6714-2910-4d96-b6b7-bce701de495c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-216d6714-2910-4d96-b6b7-bce701de495c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b90f7e6-4f8e-410d-a724-6aae7dacc51d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b90f7e6-4f8e-410d-a724-6aae7dacc51d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b90f7e6-4f8e-410d-a724-6aae7dacc51d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 47692,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 46017,\n        \"samples\": [\n          \"@AndyEaston85 Love how we are teaching the Bullshitters a lesson in football. Miss Bully and his message board posts.\",\n          \"GYUK | Anti-feminist YouTuber doubles down on vile Jess Phillips rape joke while leaping to the defence ...: In a video uploaded Thursday (April 23), former UKIP candidate Benjamin jumped to the defence of retired gay porn actor turned men's rights\\u2026 http://dlvr.it/RVRS8h\",\n          \"@Truth_Haqq Islam declared war on all mankind 1400 years ago. Now we return the favor. http://t.co/av4B4yCQzY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"not_cyberbullying\",\n          \"gender\",\n          \"ethnicity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define cleaning functions. Source: https://www.kaggle.com/code/ludovicocuoghi/detecting-bullying-tweets-pytorch-lstm-bert"
      ],
      "metadata": {
        "id": "PsXc6JwwCTMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean emojis from text\n",
        "def strip_emoji(text):\n",
        "    return emoji.get_emoji_regexp().sub(\"\", text)\n",
        "\n",
        "# Remove punctuations, stopwords, links, mentions and new line characters\n",
        "def strip_all_entities(text):\n",
        "    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n",
        "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n",
        "    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n",
        "    banned_list = string.punctuation\n",
        "    table = str.maketrans('', '', banned_list)\n",
        "    text = text.translate(table)\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
        "def clean_hashtags(tweet):\n",
        "    # Remove hashtags at the end of the sentence\n",
        "    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n",
        "\n",
        "    # Remove the # symbol from hashtags in the middle of the sentence\n",
        "    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n",
        "\n",
        "    return new_tweet\n",
        "\n",
        "# Filter special characters such as & and $ present in some words\n",
        "def filter_chars(text):\n",
        "    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())\n",
        "\n",
        "# Remove multiple spaces\n",
        "def remove_mult_spaces(text):\n",
        "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
        "\n",
        "# Function to check if the text is in English, and return an empty string if it's not\n",
        "def filter_non_english(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except LangDetectException:\n",
        "        lang = \"unknown\"\n",
        "    return text if lang == \"en\" else \"\"\n",
        "\n",
        "# Expand contractions\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "# Remove numbers\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Lemmatize words\n",
        "def lemmatize(text):\n",
        "    words = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "# Remove short words\n",
        "def remove_short_words(text, min_len=2):\n",
        "    words = text.split()\n",
        "    long_words = [word for word in words if len(word) >= min_len]\n",
        "    return ' '.join(long_words)\n",
        "\n",
        "# Replace elongated words with their base form\n",
        "def replace_elongated_words(text):\n",
        "    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n",
        "    return re.sub(regex_pattern, r'\\1\\3\\4', text)\n",
        "\n",
        "# Remove repeated punctuation\n",
        "def remove_repeated_punctuation(text):\n",
        "    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
        "\n",
        "# Remove extra whitespace\n",
        "def remove_extra_whitespace(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def remove_url_shorteners(text):\n",
        "    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)\n",
        "\n",
        "# Remove spaces at the beginning and end of the tweet\n",
        "def remove_spaces_tweets(tweet):\n",
        "    return tweet.strip()\n",
        "\n",
        "# Remove short tweets\n",
        "def remove_short_tweets(tweet, min_words=3):\n",
        "    words = tweet.split()\n",
        "    return tweet if len(words) >= min_words else \"\"\n",
        "\n",
        "# Function to call all the cleaning functions in the correct order\n",
        "def clean_tweet(tweet):\n",
        "    tweet = strip_emoji(tweet)\n",
        "    tweet = expand_contractions(tweet)\n",
        "    tweet = filter_non_english(tweet)\n",
        "    tweet = strip_all_entities(tweet)\n",
        "    tweet = clean_hashtags(tweet)\n",
        "    tweet = filter_chars(tweet)\n",
        "    tweet = remove_mult_spaces(tweet)\n",
        "    tweet = remove_numbers(tweet)\n",
        "    tweet = lemmatize(tweet)\n",
        "    tweet = remove_short_words(tweet)\n",
        "    tweet = replace_elongated_words(tweet)\n",
        "    tweet = remove_repeated_punctuation(tweet)\n",
        "    tweet = remove_extra_whitespace(tweet)\n",
        "    tweet = remove_url_shorteners(tweet)\n",
        "    tweet = remove_spaces_tweets(tweet)\n",
        "    tweet = remove_short_tweets(tweet)\n",
        "    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "8RjOd2SaI8R_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_clean'] = [clean_tweet(tweet) for tweet in df['text']]"
      ],
      "metadata": {
        "id": "ECncuUbvLAHI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "RYNGL2_Uc4vP",
        "outputId": "d63b2402-6899-4918-dee6-0fe145657592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text          sentiment  \\\n",
              "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
              "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
              "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
              "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
              "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
              "\n",
              "                                          text_clean  \n",
              "0             word katandandre food crapilicious mkr  \n",
              "1  aussietv white mkr theblock imacelebrityau tod...  \n",
              "2                    classy whore red velvet cupcake  \n",
              "3  meh thanks head concerned another angry dude t...  \n",
              "4  isi account pretending kurdish account like is...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a6b891e-6c52-45bf-97db-9bf3a066dde4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In other words #katandandre, your food was cra...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>word katandandre food crapilicious mkr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>classy whore red velvet cupcake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>meh thanks head concerned another angry dude t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
              "      <td>not_cyberbullying</td>\n",
              "      <td>isi account pretending kurdish account like is...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a6b891e-6c52-45bf-97db-9bf3a066dde4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a6b891e-6c52-45bf-97db-9bf3a066dde4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a6b891e-6c52-45bf-97db-9bf3a066dde4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2700e97e-2fd4-4c44-91f5-f508a9e6cef3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2700e97e-2fd4-4c44-91f5-f508a9e6cef3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2700e97e-2fd4-4c44-91f5-f508a9e6cef3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 47692,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 46017,\n        \"samples\": [\n          \"@AndyEaston85 Love how we are teaching the Bullshitters a lesson in football. Miss Bully and his message board posts.\",\n          \"GYUK | Anti-feminist YouTuber doubles down on vile Jess Phillips rape joke while leaping to the defence ...: In a video uploaded Thursday (April 23), former UKIP candidate Benjamin jumped to the defence of retired gay porn actor turned men's rights\\u2026 http://dlvr.it/RVRS8h\",\n          \"@Truth_Haqq Islam declared war on all mankind 1400 years ago. Now we return the favor. http://t.co/av4B4yCQzY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"not_cyberbullying\",\n          \"gender\",\n          \"ethnicity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_clean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41391,\n        \"samples\": [\n          \"litto bro really dumb pretty lame told jerk hitting hand bully\",\n          \"thursday friday confirmed\",\n          \"trying hangout ugly as\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are around {int(df[\"text_clean\"].duplicated().sum())} duplicated tweets, we will remove them.')"
      ],
      "metadata": {
        "id": "Eh56r73aewhE",
        "outputId": "026d5aab-fcce-462b-df22-abb3dae64c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are around 6301 duplicated tweets, we will remove them.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(\"text_clean\", inplace=True)"
      ],
      "metadata": {
        "id": "LNlO3hdhez4-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sentiment.value_counts()"
      ],
      "metadata": {
        "id": "LGEU8XC9e3Ue",
        "outputId": "580eaa1e-5992-4f36-91d1-dc0c79690dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "religion               7912\n",
              "age                    7820\n",
              "ethnicity              7403\n",
              "gender                 7274\n",
              "not_cyberbullying      6066\n",
              "other_cyberbullying    4916\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>religion</th>\n",
              "      <td>7912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>7820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ethnicity</th>\n",
              "      <td>7403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender</th>\n",
              "      <td>7274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not_cyberbullying</th>\n",
              "      <td>6066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>other_cyberbullying</th>\n",
              "      <td>4916</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, after cleaning classes are unbalanced so we will drop \"other_cyberbullying\" class as there is not enough data. Later we will oversample \"not_cyberbullying\" class."
      ],
      "metadata": {
        "id": "8qZ7PoaKC808"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"sentiment\"]!=\"other_cyberbullying\"]"
      ],
      "metadata": {
        "id": "i-4tp2O8e-zw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_len'] = [len(text.split()) for text in df.text_clean]"
      ],
      "metadata": {
        "id": "Q712dE4kftOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1650bcc0-abc0-4558-ed8f-81aa16a3d992"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-fe586f79ef4e>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text_len'] = [len(text.split()) for text in df.text_clean]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
        "plt.title('Count of tweets with less than 10 words', fontsize=20)\n",
        "plt.yticks([])\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WQjmvAMHfyG6",
        "outputId": "0f15df02-cebd-4a19-bcf4-0cadd6ffed27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-1c28dcd112f5>:2: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n",
        "plt.title('Count of tweets with high number of words', fontsize=25)\n",
        "plt.yticks([])\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ir99Mjatf7Sz",
        "outputId": "7faaeb8d-8bfb-420d-982c-945eee17035a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-c8466336e1b1>:2: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABjYAAAHpCAYAAAAh7wXQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxuElEQVR4nO3dd3gUZf/+/XOTAKFD6KETIfQSehXB3hVRuJUiRQFFQVHAhiCC/gABQVFQEBARVEABkY5iQTqiYEB6JwECJCEkZPf5gyf5JtnZ7CbZnc3A+3UcHvfNzOzOJ1uumb3OmeuyORwOhwAAAAAAAAAAACwgwN8FAAAAAAAAAAAAeIpgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACwjyN8FAACAG1NMTIyWLFmijRs36r///lNMTIwSEhLSbVO+fHmtW7fOTxUC/nX8+HF17NjRcF1kZKRP9jls2DAtXrzYafnzzz+vgQMH+mSf7nTr1k2bN292Wj527Fg9+uijXtlHeHi44fK1a9eqQoUKXtlHVixatEjDhw93Wt6sWTPNnTvX9HrgO2Z8vpF9DodD69ev14oVK/TPP//o7NmziouLk91uT7fdnDlz1Lx5cz9VCSO5rV0HAJiPYAMAkCohIUFbtmzR5s2btXPnTp09e1YxMTGKjY1VcHCwihYtqipVqig8PFxt2rRR06ZNlTdvXn+XjVzojz/+0ODBg3XhwgV/lwIAAODk8uXLeu655/Tnn3/6uxQAAJANBBsAAF2+fFnz5s3T7Nmzdf78ecNtYmNjFRsbqxMnTui3337TzJkzVaRIET366KN65plnVKJECZOrvjHkxqunc+rMmTMaMGCA4uPjfb4vroT1L66WNN+N2GYAgD8MGzaMUAMAAAtjjg0AuMlt3rxZd999tyZOnOgy1HDl0qVL+uKLL7Rjxw4fVQcrmjdvnimhBgAAQHYcOnRIa9as8XcZAAAgB7hjAwBuYl9++aXGjBmj5ORkf5eCG8iWLVsMlxctWlT33nuvypQpo8DAQElSoUKFzCwNAABAW7dudbnutttuU+3atRUcHJy6rGLFimaUBQAAsoBgAwBuUqtWrdLo0aPlcDhcblO+fHk1b95cpUuXVmBgoC5cuKB9+/bpr7/+UmJioonVwkpc3fnz2muv6eGHHza3GCAXq1Chgs8mCUd6vM4A0jp37pzh8hYtWuiTTz4xuRoAAJAdBBsAcBM6cOCAXn31VZehRtWqVfX666+rbdu2huuvXLmipUuX6osvvtCBAwd8WSosKCkpyXB5uXLlTK4EAADAGecqAABYH8EGANyEJk+erCtXrhiua9iwoWbMmKEiRYq4fHz+/Pn1+OOP69FHH9WsWbOUP39+t/s8f/68VqxYoT///FORkZGKiYlRXFycChUqpOLFi6t27dpq0aKF7rnnHo+GJ+rQoYNOnDjhtHzOnDlq3ry5Vx7jbmLkuLg4LV68WCtXrtShQ4cUExOjokWLqlatWrr//vv10EMPyWazOT3e1eS/aU2dOlVTp051Wt6sWTPNnTs308dmRVxcXOr78s8//+jChQu6fPmyChQooGLFiik8PFzNmjXTvffem+kE8a5e27S6d+/utCw7k3y7mjA8reHDh2v48OFOyx955BG99957+uWXX9S3b1+n9Z06ddKYMWOclv/111/q3Lmz0/Lff//d8HXp3r274YSkK1asULVq1VzWfenSpdT3Y8+ePbpw4YLi4uJUuHBhlSpVSk2bNlXHjh3VqlUrl8+RmW3btmnDhg3asmWLTp8+rZiYGElSSEiIwsLC1LZtW913332ZvteuvhdpdezY0XC50fttt9v1yy+/aN26dYqMjNSxY8cUFxenpKQk5cuXT8WLF1doaKjCwsJUu3ZtRUREqHr16p7/0S48/vjj2rVrl9PyDRs2GHZs9e3bV7/88ku6Zf/73/80YsQIp223bNmip556ymn5fffdpw8++CD138ePH3f5WqW9w8DsNmPLli1atGiRtm3bpjNnzihPnjyqUKGC2rVrp549eyokJCRLz+cN69ev1/fff6+//vpLUVFRyp8/v6pUqaKOHTvqqaeeUsGCBTN9fHYnunc4HFqxYoV++OEH7dmzR+fPn1fx4sVVtWpV3XvvvXr44YcVHBysP//807CNK1++vNatW5e1P/b/d+TIEX3zzTf6+eefderUKSUlJalMmTJq1qyZunXr5tF3MSumTJli+BlKaTcladeuXVq0aJE2bdqks2fPSrreEdyqVSv17Nkz09dSyh3H7uPHj+vrr7/Whg0bdOLECeXLl09VqlTRgw8+qEcffTTd8EOSdPbsWX399ddau3atjh8/LrvdrtDQULVu3VpPP/10jjvCHQ6HVq9ere+//1579+5VdHS0ChcurGrVqunee+9Vp06dlDdv3iw959GjR/XTTz9p8+bNOnz4sGJiYpSQkKCQkBCFhoaqVatWuueeezxqSxctWmR4PE3btmzdulVLlizR1q1bFRUVpdjYWK+fr2zatEk///yztm/fnnr8stlsKlasmEJDQ9W0aVN16NBBDRo0cPkcrj7jaS1evNipvc3u3zJ+/HjNmDHDafn7779veAfrjBkzNH78+HTLbrnlFi1fvtxp28TERDVp0kRXr15Nt7xUqVL69ddfM63rwIEDWrVqlbZs2aLDhw/r4sWLunr1qooVK6aSJUuqUaNGat26tW677bbUoUMz4+47d/bsWX333Xdat26dTp48qfPnz8tut7tsf3ft2qWFCxfqzz//1NmzZ1WgQAFVqFBBd955pzp16pTpOYo7iYmJWrt2rX7++Wft27dPJ0+eVHx8vK5du6b8+fOrRIkSKleunKpXr67atWurSZMmqlSpUrb3BwDwLYINALjJ/Pvvv1q1apXhumLFimnKlCmZhhppBQUFGXYOp5WQkKDJkydr/vz5hmHKhQsXdOHCBR08eFDLli3TuHHj9PTTT+uZZ57x6MeUv/z666964403dOrUqXTLo6OjtXHjRm3cuFHfffedpk2blivnkUhOTtZnn32mmTNnpnZup3Xx4kVdvHhRR44c0apVqzR+/Hh17dpVgwYNcur0saLGjRsrKChI165dS7d8+/bthttv27bNcPn27dt1xx13pFuWlJSkv/76y2nbUqVKuQw1EhMT9fHHH2vu3LmKjY11Wn/+/HmdP39ekZGR+vLLL9W4cWONGDHC447Nv/76S2PGjNGOHTsM1584cUInTpzQL7/8okmTJqlv37569tlnFRAQ4NHzZ9e///6rV155Rfv27TNcHx8fr/j4eJ04cSLd3C116tTRokWLcrTvZs2aGQYb27dv13333Zdumd1u186dO522dfW5cDV2e7NmzbJeqIliY2M1cuRI/fDDD+mWJyQkaO/evdq7d6/mz5+vadOmqUmTJqbUdPbsWQ0fPtypoy4xMVG7du3Srl279NVXX2nmzJkKCwvz6r6joqL00ksvOQWpZ8+e1dmzZ/Xnn3/qiy++0IQJE7y6X4fDoRkzZmjKlClOwz4eOXJER44c0aJFi/Tqq6+qZ8+eXt23K/Hx8Ro9erS+++47p3UHDhzQgQMHtHDhQr3//vu65557TKkpO7755huNGTNG8fHxqcvi4+N14cIF7dixQ19++aVmzJih8uXLS7o+bOewYcMUFxeX7nn+++8//ffff1qwYIHGjx/vdBzw1JkzZ/Tqq69q06ZN6ZZfvXpV0dHR2rx5s7788ktNnDhRNWrUcPt8UVFReu+997RixQrD+dPOnDmjM2fOaMeOHZo2bZoefPBBDR8+XMWKFctW/VevXtWoUaP07bffZuvxntiyZYvGjh2rf/75x3D9lStXdOrUKW3btk2ffPKJmjVrptdff101a9b0WU2eatasmWGwsX37dsNgw+iYcuDAAcXExDi9R7t373YKNVL26crJkyc1ZswYrVmzxvCu7aioKEVFRWnv3r366quvVKVKFb388su68847XT6nOytWrNCIESN08eJFt9smJydr7Nix+vLLL9PVd/XqVV24cEG7d+/WzJkzNWbMGHXo0CHLtfzxxx96/fXXXV6IExsbq9jYWB05ciTdd/Kuu+7Shx9+mOX9AQB8z7e/VgEAuc6KFStcDkH1zDPPqHTp0l7b1+nTp9W1a1fNnDnT5R0iGV28eFGTJk1Snz59dPnyZa/V4k3Lly/Xs88+6xRqZLR582a9+uqrJlXluUuXLqlPnz764IMPDEMNIwkJCZo1a5a6du2q06dP+7ZAExQsWFB16tRxWn7o0CHDOUJcdWAbLd+zZ4/h591VZ0N0dLS6deumadOmGYYarvbbtWtXpzsIjCxcuFD/+9//XIYaGcXFxWnSpEl67rnnPP7eZsfx48f11FNPuQw1MuONtsHV+2EUbu3bt0+XLl1yWr5//37DWlx9XnJzsHH58mX16tXLKdTI6NKlS3rmmWfctn/ecPr0aT311FNurz4+ffq0evfu7fH3xxPnz59Xjx493N4ddujQIfXs2TNbn2MjDodDI0eO1IQJEzKdyyqlA3DNmjVe2W9mUj4bRqFGWlevXtXLL7+s3bt3+7ym7FiwYIHeeOONdKFGRgcOHFC3bt0UHx+vH374QQMHDnQKNdJKSEjQoEGDtGfPnizXExMTox49ejiFGhn9999/6tGjh9uhP3fu3KlHHnlEy5YtMww1MrLb7VqyZIm6dOmiY8eOZan2lMe/8sorPg01Zs+erZ49e7oMNYxs3rxZXbp00Y8//uizujyVchFFRkbHY4fD4XK50XEpq8eZzZs369FHH9Xq1asznV8vrcOHD2vgwIF6//33Pdo+ozVr1uill17yKNRwOBwaPny45s6dm2l9Fy5c0PPPP5/ltm/Xrl3q27ev27uLjRgd/wEAuQPBBgDcZH777TfD5UFBQerUqZPX9nPlyhU9++yz2fqxL10f4mfw4MGy2+1eq8lbPvjgA6cr/V1Zu3at/vjjDx9X5Dm73a7Bgwfr999/z9bj9+zZo379+vm0w9ssrn78G3UsZOVOjqxcrX/16lX179/f8G4Ad+Li4jRw4MBMJ0VetWqV3nrrLZdjiWdm3bp1evPNN7P8OE9NnDjRr+Glqw6nrHQg2e12p+3tdrvhZyizO3Zygy+//NLwDhYjcXFxmjhxoo8ruj681pEjRzza9tSpU/r888+9tu833njD4zmkLl26lDpUU05t375d8+fP93j7MWPG+Pw4uWbNGo+D0eTkZK+9Ft42ffp0j7Y7ceKEhg4dqrfeesuj7a9du6axY8dmuZ5Jkybp0KFDHm17/vx5DR482GVbfvToUT377LOKiorKch2HDh1S3759Mw1wjGzfvl0rV67M8v489cMPP2jMmDEen2+ldeXKFb3yyitug0lfc3URxf79+506y1PuzDBidAzKSrBx4MAB9evXTxcuXPCgamczZ87M1oTq77//vsft04IFC/T99997tG1ycrJefvnlLNUyduzYbJ0LAQByN4aiAoCbSGJiosur3sLDw7M9FIGRCRMm6N9//zVcFxISojvuuEOlS5fWyZMntWrVKsMOzo0bN2rOnDmmDbWRVZUqVVLHjh0VHBys9evXu/x7Fy1apJYtW6b+++67707t4Pzpp58M35OWLVsazqOQ07G8Z8+e7fLq54IFC+rOO+9UhQoVFB0drdWrVys6Otppu71792rChAl64403Upf16dMn9WrpGTNmGF7d9vjjj6tixYrplhn94Hfn8ccfT53YfsGCBTp+/LjTNnfddZfq1q3rtDztUB7Nmzc3HCJi27Zt6eY9OHz4sM6dO2dYy969e3XlypV088xkpbNh0qRJhsNWSdfH5m/ZsqVKly6t48ePa+3atU4dTwkJCRo8eLCWLl3qNHRbVFSUhg4danjlY0BAgNq2bavq1avr2rVr2r59u2EdS5cu1a233qoHHnggdVnazgRXQ/D06dNHRYsWdVqe8n47HA6tX7/e8LENGjRQo0aNVKRIESUmJurcuXP677//9O+//3o1UCtYsKDq1q3rFCpFRkYqNjY23RByroKtlHW33nqr0+MzysndGma0GSlXeOfPn1933HGHKleurAMHDuinn34y7JhauXKlRowY4XZui5xIqalo0aK66667VKZMGe3evVsbNmww3H7x4sV68cUXc7zfDRs2aO3atYbrbDab2rdvrzp16ig2NlZr167VsWPHstX5aiTtlfbt2rVTvXr1dOHCBS1dutTwOHnixAn9+eef6Y4xvhIUFKSOHTuqevXqOnnypJYvX244FM7WrVt19OjRXDsufbNmzdSkSROdPn3a5d+QdsjOsLAwdezYMXVOKqO7+jZv3qxjx445HeMyk7LfEiVK6M4771TJkiV1+PBhrV69WgkJCU7bR0ZGat68eYbnRIMHD3bZKV6vXj01aNBAhQoVUmRkpH7++Wen7/ShQ4f03nvv6Z133vG4/rTPUaZMGbVv316lS5fW+fPnc3zXzpkzZzIN1ps3b66IiAjZ7XZt2rTJMJS9du2aXnrpJa1evTr1GN2yZUvly5dP0vULaIwuPKlTp47uvvvudMtycu7VvHlzp/pS7s5Ie+zI7DiT8bzC1d0dRgG6w+HQSy+95DK4qlatmtq3b68CBQro33//1fr16w3v+Jk8ebLatGljeG7lSspnJE+ePGrXrp1q1Kiha9eu6ciRI9q4cWPqdpcvX3aaWyStOnXqqE2bNrLZbPrzzz+1Y8cOw++IK+fPnze8gCQgIEBNmzZVnTp1VLhwYV25ckVRUVHav3+/IiMjCUIAwAIINgDgJpIyWZ8Rb05CeubMGS1YsMBwXaNGjfTpp5+m6/R88cUX1bNnTx08eNBp+xkzZqhLly65bl6H+++/X2PHjk2d0HPgwIF67rnnDDtrM/5Ybd++vdq3by9JOnjwoGEnZePGjfXMM894teYrV64YduRLUpUqVTR79myVLVs2ddnLL7+sfv36Gd6BsGDBAvXt21dlypSRdH0S5RRff/21YbBx//33u5wcNivSdrJv3LjRMNho376920nJPZ1nw1VQIf3ffBpp/y6jzgmjzobo6Gh99dVXhs/bo0cPDRkyJN2EsadPn9aAAQOcPi8pnc8Z54X47LPPDIdcKVWqlD7//HOn7/xXX32lkSNHOm0/bdo03X///bLZbJKU7nPpKtjo2rVrppMIp0yKnlHv3r1dDt+WmJionTt3avXq1fr7779dPndWNGvWzKmzIzk5Wbt27VLr1q1Tl7kLNtJydcdOTj77ZrUZpUqV0ty5c1W1atXUZXfccYcGDx7stG1CQoL27Nmjpk2b5mif7lSvXl2zZs1SqVKlUpdNnz7d8LN36tQpnTx5UqGhoTnap6tJgvPkyaOPPvooXWfkyy+/rFdffVUrVqzI0T7TCgoK0qRJk9LN29CjRw916tTJMDTbtm2bz4ONAgUKaObMmWrUqFHqss6dO+upp54y7ATdvn17rgw2Bg8erH79+qX+u3379nrhhRdcbn/HHXdo4sSJypMnjySpe/fuevDBBw3DkG3btmUp2JCunxNNnz493dxm//33n7p162YYoMybN089evRIbY+l63eGGrWJgYGBGjdunNOxYceOHXrmmWecjtOLFy/WgAEDstyJ37VrV7322mtOE5wb1e+p6dOnG3ZcBwQE6P/9v/+X7jxAkmbNmmV4p1BUVJS+/vprPf3005KkJk2apM4PdPXqVcNgo0aNGl49/2rWrJnh3UIZQ/HMzjX++ecfJSYmpr7G+/btMxzeyeg4s2rVKpcX3jz55JN6/fXX010YsXXrVvXt29fp/MFut2vq1KlZvnOjSpUq+vTTT1WlSpV0y2NjY1P/niVLlri8g3PAgAFOgfUXX3yRpbukjh8/bniRxxtvvKEnn3zS8DFXrlzRli1btHLlylw7NC4AgKGoAOCmktmPTG/erfHjjz8ajg2eN29eTZgwwelK7jJlyrgcvzdlMu7cpGzZsnr33XfT/YgPDAxUr169DLc/fvx4pmOlm2Xjxo0u7zx4//3304UaklS4cGGNHz/eqbNCut7J7M2OPH8oUKCA4ZWHKR0IKTJ2NmTssE+7/sCBA4ZDPRh1Nixfvtyw4yYiIkLDhw93et1TPndGvv7663T/Thk73ch7771nGGT+73//S3enSooDBw54fTgPV+Nn169f3+Vj8ubNmzopbFaG6smMJ/NsnD59WidPnkz9d9myZdMNYfXXX3+lu6rTivNrpHjnnXfShRqSdO+996ZOpJyRURjtTTabTePHj08XakhSz549Ddslb9QUGxvrcqi+7t27p+uIlK5/LkePHq2QkJAc7TfjfjJORl2lShWXE/h6OpxRTrz88svpQg3pelvVuHFjw+19/dnIjtq1a6cLNSTp9ttvT3fHXVrBwcEaNWpUaqghXX8fjO6MkrL+N+fJk0fjxo1LF2pI0i233KKhQ4caPubo0aNOd0MsWrTIcNs+ffo4hRrS9TDlueeec1qelJSU5fkyWrZsqbffftvw+5jd74Tdbnc510/nzp2dQg1Jevrpp9WuXTvDxyxevDhbdXiLp8MeZnaukZiYmO6uyqwcZ1z9/bfccotTqCFdD38GDBhg+JgNGzZ4PDebdL19NAo1JKlQoUKpnxtX55N16tQxDB579uyZ7uKD7MrsnCN//vxq166d3n33XSYOB4BcjGADAG4imd227aqTKDtcTYTZtm1blx1k9evXV82aNbP0fP7SqVMnwztIMhs/Pzdc7eXqdaxZs6YaNmxouK5cuXKpwz55+nxWYtQJkJiYmK7jKGMHQsYAK+36rHQ2uJp75aGHHkp3NW5atWrVMgwhd+7cme4K4r179xp2PpQqVSrTzgBXHXZ//vmny8dkR7FixVSgQAGn5e+9956++eYbHTp0yJT5dTzpcMp4B0aLFi1Uq1at1H8nJCSku4PC6DNQunRpp8AgtwkNDdVtt91muM5V2+brCVWbNGlieFzImzevyzuCclrTrl27XH72OnfubLi8UKFCuvfee3O037TS3gGXlqvPkK/fh+DgYJd3wPnrs5EdRh3igYGBLu9QaNWqlWHnvLfeh+bNm7u8w+Pee+91Ocxb2mGNkpOTXQbPDz/8sMt9e6ut79+/f5a298Q///zj8rXs2rWry8c98cQThsv37duXo7tHcsrVRRRpQ/GzZ8+mm8A9NDRUd911V7rt0x6XPD3XSE5OdnkXYefOnZ1CjRSPPfaY4TqHw5Glz8h9991nGGqkde3aNZfD5Hbq1Mnl+VBW5gV0dRff8OHDtXTpUh07dszjCdUBALkLQ1EBwE0ks7syvNnx/t9//xkud9V5nna90e3yrp7PX1xdoVq4cGGXj8nKWMC+kpP3xWi8+dz2vmRHZkNENG7cWOfPn9fhw4dTl5cuXVoPPPCARo8endr5uXPnTtntdgUEBGRp4vD9+/cbbjtixAiNGDEiS39HYmKiIiMjU68+dPXcUVFRLgPEzHhr6KcUgYGBuvXWW52u0jx16lTq3C158uRRhQoVdMsttyg8PFxNmzZVRESEV0PYlA6njMNR7dy5U8nJyQoMDHTqQIqIiFCxYsWcwq+GDRvq2LFjOnv2rNN+rHC3RkREhMt1rto2o+F4vMlVWyv5rqa0nYsZ95dZOFWvXr0c7TdF6dKlXXZ2u/qbfX18qVWrlmEQKfnvs5EdruZ0ynjHhLvtXd3hkdW/uUGDBi7X5c2bV7Vq1TI8pqS9M+TMmTMuQ4B77rknS/VIWWvr8+fP75Oh6A4cOOByf5kNm+rqXMbhcOjgwYNevasqq4yGPUxISNDevXtVv359w+NM48aN9fnnn6cuc3cRhVGAfubMGZfn9xnvwEqrePHiqly5suFdSP/9959T6OKKqwtj0jp16pTLNiyzdjWzuy0yKlmypOrXr+80l9j+/fs1ZMgQSVK+fPlUuXJlhYWFqXbt2mratKnq16/vMvwBAOQO3LEBADeR4sWLu1x36tQpr+3HaCge6fqPrsxkHG7E3fP5S8Yhm1KkHa4io9xwJVh23xdX63Pb+5IdjRs3NnzfUjoNMg4VERERoSJFiuiWW25JXRYbG6vIyMh0j0vL1dX63n790j6ft5/bF1e7vvDCC5mGgUlJSTp06JBWr16tqVOnqkePHmrbtq3Gjx/vchLU7DAaJiw+Pj41ZM34GWjcuLFTh3vK+27lYahctWuS67bN1+1ayhw+RnxVk6tO4hIlSmT6OHfrPZWd98HXcuNnIztKlixpuNzV3+DqPXV1BXlW/2ZX5zwpXNWb9jPq7bb+ypUrunLlikfbVqpUSQEB3u9OcPU3lSxZMtP9Zbbe3+crruZYyuxcIyIiIt1nbceOHXI4HDp58qThObvRcSazv9uMc3JP7lTMbGirzGp09f1w5dVXX820Db169ar27dunFStWaMKECerSpYtuu+02TZ8+nUnEASAXI9gAgJtIsWLFXN61kfEqppxwNZ+EuyutXa331pWf3vphki9fPsPlvviB702u3hd3nWWu1ufGK3KzytUQESkdCEZXUUrOV5Jv27ZNZ86cMZzI3FWntrfnXUnbOeDt98ZoktKcqlatmubMmZNuWCd3YmJiNGPGDHXp0sVrd5llNs9GbGys9u3bl7qsaNGiCgsLc3r/UzqlfDFxuFmMhtdL4a+2LTfW5Iqrzu6syo1/s79r8vWx29V752p7bzEaBi8tT469vjgOe9reZxZM54Srv8mTYM/Va+rv85WIiAjD+lOOHRnPNRo3bqzixYunG+rt4sWL2r9/f5aOM5n93dk998vKuYsnn5HMni+z70hWg96mTZtqxowZLu+IM3LmzBlNmDBBffr0IdwAgFwqd/0iAAD4lM1mU8uWLQ3XRUdHu/yxlFUZJwdP4a4j0tX6rExsfu3aNcPlDocjV475bSZX70tsbGymj/PG+5KbGXVsx8TE6ODBgx4HG9u3b3d5tb6rTm1X70d2JScnm/Lc3lS7dm0tWbJEc+bMUY8ePVSnTh2Xw7yktW/fPn388cdeqSGzDqft27enm2+hUaNGstlsKlGiRLpxwy9cuGD4eZGu33Xgboxx5B6uvjvuJsyNjo72QTU3j5vx2O3uzjNXx+a0ncXebusl1+9FRr4KtbJ7DpmYmOiyk9wXr1NWuLqIYvv27YqLi0u961O6PmdPjRo1JGXtXMPoXCazv9vd6+nq85eV19KTwNfVUHCZ1eBunSstW7bUypUr9emnn6pLly6qUaOGR8Nbbtq0SQsXLszy/gAAvsccGwBwk2nbtq3TuPYpPvvsMzVp0iTH+yhVqpRhJ4+rcZNTGI3lm/J8Gbm6isvVEApHjhzJFfNc+JOrYQW8+b5YUbNmzfTpp586Lf/999+1Z8+e1H8XKFAg9e4Co84GV0O9ubojwNX35IknnnA5MXJm0o4H7+q9qVSpkssJkDNTqFChLD8mK5o3b54aADkcDp0+fVpHjhzRvn37tHLlSsPQdfny5Ro6dGiO953S4bRjx450y7dv365KlSqlW5b2fY+IiEg3/8qaNWt06NAhp+f3xRj08J2M73mKmJgYnThxQuXLlzdcn3bOFbjGsfv/uDq2pjBqT6T07burtj4gIECDBg3K1p1E/g4BXP1NUVFRunTpksuO8Mxez9xwvtKsWTOn40x0dLSWLl2aLkxq2LBhamjUuHHjdB3q27ZtM5yLzlWAntnffeDAAZdDRTkcDo8+f96Q2TB+hw8fdnlhQNrjb1YEBgaqffv2at++vaTrF26cPHlSR48e1d69e7Vs2TLt3bvX6XHLli3Tk08+ma19AgB8h2ADAG4y9913nyZOnKioqCindevXr9eiRYv06KOPevx8ly9fVmxsrMqVK5e6LCIiwvBHwYYNGzR8+HDDH9oJCQnatGmT4T6MJpAtWLCg4banT582XL5q1SrD5f7m6koxT8e4zoqIiAitWbPGafkff/yhK1euGF4p73A49PPPPxs+X2YT+5rFG69fyhX7GYcZmDt3brpl9evXT+2UCw0NVbly5VLHuT516pRWrlzp9NyZXa3fuHFjw+9JpUqV1KdPH4/rN5IyNnfGMd8vXbqkHj16eG2IFaPXTcrZZMY2m03lypVTuXLl1KJFC3Xr1k2dO3d26jg+c+aMYmNjvRK6GHU4nT592ikETjvBduPGjbVo0aLUf8+ZM8dwjH1vD0NlZptxM6pXr54CAgLS3amT4ocfflD//v2dlsfGxurHH380ozzLu1GO3d6wceNG2e12wzsfjh496rLjNu2V/0WKFFGNGjXSDZknSXa7XS1btszSBMu5havjl3T9PPWhhx4yfNz69esNl6e8Rv7m6iKKmTNnpvt3xuNMWr///rvOnTvn9ByuAvQiRYqoevXq2r9/v9O69evX6/bbbzd83Pbt210OSebtc7+QkBCVL19eJ06ccFr3yy+/pAYQRuu8ITAwUBUrVlTFihXVunVrdevWTXfccYfOnDmTbjt3FwEBAPyDoagA4CYTHBysfv36uVz/5ptvenS79bVr1/T999/rnnvu0T///JNuXbt27Qwfc+TIEX3zzTeG6z799FOXt5W3adPGaZmryUw3btzotCwqKkqzZs0y3N7fXHXy+OLqX1fvS1xcnD755BPDdd99953LzpW2bdt6q7Rs88br52qIiCNHjqT7d9rOBsn5x71RWJjZ1fqu3o8ZM2bo6NGjLh+XcZ+ffvqp0/cqJCRE9erVc9o+JiZGEyZM8Oi57Xa7fv31V73wwgsut3EVKngyZ8/cuXM9uuLSZrOpQIEChuvi4+PdPt4TrsKHtJ+BPHnypHtNPXn/Je9PHG5mm3EzKlSokFq1amW4bvr06U6T/CYmJuqNN97Q+fPnzSjP8m6UY7c3nDx5UvPnzzdcN2nSJMPlefLkcbqz1tWxZMyYMR4HnpGRkXrnnXdyRTsSEhKS7g7EtD7++GPDIbyio6M1e/Zsw8e0atUqV8zJ42rYw8zONSpWrJhuAu3o6OgsB+iuztW+//57p0BMun4Hg6vPX8mSJbM0L5anXNW/aNEiHTt2zGl5dHS05s2bl6V9TJ8+3SmsMJIvXz7DCwi8db4BAPAu7tgAgJtQly5dtHbtWv3+++9O665du6Y333xTixcv1hNPPKEWLVqoZMmSstlsunjxovbt26c///xTS5Ys0cmTJw2f/9Zbb3V5hdioUaN0/vx5PfbYYypZsqROnz6tL7/8Up999lmmz5VRnTp1tG7dOqflGzZs0MSJE9WnTx8FBwdr69atqfvMjVxdzb9582b17t1bDRs2THd1/UMPPaQyZcpka1/Vq1dXu3btDK9y++STT3T16lV169ZN5cuX1/nz57Vo0SKXP25r1KjhsjPFTK5evyVLlig+Pt5p/OSnnnrKsJO8efPmTlfsZ5SxIzsiIkLLli3L9DEtWrRwua5du3aqWbOm07ASMTExeuyxx9SrVy917NhRVapUUZ48eXT16lWdP39e+/fv199//61ffvlFu3btkt1u1/PPP+/0/H379tXAgQOdls+ePVsHDhxQt27d1KBBAxUvXlx2u12xsbE6fvy4IiMjtXnzZv3888+GV4amVaVKFV24cMFp+ejRo7Vz506VK1dOgYGBkq7PyfL444+nbrNgwQK9++67qlu3rm699VbVr19fYWFhKlmypIKDg5WYmKhjx45pwYIF+vPPP532ERAQ4HL4r6xydddOWnXr1k33XaxatapKlCiR6Wvki/k1zGwzblbdu3fXr7/+6rQ8Pj5eTz31lDp27Kjw8HDFxsZq7dq1HgeRuHGO3d4yduxYxcXF6bHHHlPx4sV17NgxTZs2TcuXLzfcvkOHDgoJCUm3rHv37po7d67TRNE7duzQI488or59+6ply5YqV66cbDab4uLidPbsWUVGRmrnzp3asGFD6rBDd955p2/+0Czq3bu3Bg8e7LT88OHD6tGjh4YNG6aGDRvKbrdr8+bNevfddw2PRSnPlRu4GvYwraCgIDVo0CDdsoiICP3000+ZPndmwUaPHj2c7kCVpKSkJPXq1UvDhg1Thw4dFBwcrH379mnSpEnavHmz4XM9/fTTPgmJOnfunO4OyBRXrlxRz5499eabb6pVq1ay2WzaunWr3nnnHbfzHmU0bdo0TZo0SREREWrXrp3q1KmjsLAwhYSEKG/evEpISNChQ4f0+eefG4YpJUuWzO6fBwDwIYINALgJBQUFafLkyXr88cddjqGbMnGu9H+T/xldJWbEZrNp2LBh6tu3r9NwHklJSZo4caImTpzocriPFPnz59eQIUMM191xxx2aMmWK4bpPPvlEn3zyicuhDHKTzOY0+fXXX50615o1a5ajTsohQ4Zoy5Ythldxzpo1S7NmzXL7vgQEBGjYsGHZGrvb25o2bWo4tIPD4dDKlSudhod65JFHDIONZs2aubxrRbr+Nzds2DDdMk+GY8jsan2bzabXXntNvXr1cpqs9eLFi6nfE+n6d9bTCV1T3HHHHWrVqpVhgJn2sxUYGCi73Z6t70rTpk0NO2ni4uK0YMGCdMuqVq2aLtiQrr9Pu3fvdrpKODAw0O2E5U2aNDG8+jU78ufP77bDqVGjRk7LIiIitHr1apeP8fbdGpL5bcbN6NZbb1XHjh21du1ap3XJyclatWqV0xBJ+fLlc+pYhrMb5djtLUlJSZowYYImTJjg9tgbFBSkAQMGOC0vU6aM+vXrp8mTJzutO3TokF577TVJ1485AQEBbtvW3ODuu+/WV199pS1btjit2717t5588kmPzk0feuihXDUcl7uLKGrWrOl0jtK4ceNMg42yZcuqcuXKma7v06ePpk2b5rQuKipKL7/8siS5/fxVq1ZN//vf/1yuz4mIiAi1bt1av/32m9O648eP69lnn83ybxEjycnJ2rJli9PnypNzjpYtW2Z7vwAA3/H/PZkAAL8oUqSIvvrqK49O1B0OR5Z/SLRp08ZlKJHCXef5mDFjXI6LHB4e7nYopLQ1V65cWcWKFct0e38ICwsz9cdSeHi4xowZk+kVd5m9L5L0yiuvqHXr1t4uLVtatWrllSviXQ0RkaJGjRpOwy7VqFHD5SSmkvvOBul6J8ebb77pNiTKaqghXe/Emjx5sm655ZZMt0tOTs52R0Hnzp1dzvmQE550vBnNdZAT7ubCMAqy3IVb3p5fQzK/zbhZjR49WmFhYR5tW6JECb355puG61LuWMJ1N8qx2xsyHkfdHXufffZZ1axZ03Bd//799cADD2T6eIfDYYlQQ7p+Dvjhhx+qQoUKLrdxd25ar149jRo1yhflZZu7sDvjkJeS++NMZkNepnjhhRfUoUOHTLfJ7PNXrFgxffzxxy6HhfSGkSNHqnDhwi7XZ3y/vXke6u57kTdv3lxz5w8AID2CDQC4iYWEhOjzzz/X4MGDM+2gzUyePHlUtGhRw3W9e/fWuHHjDCelzkyxYsX02Wef6d577810u5EjR3p0JXK1atU0d+5cl2PT+9u7777r9eFqMnPvvffqs88+y/IwPsHBwRo3bpx69erlo8qyLjAwUBMnTszxEAEpV+y7YtSxYHQXR1qedDZI14eGmzZtWrY77/LkyaNy5coZritSpIgWLFigu+++O1vPLbke+ki6PtH5qFGjvHbnhCcCAgL01ltvuZwHIbt80eHkizs2JPPbjJtRSEiIZs+e7fZ7XLNmTc2dO1clSpQwXO+t4dJuJDfKsTun7r//fg0dOtSjux8ff/xxw6EFU9hsNo0bN06DBg3KdphWrFgxl+dz/hASEqJvvvkmW0Hufffdp7lz5yo4ONgHlWWfu4sojI4pNWvWzPQ74EmAHhAQoClTpqhnz54e1Zlx/99++62qVq2a5cdmRcWKFTVjxgyPvu8NGjTQhx9+6NN6UgQHB2vChAluLxIBAPgHwQYA3OQCAwPVr18/rV+/Xq+88ooaNGigoKDMRyq02WyqX7++XnrpJW3YsCHTjp8HH3xQK1euVK9evdx28JQuXVrPP/+8Vq5c6dGVWOXLl9dXX33l8urPAgUKqFevXlq8eHGuHoqlfPnyWrRokd544w21atVKpUqV8nlHcevWrbVy5Uq98MILLidzTVGsWDH16NFDq1at0oMPPujTurKjdu3aWrZsmQYPHqwmTZooJCQkW69fZp0DRp3aUuYd21m5Wv+2227TqlWr9NJLL2V6hWqKggUL6rbbbtOIESP066+/6rHHHnO5baFChTR58mTNnTtXHTp0SDf/giu33HKLunXrpnnz5jkN55XRI488ou+//17dunVT7dq1VaRIEY/G4H7zzTfVs2dP1apVy22bI13/Pt9zzz1asmSJnnzySbfbZ1VmHU5VqlRxGtdeuv7ZcxXcenLHTnb5o824GZUqVUpz587VxIkTddttt6l06dLKkyePSpcurVatWmnMmDH65ptvFBYWZjgJr6R0E//iuhvl2O0NvXr10hdffOHy7tRSpUpp7Nixeuedd9wGIDabTf3799eKFSvUtWtXj8Ly0qVL68EHH9SHH36ojRs3urwjxF9CQkI0a9YsffTRR2rcuHGmx5agoCC1adNGX3zxhT744IMsX1Rjhvz586tevXou1xudawQGBmZ6EYWnAXpQUJCGDx+uxYsX65577nEb+qTc4fvtt9+qYsWKHu0jpxo1aqTvv/9ebdu2Nfy8FyxYUP369dO8efOc7qJ1Z/z48erSpYtuueUWj85RihYtqscee0zLly/PNXPPAACc2Rw3ywCmAACPxcXF6a+//lJ0dLRiYmJ0+fJlBQcHq0iRIqpSpYqqV6+erav6HA6H9u3bp8jISMXExCguLk6FChVS8eLFVbt2bVWrVi3bNR87dkxbtmxRdHS0goODVaFCBbVs2TJX/rDNjQ4fPqx//vlHFy5c0OXLl1WgQAEVK1ZM4eHhCg8PzxXzadxMTp8+rb/++kvnzp3TpUuXZLPZVLBgQZUpU0ZVq1ZV5cqVPQoDjCQmJurvv//WkSNHdPHiRcXHx6tAgQIqUqSIKlWqpFtuucX0oV+uXr2qAwcO6NixY4qKilJ8fLyuXbuW+jmsVq2awsPDPQplALMlJCTovvvu0/Hjx53WDR8+PFtXSd8sOHb/n3///Vd79uxRdHS0ChYsqLCwMDVt2jTbd2A4HA7t379fkZGRunjxoi5fvqy8efOqUKFCKl++vMLCwlze7ZdbXb58Wdu3b9eZM2cUExMjm82mYsWKKTQ0VI0aNfLpUEk3msTERP3111+p5wKJiYkqWrSoSpUqpYYNG/p9suwTJ05oy5YtOnv2rAoUKKDy5curZcuWXrkLJy4uTgcOHNDx48cVHR2t+Ph4ORwO5c+fXyVLllS1atVUo0aNbJ9nAQDMQ7ABAAAAAGmcPHlSc+bM0YMPPqhatWq5DHdPnTql4cOH648//nBaFxAQoJUrV6pSpUq+LhcAAAC46RBBAwAAAEAaV65c0axZszRr1iyVKFFC9erVU+XKlVPno4qJiVFkZKS2bdvmcuLZRx55hFADAAAA8BGCDQAAAABw4dy5c9qwYUOWHlO5cmUNGTLENwUBAAAAYPJwAAAAAPCWsLAwzZw503DCeQAAAADeQbABAAAAAGnky5cvyxMRFylSRP369dPixYtVoUIFH1UGAAAAQGLycAAAAABwkpCQoN9//11btmzRP//8o+PHj+vChQtKSEhQcHCwihQpojJlyqhOnTpq0qSJbr/9duXLl8/fZQMAAAA3BYINAAAAAAAAAABgGQxFBQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBlB/i5Akux2u65du6aAgADZbDZ/lwMAAAAAAAAAAEzkcDhkt9sVFBSkgIDM78nIFcHGtWvXtHv3bn+XAQAAAAAAAAAA/KhevXrKmzdvptvkimAjJX2pV6+eAgMD/VwNAAAAAAAAAAAwU3Jysnbv3u32bg0plwQbKcNPBQYGEmwAAAAAAAAAAHCT8mS6CiYPBwAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsIxcMXl4iunTp2vNmjU6ePCggoOD1ahRIw0ZMkTVqlVL3eatt97S77//rrNnz6pAgQKp24SFhaVuM3r0aG3fvl379u1TWFiYvv/+e5f7PHLkiB5++GEFBgZq69atPv37AAAAAAAAAABAzuSqOza2bt2qJ598UgsXLtSsWbN07do19e7dW/Hx8anb1KlTR2PHjtWPP/6ozz//XA6HQ71791ZycnK65+rUqZPuvffeTPeXlJSkl156SU2aNPHJ3wMAAAAAAAAAALwr192xERgYmPrv9957Ty1bttQ///yjpk2bSpKeeOKJ1PUVKlTQoEGD9NBDD+nEiROqVKmSJOmNN96QJJ0/f16RkZEu9zdp0iRVq1ZNLVu21I4dO3zxJwEAAAAAAAAAAC/KVXdsZHT58mVJUtGiRQ3Xx8fHa9GiRapQoYLKli2bpef+448/9NNPP2nEiBE5rhMAAAAAAAAAAJgjV92xkZbdbteYMWMUERGhGjVqpFs3b948jR8/XvHx8apatapmzZqlvHnzevzcFy5c0PDhwzVu3DgVKlTI26UDAAAAAAAAAAAfybV3bIwcOVL79+/XxIkTndY9+OCDWrx4sb788ktVqVJFgwYN0tWrVz1+7jfffFP3339/6vBWAAAAAAAAAADAGnLlHRujRo3Shg0b9OWXXxoOMVW4cGEVLlxYVapUUYMGDdSsWTOtXr1a999/v0fPv2nTJq1bt04zZ86UJDkcDtntdtWuXVujRo3SY4895tW/BwAAAAAAAAAAeEeuCjYcDodGjRql1atXa+7cuapYsaLHj0tMTPR4PwsWLFBycnLqv9euXasZM2bo66+/VpkyZbJcNwAAAAAAAAAAMEeuCjbeeecdLV++XB9//LEKFiyoqKgoSdfv0AgODtaxY8f0448/qnXr1goJCdHp06c1ffp0BQcH69Zbb019niNHjig+Pl5RUVFKSEjQ3r17JUlhYWHKmzevwsLC0u3377//VkBAgNNcHgAAAAAAAAAAIHfJVcHG119/LUnq1q1buuVjx47Vo48+qrx582rr1q2aPXu2Ll26pBIlSqhJkyaaP3++SpQokbr9G2+8oc2bN6f+++GHH5Z0/c6MChUq+P4PAQAAAAAAAAAAPmFzOBwOfxeRnJysnTt3qmHDhgoMDPR3OQAAAAAAAAAAwERZyQkCTKoJAAAAAAAAAAAgx3JlsJGcbL8p9gkAAAAAAAAAALImV82xkSIwMED9R3ymfYdPm7K/GlXKatrIPqbsCwAAAAAAAAAAZF+uDDYkad/h09odedTfZQAAAAAAAAAAgFwkVw5FBQAAAAAAAAAAYIRgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDY8EBysv2m2CcAAAAAAAAAALldkL8LsILAwAANeGeO9h05bcr+alQuq4/f7G7KvgAAAAAAAAAAsBKCDQ/tO3Jau/cd93cZAAAAAAAAAADc1BiKCgAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsWlJxsvyn2CQAAAAAAAABARkH+LgBZFxgYoOfGzNf+o2dN2V/1SqX10WtdTdkXAAAAAAAAAACZIdiwqP1Hz2r3fyf8XQYAAAAAAAAAAKZiKCoAAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwgx5Lt9ptinwAAAAAAAAAA/wvydwGwvsCAAD33/77Vf8eiTdnfLRVL6qNXHzNlXwAAAAAAAACA3IVgA17x37Fo7T5wyt9lAAAAAAAAAABucAxFBQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLCMruA6dOnSpJeuyxx1S2bNl062JjY7V3715JUtOmTXNQHgAAAAAAAAAAwP/JUbBhs9nUqlUrp2AjMjJS3bp1U0BAgPbs2ZPjIgEAAAAAAAAAACQfDUWVmJgoSXI4HL54egAAAAAAAAAAcJPK0h0bmzdv1ubNm9Mt++677/T777+n/tvhcGjjxo2SpODgYC+UCAAAAAAAAAAAcF2Wg42PPvoo9d8Oh0OLFi0y3NZms6lq1ao5qw4AAAAAAAAAACCNLM+xkTK8lM1mS/fvjPLmzauXXnopB6UBAAAAAAAAAACkl6Vg4/bbb1f58uUlScOHD5fNZtOzzz6rKlWqpG5js9lUtGhRNWzYUMWLF/dqsQAAAAAAAAAA4OaWpWCjZs2aqlmzpiRpypQpkqQ777xTderU8X5lAAAAAAAAAAAAGWR5KKoU69at82YdAAAAAAAAAAAAbmU72JCky5cva+nSpTp69KguXbrkNN+GzWbTmDFjclQgkFXJdrsCAwJu+H0CAAAAAAAAwM0o28HGpk2b9PzzzysuLi7T7Qg2YLbAgAA9/8EP2n8s2pT9Va9YUlNfetCUfQEAAAAAAADAzS7bwcbYsWMVGxub6TY2my27Tw/kyP5j0fr74Bl/lwEAAAAAAAAA8LJsBxsHDx6UzWZTeHi4nnnmGRUvXlyBgYHerA0AAAAAAAAAACCdbAcbZcuW1fHjxzVo0CC1b9/eiyUBAAAAAAAAAAAYy/Zsx08++aQcDod27NjhzXoAAAAAAAAAAABcyvYdG0WKFFHFihU1ffp0HThwQE2bNlXRokWdtnv44YdzUh8AAAAAAAAAAECqbAcbr732mmw2mxwOh9auXau1a9c6bWOz2Qg2AAAAAAAAAACA12Q72JAkh8OR7n8BAAAAAAAAAAB8KdvBxvPPP+/NOgAAAAAAAAAAANwi2AAAAAAAAAAAAJYR4O8CAAAAAAAAAAAAPJXtOza6d+/udhubzabZs2dndxcAAAAAAAAAAADpZDvY2Lx5s2w2m8v1Docj0/UAAAAAAAAAAABZle1gQ7oeXhgh0AAAAAAAAAAAAL6Q7WBj7dq1TssuXLigX375RR9//LGqVKmiyZMn56g4AAAAAAAAAACAtLIdbJQvX95wWd26dXX16lVNnz5d33zzjYYNG5ajAgEAAAAAAAAAAFIE+OJJCxYsKIfDoaVLl/ri6QEAAAAAAAAAwE0q23dsTJ061WmZ3W5XdHS0fvjhB0lSXFxc9isDAAAAAAAAAADIIEfBRmaThNtsNjVt2jS7Tw8AAAAAAAAAAOAk28GGJDkcDpfrGjZsqLfffjsnTw8AAAAAAAAAAJBOtoONsWPHOi2z2WwqVKiQKleurOrVq+eoMAAAAAAAAAAAgIyyHWw88sgj3qwDAAAAAAAAAADArRwNRSVJZ86c0cqVK3X48GFJUpUqVXTXXXepTJkyOX1qAAAsJdnuUGCA6/mnrL4/AAAAAACA3CBHwcb8+fM1duxYJSUlpVs+fvx4vf7663riiSdyVBwAAFYSGGDTOws36UjUJZ/vq3KpInrz8RY+3w8AAAAAAEBuk+1g448//tCoUaMkOU8inpiYqJEjR6py5cpq0YJOFwDAzeNI1CXtPxnj7zIAAAAAAABuWNkONmbNmiWHw6GAgADdcccdql+/vmw2m3bt2qU1a9bI4XBo5syZBBsAAAAAAAAAAMBrsh1s7Nq1SzabTf3799fAgQPTrZsyZYo++ugj7dq1K8cFAgCA7GHODwAAAAAAcCPKdrARFxcnSWrQoIHTupRlKdsAAADzBQbYNHbRFh2NuuzzfVUqVVjDH23q8/0AAAAAAABkO9goWbKkzpw5o8WLF6t169YKDAyUJNntdi1evDh1GwAA4D9Hoy7rv9Mx/i4DAAAAAADAa7IdbLRs2VKLFy/WTz/9pK1bt6pOnTqSpD179igqKko2m00tW7b0WqEAAAAAAAAAAADZDjb69++vVatWKT4+XtHR0fr5559T1zkcDhUqVEj9+/f3SpEAAAAAAAAAAACSFJDdB1aqVEmzZs1StWrV5HA40v0XFhammTNnqlKlSt6sFQAAAAAAAAAA3OSyfceGJNWvX1/Lly/X3r17dejQIUlS1apVVatWLa8UBwAAAAAAAAAAkFa2g4158+Zp5cqVKleunN5///10Ycarr76q06dP66677tKTTz7plUIBAMgo2e5QYIDtht8nAAAAAAAA/k+2g43vvvtOe/fu1SuvvOK0rnbt2vrhhx8UGxtLsAEA8JnAAJvenv+7Dp+9ZMr+qpQuore7tjJlXwAAAAAAADCW7WDjyJEjkqTw8HCnddWrV0+3DQAAvnL47CXtO3HB32UAAAAAAADAJNmePDw5OVmSdOrUKad1KctStgFuZsl2+02xTwAAAAAAAAAwQ7bv2ChfvrwOHDigjz/+WI0bN1bVqlUlSYcOHdK0adNStwFudoEBARo4ebn+O37OlP3dUqGEprx4nyn7AgAAAAAAAACzZTvY6NChgw4cOKBTp07pgQceUIUKFSRJx48f17Vr12Sz2dShQwevFQpY2X/Hz+nvQ2f9XQYA+JXd7lCAiROvm70/AAAAAABgjmwHG3369NGyZct06tQpXbt2LXU+DYfDIUkqW7asevfu7Z0qAQCA5QUE2DTuh206Fn3Z5/uqWLKwXnmwsc/3AwAAAAAAzJftYKNo0aKaP3++3n77bf3yyy+y//9j+gcEBKhdu3YaMWKEihUr5q06AQDADeBY9GUdOHPR32UAAAAAAAALy3awIV2/K+OTTz7RxYsXU+/YqFy5sooWLeqV4gAAAAAAAAAAANLKUbCRomjRoqpfv743ngoAAAAAAAAAAMClAH8XAAAAAAAAAAAA4CmCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAwGPJdvtNsU8AAAAAAADkXkH+LgAAYB2BAQF6c+6vOnT2kin7q1q6iN7p1saUfQEAAAAAAMAaCDYAAFly6OwlRR4/7+8yAAAAAAAAcJNiKCoAAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAMBNyW533ND7AwAAAADgRsUcGwAA4KYUEGDTxOU7dPx8rM/3VSGkkAbf18jn+wEAAAAA4GZAsAEAAG5ax8/H6uDZS/4uAwAAAAAAZAFDUQEAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBvATSbZbr8p9gkAAAAAAADgxhTk7wIAmCswIECDP/pJ/504b8r+bikfoonP3W3Kvm5EyXa7AgPMzaD9sU8AAAAAAADAUwQbwE3ovxPn9c/hKH+XAQ8EBgTotS9+1sHTF03ZX7WyRTWm562m7AsAAAAAAADIDoINAMjlDp6+qH+PnfN3GQAAAAAAAECuwFgjAAAAAAAAAADAMgg2ACANJlcHAAAAAAAAcjeGogKANAIDAvTqZ+t08NQFU/ZXrVxx/b8+HUzZFwAAAAAAAHAjINgAgAwOnrqgvUeZ0wIAAAAAAADIjRiKCgAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgD4VbLdflPsEwAAAAAAAIB3BPm7AAA3t8CAAA35dLUOnLxgyv7CQotr/LN3mLIvAAAAAAAAAN5HsAHA7w6cvKA9R6L9XQYAAAAAAAAAC2AoKgAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAIBcwG533ND7AwAAAADAW4L8XQAAAACkgACbPly5SyfOx/l8X+VDCuqFuxr4fD8AAAAAAPgCwQYAAEAuceJ8nA5FXfJ3GQAAAAAA5GoMRQUAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAADAid3uuKH3BwAAAACwriB/FwAAAIDcJyDApmlr/taJC3E+31f54gXV//a6Pt8PAAAAAODGQLABAAAAQycuxOlI9GV/lwEAAAAAQDoMRQUAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAADkena744beHwAAAADAc0H+LgAAAABwJyDAps/W79GpmDif76tcsYLqc1ttn+8HAAAAAJA9BBsAAACwhFMxcTp6LtbfZQAAAAAA/IyhqAAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAAAAAAADLINgAAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAMgCu91xU+wTAAAAAHKrIH8XAAAAAFhJQIBNX/zyr05fjDdlf2WLFlDPdjVN2RcAAAAAWAHBBgAAAJBFpy/G6/j5OH+XAQAAAAA3JYaiAgAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZBsAEAAAAAAAAAACyDYAMAAAAAAAAAAFgGwQYAAAAAAAAAALAMgg0AAAAAAAAAAGAZBBsAAAAAAAAAAMAyCDYAAAAAAAAAAIBlEGwAAAAAFmZ3OG6KfQIAAABAiiB/FwAAAAAg+wJsNs37fZ/OXLpiyv7KFMmvJ1vVMGVfAAAAAGCEYAMAAACwuDOXrujEhTh/lwEAAAAApmAoKgAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAADwGrvDcVPsEwAAAID/MHk4AAAAAK8JsNm04M8Dirp8xZT9lSqcX080DzNlXwAAAAByB4INAAAAAF4VdfmKTsbE+7sMAAAAADcohqICAAAAAAAAAACWQbABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAANyw7A7HTbFPAAAA4GYS5O8CAAAAAMBXAmw2Ldp2UFGXE0zZX6nCwXq0cTVT9gUAAADcrAg2AAAAANzQoi4n6PTFeH+XAQAAAMBLGIoKAAAAAAAAAABYBsEGAAAAAJiEOT8AAACAnGMoKgAAAAAwSYDNpmU7D+tcrDlzfpQoFKz7G1YxZV8AAACAWQg2AAAAAMBE52ITdObSFX+XAQAAAFgWQ1EBAAAAAHIFhuoCAACAJ3LFHRuO//9EMjk5OXVZnVtClS+vOeXdUql0un0bqRMWqnx5Ak2qp4zbempXLWtaPWEVSrmtp1aVUsqXx5ycrFr5Eu5fHxPrCfOgnlqVSypfkFmvT4j7eiqVMK+e0OJu6wmvUFx5g2ym1FO1bDG39dQob149Vcp4UE9oUeUNNKueIm7rqV62iGn1VC5V2G09t5Qtojy5qJ6w0ubUU6mE+1okqWrpwjKjOaxQ0rN6qpQyp57yHr4+lUsUlBnNYfniBT2qp1KJAqbUE1q8gEf1VCxeQGY0h+WKeVZP+WLm1FOmqPt6QovmV6DNnM7QMkXyu62nXJFg0+opXTjYbT2lCwcrUObUU8KjevIpwKR6ShbO57aekgXzmlZPSMG8mdZjdzgUYDPnOOrpPrcdjVbc1URTaimYL68aViopV69Qbnx9AAAAblQp560ODy48sTk82crHEhMTtXv3bn+XAQAAAAAAAAAA/KhevXrKmzdvptvkimDDbrfr2rVrCggIkI0rUwAAAAAAAAAAuKk4HA7Z7XYFBQUpICDz4QxyRbABAAAAAAAAAADgCSYPBwAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWAbBBgAAAAAAAAAAsAyCDQAAAAAAAAAAYBkEGwAAAAAAAAAAwDIINgAAAAAAAAAAgGUQbAAAAAAAAAAAAMsg2AAAAAAAAAAAAJZhuWBjy5Yt6tevn9q0aaPw8HCtWbMm3fpVq1apV69eat68ucLDw7V3716/1ZOUlKRx48bpgQceUMOGDdWmTRu9+uqrOnPmjF/qkaQpU6bo7rvvVsOGDdW0aVP17NlTu3bt8ls9ab311lsKDw/XF1984bd6hg0bpvDw8HT/9e7d22/1SNKBAwfUr18/NW7cWA0bNlSnTp108uRJv9ST8bVJ+e+zzz7zSz1xcXEaNWqU2rVrp/r16+vee+/V/PnzfVKLJ/VER0dr2LBhatOmjRo0aKDevXvr8OHDPqnl008/VadOndSoUSO1bNlSAwYM0MGDB9Ntc/XqVY0cOVLNmzdXo0aNNHDgQEVHR/utngULFqhbt26KiIhQeHi4Ll265JNaPKknJiZG77zzju666y7Vr19f7du31+jRo3X58mW/1CNdbwNvv/121a9fXy1atFD//v114MABv9WTwuFwqE+fPm7bcF/X061bN6e256233vJbPZK0Y8cOde/eXQ0bNlRERISefPJJJSQkmF7P8ePHXbbPK1asML0eSYqKitIrr7yi1q1bq2HDhnrkkUe0cuVKr9fiaT1Hjx7Vc889pxYtWigiIkIvvviiz9rDr776Sg888IAiIiIUERGhJ554Qj///HPqejPbZk/qMbNtdleP2W2zu3okc9tmT+pJYUbb7Ek9ZrbNntQjmdc2u6vH7LbZXT2SuW2zJ/WY2TZnNH36dIWHh+vdd99NXWZ2++yuHrPbZ3f1mN0eStKZM2c0ZMgQNW/eXPXr19cDDzyg3bt3p6438/dphw4dDPc1cuRISb7/frn7LepwODR58mS1adNG9evXV8+ePZ1+i06bNk1dunRRgwYN1KRJE5/W40nfnDc/496ox1ufcW/1W3rreOaNfkszPzue9ltu2LBBnTt3Vv369dW0aVMNGDDAb/UcOnRI/fv3V/PmzRUREaGuXbtq06ZNPqnHk35LM98vT9rhfv36qX379qpXr57atGmjV155xWt945YLNuLj4xUeHq4RI0a4XB8REaEhQ4b4vZ6EhATt2bNH/fv316JFizR16tTUD7s/6pGkKlWq6K233tLSpUv11VdfqXz58urVq5fOnz/vl3pSrF69Wrt27VLp0qV9UkdW6mnbtq1+/fXX1P8++OADv9Vz9OhR/e9//1O1atU0d+5c/fDDDxowYIDy5cvnl3rSvi6//vqrxowZI5vNprvuussv9bz33nvauHGjxo0bpx9//FE9evTQO++8o7Vr15pej8Ph0HPPPadjx47p448/1uLFi1W+fHk9/fTTio+P93otmzdv1pNPPqmFCxdq1qxZunbtmnr37p1uX2PGjNH69es1adIkzZ07V2fPntXzzz/v9Vo8refKlStq27at+vXr55MaslLP2bNndfbsWQ0dOlTLli3T2LFjtXHjRr3++ut+qUeS6tSpo7Fjx+rHH3/U559/LofDod69eys5Odkv9aSYPXu2bDab12vITj2PP/54ujbo1Vdf9Vs9O3bsUJ8+fdSmTRt98803+vbbb/Xkk08qIMD7p1bu6ilXrpxT+zxw4EAVKFBA7dq1M70eSRo6dKgOHTqkadOmaenSpbrjjjs0aNAg7dmzx/R64uPj1atXL9lsNs2ePVvz589XUlKS+vXrJ7vd7vV6ypYtqyFDhmjRokX67rvv1KJFCz333HPav3+/JHPbZk/qMbNtdleP2W2zu3okc9tmT+pJYUbb7Gk9ZrXNntRjZtvsrh6z22Z39Ujmts3u6jG7bU7rr7/+0tdff63w8PB0y81un93VY3b77K4es9vDixcvqmvXrsqTJ49mzJih5cuXa+jQoSpatGjqNmb+Pv3222/T7WvWrFmSpLvvvluS779f7n4bz5gxQ3PnztXbb7+thQsXKn/+/Ordu7euXr2auk1SUpLuvvtude3a1ef1eNI3583PuDfq8dZn3Bu1ePN45o1+SzM/O570W65cuVKvvvqqHn30UX3//feaP3++7r//fr/V069fPyUnJ2v27NlatGiRatasqX79+ikqKsrr9Uju+y3NfL88aYdbtGihSZMm6aefftKHH36oY8eO6cUXX8xxbZIkh4XVqFHDsXr1asN1x44dc9SoUcOxZ8+eXFFPil27djlq1KjhOHHiRK6o5/Lly44aNWo4fv/9d7/Vc/r0aUfbtm0d+/btc9x2222OWbNm+bwWV/UMHTrU0b9/f1P270k9gwYNcgwZMiTX1JNR//79Hd27d/dbPffdd59j6tSp6ZY98sgjjg8++MD0eg4ePOioUaOGY9++fanLkpOTHS1atHAsXLjQ5/WcO3fOUaNGDcfmzZsdDofDcenSJUedOnUcK1asSN3mv//+c9SoUcOxY8cO0+tJa9OmTY4aNWo4Ll686PM6PKknxY8//uioU6eOIykpKVfUs3fvXkeNGjUcR44c8Vs9e/bscbRt29Zx9uxZj9oEX9bz1FNPOUaPHm3K/j2pp3Pnzo6JEyfmmnoyeuihhxzDhw/3Wz0NGzZ0LF68ON12zZo180t7uHHjRkfNmjUdly9fTt3m0qVLjvDwcMdvv/3m83ocDoejadOmjoULF/q9bc5YT1r+aJszqyeFmW2zJ/WY2Ta7qsdfbbNRPf5sm43q8WfbbFRPRma2zUb1+LNtzliPv9rm2NhYx5133un47bff0n1+/dU+u6onLTPbZ0/qSeHr9nDcuHGOrl27ZukxZv4+HT16tOP222932O12h8Nh7vcrY9tvt9sdrVu3dnz22Wepyy5duuSoW7euY9myZU6P/+677xyNGzf2WT1pedI35+3PuLf6Cr3xGc9uLb46nuW039LMz06KjP2WSUlJjrZt25ry3fKknpTfHlu2bHHaJqfHs5z2W/rj/fKkHV6zZo0jPDzckZiYmOOaLHfHhtXFxsbKZrOpSJEi/i5FiYmJWrBggQoXLux0NYZZ7Ha7XnnlFfXu3VvVq1f3Sw0Zbd68WS1bttRdd92lESNG6MKFC36pw263a8OGDapSpYp69+6tli1bqnPnzj4dbiAroqOj9fPPP+uxxx7zWw2NGjXSunXrdObMGTkcDm3atEmHDh1SmzZtTK8lMTFRktLdTRMQEKC8efNq27ZtPt9/yjAdKVcw/f3330pKSlKrVq1StwkLC1NoaKh27txpej3+5kk9sbGxKlSokIKCgvxeT3x8vBYtWqQKFSqobNmyfqnnypUrevnll/XWW2+pVKlSPq/BXT2StHTpUjVv3lz333+/JkyYoCtXrvilnnPnzmnXrl0qUaKEunTpolatWumpp57S1q1b/VJPRn///bf27t1rWvtsVE+jRo20YsUKxcTEyG63a/ny5bp69aqaNWtmej2JiYmy2WzKmzdv6jb58uVTQECAz9vn5ORkLV++XPHx8WrUqJHf2+aM9fibJ/WY2Ta7q8fsttmoHn+2za5eH3+1zRnr8Xfb7O7zY3bbbFSPP9vmjPX4q20eNWqUbr311nTtsOS/c2dX9fiLp/WY0R6uW7dOdevW1QsvvKCWLVvq4Ycf1sKFC11ub+bv08TERP3www/q1KlT6t1z/vx+HT9+XFFRUenet8KFC6tBgwbasWOHz/d/IzL7mJ+Wv49nub3fcs+ePTpz5owCAgL08MMPq02bNurTp4/27dvnl3qKFy+uqlWrasmSJYqPj9e1a9e0YMEClShRQnXq1PFJHbml3zIjT9rhmJgYLV26VI0aNVKePHlyvE/f/0JAqqtXr2r8+PG67777VKhQIb/VsX79er300ku6cuWKSpUqpZkzZyokJMQvtcyYMUNBQUHq3r27X/afUdu2bXXHHXeoQoUKOnbsmD744AP17dtXCxYsUGBgoKm1nDt3TvHx8ZoxY4YGDRqkIUOGaOPGjXr++ec1Z84cU06QMrN48WIVLFhQd955p99qePPNN/Xmm2+qXbt2CgoKks1m0+jRo9W0aVPTa6lWrZpCQ0M1YcIEjRo1Svnz59cXX3yh06dPZ+v2w6yw2+0aM2aMIiIiVKNGDUnXDyh58uRxOhkpUaKEX+rxJ0/qOX/+vD7++GM98cQTfq1n3rx5Gj9+vOLj41W1alXNmjUr3Q9+M+sZO3asGjVqpNtvv92n+/e0nvvvv1+hoaEqXbq0IiMjNX78eB06dEhTp041vZ5jx45JkqZOnapXX31VtWrV0pIlS9SzZ08tW7ZMVapUMbWejL799luFhYUpIiLCZ3W4q2fSpEkaPHiwmjdvrqCgIAUHB2vq1KmqXLmy6fU0bNhQ+fPn17hx4/TSSy/J4XBowoQJSk5O9ll7GBkZqS5duujq1asqUKCAPvroI91yyy3au3evX9pmV/X4i6f1mNU2u6vH7LY5s3r80TZnVo8/2mZX9aR0PpvdNnv6eTarbc6sHn+0za7qCQkJMb1tXr58ufbs2aNvv/3WaZ0/zp0zq8cfPKnHzPbw2LFjmj9/vp5++mn169dPu3fv1ujRo5UnTx498sgjTtub+ft0zZo1unz5cro6/HXuIyn1M1qiRIl0y0uUKGHaPDE3Cn/8HsvIn781rNBvmfb1GTZsmMqXL69Zs2apW7duWrlypYoVK2ZqPTabTV988YUGDBigiIgIBQQEKCQkRJ999plPLvTMTf2WGWXWDo8bN07z5s3TlStX1LBhQ33yySde2SfBhkmSkpL04osvyuFwpE4u5S/NmzfXkiVLdOHCBS1cuFCDBg3SN99843QQ9LW///5bc+bM0aJFi0wZI9gT9913X+r/T5nw5vbbb09NQ82UMrZsx44d1bNnT0lSrVq1tH37dn399dd+Dza+++47PfDAAz6b78MTc+fO1c6dOzVt2jSFhoZq69atGjlypEqXLm36VU958uTRlClT9Prrr6tZs2YKDAxUy5Yt1a5dOzkcDp/ue+TIkdq/f7+++uorn+7HU1arJzY2Vs8++6zCwsJMGUc5s3oefPBBtW7dWlFRUfr88881aNAgzZ8/36ffM6N61q5dq02bNmnx4sU+229W6pGUrmMzPDxcpUqVUs+ePXX06FFVqlTJ1HpS2ucnnnhCnTp1kiTVrl1bf/zxh7777ju9/PLLptaTVkJCgpYtW5btyfO8Vc/kyZN16dIlffHFFypevLjWrFmjQYMGad68eT69S9SonpCQEE2ePFlvv/225s6dq4CAAN13332qU6eOz84/Uq7aunz5slauXKmhQ4fqyy+/9Mm+clKPv8INT+oxs212V4/ZbbOreo4cOeKXtjmz18cfbbOrevzVNnvyeTazbc6sHn+0ze7qMattPnXqlN59913NnDnTr79frF6Pme2hw+FQ3bp19dJLL0m6/n3ev3+/vv76a8Ngw8zfp999953atWunMmXKpC7z17kPvMsfv8cy8tfxzCr9limvT79+/VLncRg7dqzatWunn376SV26dDG1npTXq0SJEpo3b56Cg4P1zTffqF+/fvr222+9Ppdwbuq3zCizdrh379567LHHdPLkSU2dOlVDhw7Vp59+muNjPsGGCZKSkjRo0CCdPHlSs2fP9mvqKUkFChRQ5cqVVblyZTVs2FB33nmnvv32Wz377LOm1rF161adO3dOt912W+qy5ORkvf/++5ozZ47WrVtnaj1GKlasqOLFi+vIkSOmNxDFixdXUFCQwsLC0i0PCwszZWijzGzdulWHDh3SpEmT/FZDQkKCJk6cqKlTp6p9+/aSpJo1a2rv3r36/PPP/XI7d926dfX999/r8uXLSkpKUkhIiDp37qy6dev6bJ+jRo3Shg0b9OWXX6a7RbZkyZJKSkrSpUuX0l15du7cOZ8OXeGqHn9xV09sbKz69OmjggUL6qOPPvLKrZA5qadw4cIqXLiwqlSpogYNGqhZs2ZavXp1tidCy249mzZt0tGjR53ufho4cKCaNGmiuXPnmlqPkQYNGkiSjhw54rPOM1f1pHyHjNrnkydP+qSWzOpJ66efflJCQoIefvhhn9Xhrp6jR4/qyy+/1LJly1KHmaxZs6a2bt2qefPmadSoUabWI0lt2rTRmjVrdP78eQUFBalIkSJq3bq17r33Xp/Ukjdv3tQrNOvWravdu3drzpw5uueee/zSNruqx1fvRU7rMbttdleP2W2zq3ry5cvnl7Y5K58fM9pmV/X07dtXkvltsyevj5lts6t6+vTp45e2ObPXx8y2+Z9//tG5c+f06KOPpi5LTk7Wli1bNG/ePH3++eemts/u6tm9e7epV956Wo+Z7WGpUqWcvs/VqlXTypUrnbY18/fpiRMn9Pvvv2vKlCmpy/x17pMi5TN67ty5dJ2o586dU82aNX267xuN2cd8I/74rWGlfkuj1ydv3ryqWLGiTp06ZXo9mzZt0oYNG7Rly5bU161OnTr6/ffftWTJEj3zzDM+qSmFP/st03LXDoeEhCgkJERVq1ZVWFiYbr31Vu3cuTPHw+MSbPhYSuNw5MgRzZkzR8WLF/d3SU7sdnvq/ABmeuihh5w6n3v37q2HHnoo3QmVP50+fVoxMTGmj2EsXW+Y69Wrp0OHDqVbfvjwYZUvX970etL69ttvVadOHb+eJF27dk1JSUlO6W5gYKDP75Bwp3DhwpKuv1d///23XnzxRa/vw+Fw6J133tHq1as1d+5cVaxYMd36unXrKk+ePPrjjz9Sr2I4ePCgTp48qYYNG5pej9k8qSc2Nla9e/dW3rx5NW3aNJ9ehZPd18fhcPikfXZXzzPPPKPOnTunW/bAAw9o+PDh6cJos+oxsnfvXknySfvsrp4KFSqodOnShu1zu3btTK8nre+++04dOnTw6RCT7upJGV8/ICD9VG6+ap+z8vqkvC5//PGHzp07pw4dOni9HiMp51pmt83u6skt0tZjZtvsST1GfNU2u6tn4MCBprbN7uox4su22V09ZrfN7upJy4y22V09ZrfN7upJy4y2uUWLFlq6dGm6ZcOHD1e1atXUt29flStXztT22V09Zg8nkt16fNkeRkREePxb2Mzfp4sWLVKJEiVSL66TzD/3yahChQoqVaqU/vjjD9WqVUvS9ePprl271LVrV5/v/0Zm9jFfMv+3htX6LevWrau8efPq0KFDatKkiaTrf8OJEycUGhpqej0p3/+MfVM2my317hJf8me/ZVpZaYdTXhdvfLcsF2zExcXp6NGjqf8+fvy49u7dq6JFiyo0NFQxMTE6deqUzp49K0mpDUHJkiV98iZnVk+pUqX0wgsvaM+ePfr000/TjRdatGhRn4zTl1k9xYoV0yeffKIOHTqoVKlSunDhgubNm6czZ87o7rvv9not7uoJDQ11ajDz5MmjkiVLqlq1aqbXU7RoUU2dOlV33XWXSpYsqWPHjmncuHGqXLmy2rZta3o9oaGh6t27twYPHqymTZuqefPm2rhxo9avX685c+b4pR7p+gnSTz/9pKFDh/qkhqzU06xZM40bN07BwcEKDQ3Vli1btGTJEg0bNswv9axYsUIhISEKDQ1VZGSkxowZo9tvv90nk5mPHDlSy5Yt08cff6yCBQumti2FCxdWcHCwChcurE6dOum9995T0aJFVahQIY0ePVqNGjXyyY8zd/VI18d+jY6OTn0N9+3bp4IFC6pcuXJeHwfTXT2xsbHq1auXrly5onHjxik2NlaxsbGSrv+49vaPSXf1HDt2TD/++KNat26tkJAQnT59WtOnT1dwcLBuvfVWr9biST2lSpUyPGaGhob6JLRyV8/Ro0e1dOlS3XrrrSpWrJgiIyM1duxYNW3a1Cc/YN3VY7PZ1Lt3b02ZMkU1a9ZUrVq1tHjxYh08eFAffvih6fWkOHLkiLZs2aLp06d7vYas1FOtWjVVrlxZb731loYOHapixYppzZo1+u233/Tpp5+aXo90vVMxLCxMISEh2rFjh8aMGaOePXv65HxjwoQJateuncqVK6e4uDgtW7ZMmzdv1ueff2562+yuHsncttldPWa3ze7qMbttdleP2W2zu3rMbpvd1WN22+yunhRmtc3u6jG7bXZXj2Ru21yoUCGnuakKFCigYsWKpS43s332pB4z22d39fijPezRo4e6du2qTz75RPfcc4/++usvLVy40OnuBzN/n9rtdi1atEgPP/ywgoL+rzvNjO+Xu9+i3bt317Rp01S5cmVVqFBBkydPVunSpdPNyXTy5EldvHhRJ0+eVHJycmoYXalSJRUsWNCr9XjSN+fNz3hO6/HmZzyntXj7eOaNfkuzPjue9FsWKlRIXbp00ZQpU1SuXDmFhoamHley07eZ03oaNmyoIkWKaNiwYXruueeUL18+LVy4UCdOnEgXgHqjHk/7Lc38rkuZt8O7du3S7t271bhxYxUpUkRHjx7V5MmTValSpRzfrSFJNoe/L23Ooj///NNwoulHHnlE7733nhYtWqThw4c7rX/++ec1cOBAU+t5/vnn1bFjR8PHzZkzR82bNze1npEjR+rll1/Wrl27dOHCBRUrVkz16tVT//79Vb9+fa/X4q6e9957z2l5hw4d1L1799Q5Jcys5+2339Zzzz2nPXv26PLlyypdurRat26tF198USVLljS9npTX59tvv9X06dN1+vRpVa1aVQMHDvTZhJGe1LNgwQKNGTNGv/76a+qdCb7irp6oqCh98MEH+vXXX3Xx4kWFhobqiSeeUM+ePX0yNq+7eubMmaPPP/889Zb1hx56SAMGDPBJiOlqnNaxY8em3vF09epVvffee1q+fLkSExPVpk0bjRgxwichryf1TJkyxXAy0bTbmFWPq/dSuj6/RIUKFUyt58yZM3rjjTf0zz//6NKlSypRooSaNGmi5557zic/7j15v4we89FHH/mk/XFXz6lTp/TKK69o//79io+PV7ly5XT77bdrwIABPrlN2tPXZ/r06Zo3b54uXryomjVrasiQIalXDfmjng8++EA//PCD1q1b53TFoNn1HD58WBMmTNC2bdsUHx+vSpUqqVevXj4ZhsWTesaPH6/Fixfr4sWLKl++vLp06eKzY8Vrr72mTZs26ezZsypcuLDCw8PVt29ftW7dWpK5bbMn9ZjZNrurx+y22V09ZrfN7uox4su22V09ZrfN7upJYVbb7Gk9ZrXNntRjZtvsST1mts1GunXrppo1a+r111+XZH777K4es9vnzOrxR3soXZ+w94MPPtDhw4dVoUIFPf3003r88cfTbWPm79Nff/1VvXv31k8//aSqVaumW+fr75e736IOh0MffvihFi5cqEuXLqlx48YaMWJEujqHDRtmOE9TdvqnvNE3583PeE7r8eZn3Fv9lt46nnmj39Ksz46n/ZZJSUn64IMP9P333yshIUENGjTQa6+9ljoUnNn17N69W5MmTdLff/+tpKQkVa9eXQMGDMhW8OuNfkszv+tS5u1wZGSk3n33XUVGRio+Pl6lSpVS27ZtNWDAgHTzFGWX5YINAAAAAAAAAABw8/LtJSMAAAAAAAAAAABeRLABAAAAAAAAAAAsg2ADAAAAAAAAAABYBsEGAAAAAAAAAACwDIINAAAAAAAAAABgGQQbAAAAAAAAAADAMgg2AAAAAAAAAACAZRBsAAAAAAAAAAAAyyDYAAAAAAAAAAAAlkGwAQAAAAAAAAAALINgAwAAAAAAAAAAWMb/B4epDpLjmdBgAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['text_len'] < df['text_len'].quantile(0.995)]"
      ],
      "metadata": {
        "id": "Yx4uFQtBgNf_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = np.max(df['text_len'])\n",
        "max_len"
      ],
      "metadata": {
        "id": "sv5RTF8ggS7a",
        "outputId": "0f30a84a-99ab-4119-907d-bd4da56ef418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})"
      ],
      "metadata": {
        "id": "QSjqH8-0gs9X",
        "outputId": "f3f4543c-b083-4d3c-b4fa-b7357282d42d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-5cd3204e082e>:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})\n",
            "<ipython-input-22-5cd3204e082e>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text tokenization"
      ],
      "metadata": {
        "id": "oSjlhVvdEqEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenize(column, seq_len):\n",
        "    ##Create vocabulary of words from column\n",
        "    corpus = [word for text in column for word in text.split()]\n",
        "    count_words = Counter(corpus)\n",
        "    sorted_words = count_words.most_common()\n",
        "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
        "\n",
        "    ##Tokenize the columns text using the vocabulary\n",
        "    text_int = []\n",
        "    for text in column:\n",
        "        r = [vocab_to_int[word] for word in text.split()]\n",
        "        text_int.append(r)\n",
        "    ##Add padding to tokens\n",
        "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
        "    for i, review in enumerate(text_int):\n",
        "        if len(review) <= seq_len:\n",
        "            zeros = list(np.zeros(seq_len - len(review)))\n",
        "            new = zeros + review\n",
        "        else:\n",
        "            new = review[: seq_len]\n",
        "        features[i, :] = np.array(new)\n",
        "\n",
        "    return sorted_words, features"
      ],
      "metadata": {
        "id": "nPIz-dF6kFEH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary, tokenized_column = Tokenize(df[\"text_clean\"], max_len)"
      ],
      "metadata": {
        "id": "KqUTb1b8kd9P"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['text_clean']\n",
        "y = df['sentiment']"
      ],
      "metadata": {
        "id": "d02_zwPL-9EZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "2azevoNY-y6g"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text embedding with pre-trained Word2vec model"
      ],
      "metadata": {
        "id": "idKd2ISaEy9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Word2vec_train_data = list(map(lambda x: x.split(), X_train))"
      ],
      "metadata": {
        "id": "_TdbROTk7nYE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 200"
      ],
      "metadata": {
        "id": "EefOe-5Y7rSm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "fSmBTU0Y7vMO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding"
      ],
      "metadata": {
        "id": "IZyohOul7yPO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an empty embedding matrix of shape (VOCAB_SIZE, EMBEDDING_DIM)\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "# Fill the embedding matrix with pre-trained values from word2vec\n",
        "for word, token in vocabulary:\n",
        "    # Check if the word is present in the word2vec model's vocabulary\n",
        "    if word in word2vec_model.wv.key_to_index:\n",
        "        # If the word is present, retrieve its embedding vector and add it to the embedding matrix\n",
        "        embedding_vector = word2vec_model.wv[word]\n",
        "        embedding_matrix[token] = embedding_vector\n",
        "\n",
        "# Print the shape of the embedding matrix\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "nx5AIcgi68QL",
        "outputId": "e9dfa4bd-1882-45f8-ff58-91f3520d74f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Matrix Shape: (37569, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tokenized_column\n",
        "y = df['sentiment'].values"
      ],
      "metadata": {
        "id": "tfO4cnjz7_Ot"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ],
      "metadata": {
        "id": "EOAc1Vm38Njv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling"
      ],
      "metadata": {
        "id": "Ly-3aQBnFRcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ros = RandomOverSampler()\n",
        "X_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train))"
      ],
      "metadata": {
        "id": "achdE0dQ8WEO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_os.shape"
      ],
      "metadata": {
        "id": "sac738LjIWXP",
        "outputId": "3163b1c9-bd05-464e-ae17-f1c95fcf9563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31330, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define our preprocessed dataset and loaders"
      ],
      "metadata": {
        "id": "dGYWBOQ0FW2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(X_train_os), torch.from_numpy(y_train_os))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ],
      "metadata": {
        "id": "jyEGOm9Z8ffa"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "WjpbNrLb8jN2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)"
      ],
      "metadata": {
        "id": "mr-o8hPq8lXW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the classical model"
      ],
      "metadata": {
        "id": "g2V1uSsNFp3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this paragraph we will implement model with classical convolution and LSTM in order to compare it with its hybrid-quantum analogue"
      ],
      "metadata": {
        "id": "660eM2fEF30E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        # The attention linear layer which transforms the input data to the hidden space\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim )\n",
        "        # The linear layer that calculates the attention scores\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "        # Concatenate the last two hidden states in case of a bidirectional LSTM\n",
        "        hidden = hidden[-1]\n",
        "        # Repeat the hidden state across the sequence length\n",
        "        hidden_repeated = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "        # Calculate attention weights\n",
        "        attn_weights = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs), dim=2)))\n",
        "        # Compute attention scores\n",
        "        attn_weights = self.v(attn_weights).squeeze(2)\n",
        "        # Apply softmax to get valid probabilities\n",
        "        return nn.functional.softmax(attn_weights, dim=1)"
      ],
      "metadata": {
        "id": "-VTp5AVlJA3l"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassicalModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_classes, lstm_layers):\n",
        "        super(ClassicalModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = lstm_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, lstm_layers, batch_first=True)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Transform words to embeddings\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded = self.conv(embedded.unsqueeze(1))\n",
        "        # # Pass embeddings to LSTM\n",
        "        # out, hidden = self.lstm(embedded.reshape(-1, embedded.size(1)*embedded.size(2), embedded.size(-1)), hidden)\n",
        "        out, hidden = self.lstm(embedded, hidden)\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(hidden[0], out)\n",
        "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
        "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
        "        # Classify the context vector\n",
        "        out = self.softmax(self.fc(context))\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return h0, c0"
      ],
      "metadata": {
        "id": "raK4JVSHExXS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of the hybrid-quantum model"
      ],
      "metadata": {
        "id": "iFolBcU3GjzL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Zx9MSDbdkO5y"
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface='torch', diff_method=\"adjoint\")\n",
        "def quanvcirc(patch, weights, wires=range(num_qubits)):\n",
        "    # Angle embedding of the patch (reshape to match expected size)\n",
        "    qml.AngleEmbedding(patch, wires=wires, rotation='Y')\n",
        "\n",
        "    # Apply RX rotations based on the weights\n",
        "    qml.RX(weights[0][0], wires=0)\n",
        "    qml.RX(weights[0][1], wires=1)\n",
        "    qml.RY(weights[1][0], wires=2)\n",
        "    qml.RY(weights[1][1], wires=3)\n",
        "\n",
        "    # Apply CNOT gates\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[1, 2])\n",
        "    qml.CNOT(wires=[2, 3])\n",
        "    qml.CNOT(wires=[3, 0])\n",
        "    # Apply RY rotations based on the weights\n",
        "\n",
        "\n",
        "    # Return the expectation values of Pauli-Z measurements on all qubits\n",
        "    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qml.draw(quanvcirc)(np.ones((1, 4)), np.ones((2, 2))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G1L7z_enlfZ",
        "outputId": "d7db3472-c709-4feb-c287-5a4d7c5e493b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ─╭AngleEmbedding(M0)──RX(1.00)─╭●───────╭X─┤  <Z>\n",
            "1: ─├AngleEmbedding(M0)──RX(1.00)─╰X─╭●────│──┤     \n",
            "2: ─├AngleEmbedding(M0)──RY(1.00)────╰X─╭●─│──┤     \n",
            "3: ─╰AngleEmbedding(M0)──RY(1.00)───────╰X─╰●─┤     \n",
            "\n",
            "M0 = \n",
            "[[1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QuanConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(QuanConv2D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        # Define weights and biases as trainable parameters\n",
        "        self.weights = nn.Parameter(\n",
        "            torch.randn(out_channels, in_channels, kernel_size, kernel_size), requires_grad = True\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Extract dimensions\n",
        "        batch_size, in_channels, input_height, input_width = input.shape\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        output_height = (input_height - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "        output_width = (input_width - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "\n",
        "        # Initialize output tensor\n",
        "        output = torch.zeros(batch_size, self.out_channels, output_height, output_width)\n",
        "\n",
        "        # If padding is required, add it to the input\n",
        "        if self.padding > 0:\n",
        "            input = F.pad(input, (self.padding, self.padding, self.padding, self.padding))\n",
        "\n",
        "        # Perform convolution\n",
        "        for b in range(batch_size):\n",
        "            for c_out in range(self.out_channels):\n",
        "                for h in range(output_height):\n",
        "                    for w in range(output_width):\n",
        "                        h_start = h * self.stride\n",
        "                        h_end = h_start + self.kernel_size\n",
        "                        w_start = w * self.stride\n",
        "                        w_end = w_start + self.kernel_size\n",
        "\n",
        "                        # Slice the input for the current window\n",
        "                        input_slice = input[b, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Perform element-wise multiplication and sum with bias\n",
        "                        output[b, c_out, h, w] = quanvcirc(input_slice.reshape(1, self.kernel_size * self.kernel_size), self.weights[c_out].squeeze(0))\n",
        "        return output"
      ],
      "metadata": {
        "id": "ikYNnTA1pr6A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qdi_reps1 = 3\n",
        "qdi_reps2 = 3\n",
        "qdi_depth = 1"
      ],
      "metadata": {
        "id": "ZXzmC0KuyEDb"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev, interface='torch', diff_method=\"adjoint\")\n",
        "def qdi_circuit(weights, input_array, wires=range(num_qubits)):\n",
        "    for r in range(qdi_reps1):\n",
        "        for i in range(len(wires)):\n",
        "            qml.RX(weights[r][i], wires=wires[i])\n",
        "        for j in range(len(wires)-1):\n",
        "            qml.CNOT(wires=[wires[j], wires[j+1]])\n",
        "        qml.CNOT(wires=[wires[len(wires)-1], wires[0]])\n",
        "        qml.Barrier()\n",
        "    for d in range(qdi_depth):\n",
        "        qml.AngleEmbedding(input_array, wires=range(num_qubits), rotation='Z')\n",
        "        for r in range(qdi_reps2):\n",
        "            for i in range(len(wires)):\n",
        "                qml.RX(weights[qdi_reps1+d*r][i], wires=wires[i])\n",
        "            for j in range(len(wires)-1):\n",
        "                qml.CNOT(wires=[wires[j], wires[j+1]])\n",
        "            qml.CNOT(wires=[wires[len(wires)-1], wires[0]])\n",
        "            qml.Barrier()\n",
        "        qml.Barrier()\n",
        "    return [qml.expval(qml.PauliY(w)) for w in wires]"
      ],
      "metadata": {
        "id": "QixHflZxxt-Y"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qml.draw(qdi_circuit)(np.ones((qdi_reps1 + qdi_reps2, num_qubits)), np.ones((4, 4))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRFx50H6RkcR",
        "outputId": "95fdf837-c86c-4d65-9c90-93a8efde136c"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: ──RX(1.00)─╭●───────╭X──||──RX(1.00)─╭●───────╭X──||──RX(1.00)─╭●───────╭X──||\n",
            "1: ──RX(1.00)─╰X─╭●────│───||──RX(1.00)─╰X─╭●────│───||──RX(1.00)─╰X─╭●────│───||\n",
            "2: ──RX(1.00)────╰X─╭●─│───||──RX(1.00)────╰X─╭●─│───||──RX(1.00)────╰X─╭●─│───||\n",
            "3: ──RX(1.00)───────╰X─╰●──||──RX(1.00)───────╰X─╰●──||──RX(1.00)───────╰X─╰●──||\n",
            "\n",
            "──╭AngleEmbedding(M0)──RX(1.00)─╭●───────╭X──||──RX(1.00)─╭●───────╭X──||──RX(1.00)─╭●───────╭X──||\n",
            "──├AngleEmbedding(M0)──RX(1.00)─╰X─╭●────│───||──RX(1.00)─╰X─╭●────│───||──RX(1.00)─╰X─╭●────│───||\n",
            "──├AngleEmbedding(M0)──RX(1.00)────╰X─╭●─│───||──RX(1.00)────╰X─╭●─│───||──RX(1.00)────╰X─╭●─│───||\n",
            "──╰AngleEmbedding(M0)──RX(1.00)───────╰X─╰●──||──RX(1.00)───────╰X─╰●──||──RX(1.00)───────╰X─╰●──||\n",
            "\n",
            "───||─┤  <Y>\n",
            "───||─┤  <Y>\n",
            "───||─┤  <Y>\n",
            "───||─┤  <Y>\n",
            "\n",
            "M0 = \n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HQLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_qubits):\n",
        "        super(HQLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Combined weights for efficiency\n",
        "        self.W_input = nn.Parameter(torch.randn(4 * num_qubits, input_size), requires_grad = True).float()\n",
        "        self.W_hid = nn.Parameter(torch.randn(4 * num_qubits, hidden_size), requires_grad = True).float()\n",
        "        self.W_quan = nn.Parameter(torch.randn(4, qdi_reps1 + qdi_reps2, num_qubits), requires_grad = True).float()\n",
        "        self.W = nn.Parameter(torch.randn(4, hidden_size, num_qubits), requires_grad = True).float()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        h_prev, c_prev = hidden\n",
        "\n",
        "        yield_input = F.linear(x, self.W_input)\n",
        "        yield_hidden = F.linear(h_prev, self.W_hid)\n",
        "        # Concatenate input and previous hidden state\n",
        "        combined = yield_input + yield_hidden\n",
        "        # combined = torch.cat((yield_input, yield_hidden), dim=1)\n",
        "\n",
        "        # Apply linear transformation\n",
        "\n",
        "        # Split into gates\n",
        "        # combined = torch.cat([torch.stack(qdi_circuit(self.W_quan[i], combined[:, i:i*num_qubits].reshape(-1, num_qubits))) for i in range(num_qubits)], dim = 0)\n",
        "        i_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[0], combined[:, :num_qubits].reshape(-1, num_qubits))).T.float(), self.W[0])\n",
        "        f_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[1], combined[:, num_qubits:2*num_qubits].reshape(-1, num_qubits))).T.float(), self.W[1])\n",
        "        g_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[2], combined[:, 2*num_qubits:3*num_qubits].reshape(-1, num_qubits))).T.float(), self.W[2])\n",
        "        o_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[3], combined[:, 3*num_qubits:4*num_qubits].reshape(-1, num_qubits))).T.float(), self.W[3])\n",
        "\n",
        "\n",
        "\n",
        "        # combined = torch.cat((i_gate, f_gate, g_gate, o_gate), dim=0).float()\n",
        "        # gates = F.linear(combined, self.W)\n",
        "\n",
        "        # # Split into gates\n",
        "        # i_gate, f_gate, g_gate, o_gate = gates.chunk(4, 0)\n",
        "\n",
        "        # Apply non-linearities\n",
        "        i_gate = torch.sigmoid(i_gate)\n",
        "        f_gate = torch.sigmoid(f_gate)\n",
        "        g_gate = torch.tanh(g_gate)\n",
        "        o_gate = torch.sigmoid(o_gate)\n",
        "\n",
        "        # Update cell state\n",
        "        c_next = (f_gate * c_prev) + (i_gate * g_gate)\n",
        "\n",
        "        # Update hidden state\n",
        "        h_next = o_gate * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "\n",
        "class HQLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_qubits, num_layers=1):\n",
        "        super(HQLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm_cells = nn.ModuleList([HQLSTMCell(input_size if l==0 else hidden_size, hidden_size, num_qubits) for l in range(num_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        if hidden is None:\n",
        "             h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "             c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        else:\n",
        "            h0, c0 = hidden\n",
        "\n",
        "        output_seq = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            input_t = x[:, t, :] # input at current timestep\n",
        "            # print(f'step:{t}/{seq_len}, hidden:')\n",
        "\n",
        "            new_h = []\n",
        "            new_c = []\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "\n",
        "                # Get hidden state for current layer\n",
        "                h_t, c_t = self.lstm_cells[layer](input_t, (h0[layer], c0[layer]))\n",
        "\n",
        "                # Update hidden states for next timestep, for current layer\n",
        "                new_h.append(h_t)\n",
        "                new_c.append(c_t)\n",
        "                input_t = h_t # The output of current layer is input for the next\n",
        "\n",
        "            h0 = torch.stack(new_h)\n",
        "            c0 = torch.stack(new_c)\n",
        "            # Append hidden state at the topmost layer\n",
        "            output_seq.append(h_t.unsqueeze(1))\n",
        "\n",
        "        # Concatenate the outputs over the sequence length\n",
        "        output_seq = torch.cat(output_seq, dim=1) # output_seq is of shape (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        return output_seq, (h0, c0)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "v_mvdRnDqcze"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy hybrid-quantum model for benchmarking"
      ],
      "metadata": {
        "id": "py3iiomANCjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyHQModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_qubits, num_classes, lstm_layers):\n",
        "        super(ToyHQModel, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.num_layers = lstm_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        # self.conv = QuanConv2D(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
        "        self.lstm = HQLSTM(embedding_dim, hidden_dim, num_qubits, lstm_layers)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transform words to embeddings\n",
        "        embedded = self.embedding(x)\n",
        "        # x = self.conv(x.unsqueeze(1)).to(device)\n",
        "        # Pass embeddings to LSTM\n",
        "        out, hidden = self.lstm(embedded)\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(hidden[0], out)\n",
        "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
        "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
        "        # Classify the context vector\n",
        "        out = self.softmax(self.fc(context))\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return h0, c0"
      ],
      "metadata": {
        "id": "RRz02NuCLNvM"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridQuantumModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_qubits, num_classes, lstm_layers):\n",
        "        super(HybridQuantumModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv = QuanConv2D(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.lstm = HQLSTM(embedding_dim, hidden_dim, num_qubits, lstm_layers)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transform words to embeddings\n",
        "        embedded = self.embedding(x)\n",
        "        # embedded = self.conv(embedded.unsqueeze(1)).to(device)\n",
        "        # Pass embeddings to LSTM\n",
        "        # out, hidden = self.lstm(embedded.reshape(-1, embedded.size(1)*embedded.size(2), embedded.size(-1)))\n",
        "        out, hidden = self.lstm(embedded)\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(hidden[0], out)\n",
        "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
        "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
        "        # Classify the context vector\n",
        "        out = self.softmax(self.fc(context))\n",
        "        return out"
      ],
      "metadata": {
        "id": "0V4YMT0JN6Sy"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the models"
      ],
      "metadata": {
        "id": "uvla_k_6IWAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 5 #We are dealing with a multiclass classification of 5 classes\n",
        "HIDDEN_DIM = 100 #number of neurons of the internal state (internal neural network in the LSTM)\n",
        "LSTM_LAYERS = 1 #Number of stacked LSTM layers\n",
        "IN_CHANNELS = 1\n",
        "OUT_CHANNELS = 5\n",
        "KERNEL_SIZE = 2\n",
        "STRIDE = 1\n",
        "PADDING = 1\n",
        "\n",
        "\n",
        "LR = 8e-4 #Learning rate\n",
        "EPOCHS = 10 #Number of training epoch\n",
        "\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "bbH3jVkhSWCr"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classical_model = ClassicalModel(VOCAB_SIZE, EMBEDDING_DIM, IN_CHANNELS, OUT_CHANNELS, KERNEL_SIZE, STRIDE, PADDING, HIDDEN_DIM, NUM_CLASSES, LSTM_LAYERS)\n",
        "classical_model = classical_model.to(device)\n",
        "# classical_model = torch.compile(classical_model)\n",
        "\n",
        "cl_optimizer = torch.optim.AdamW(classical_model.parameters(), lr=LR, weight_decay = 5e-6)\n",
        "\n",
        "print(classical_model)"
      ],
      "metadata": {
        "id": "lJnLZ5GCMMNF",
        "outputId": "82e9989a-2d7f-4ed2-e72d-6b5c45276015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClassicalModel(\n",
            "  (embedding): Embedding(37569, 200)\n",
            "  (conv): Conv2d(1, 5, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (lstm): LSTM(200, 100, batch_first=True)\n",
            "  (attention): Attention(\n",
            "    (attn): Linear(in_features=200, out_features=100, bias=True)\n",
            "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid_quantum_model = HybridQuantumModel(VOCAB_SIZE, EMBEDDING_DIM, IN_CHANNELS, OUT_CHANNELS, KERNEL_SIZE, STRIDE, PADDING, HIDDEN_DIM, num_qubits, NUM_CLASSES, LSTM_LAYERS)\n",
        "hybrid_quantum_model = hybrid_quantum_model.to(device)\n",
        "# hybrid_quantum_model = torch.compile(hybrid_quantum_model)\n",
        "\n",
        "hq_optimizer = torch.optim.AdamW(hybrid_quantum_model.parameters(), lr=LR, weight_decay = 5e-6)\n",
        "\n",
        "print(hybrid_quantum_model)"
      ],
      "metadata": {
        "id": "_yVpeUhBSeXi",
        "outputId": "09ca474a-a3f8-494e-c55c-d31951622225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HybridQuantumModel(\n",
            "  (embedding): Embedding(37569, 200)\n",
            "  (conv): QuanConv2D()\n",
            "  (lstm): HQLSTM(\n",
            "    (lstm_cells): ModuleList(\n",
            "      (0): HQLSTMCell()\n",
            "    )\n",
            "  )\n",
            "  (attention): Attention(\n",
            "    (attn): Linear(in_features=200, out_features=100, bias=True)\n",
            "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "ObgTsuW6kO5z"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(model, criterion, optimizer,\n",
        "          train_dataloader, test_dataloader, num_epochs):\n",
        "\n",
        "    train_losses = np.zeros(num_epochs)\n",
        "    test_losses = np.zeros(num_epochs)\n",
        "\n",
        "    train_accuracy_arr = np.zeros(num_epochs)\n",
        "    test_accuracy_arr = np.zeros(num_epochs)\n",
        "\n",
        "    for i_epoch in tqdm(range(num_epochs)):\n",
        "        it = 0\n",
        "        train_loss = 0\n",
        "        test_loss = 0\n",
        "\n",
        "        train_accuracy = 0\n",
        "        test_accuracy = 0\n",
        "\n",
        "        # train step\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            start_time = time.time()\n",
        "            X = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            h = model.init_hidden(y.size(0))\n",
        "            # model forward-pass\n",
        "            preds, h = model(X, h)\n",
        "\n",
        "            # model backward-pass\n",
        "            optimizer.zero_grad() # t.grad = torch.tensor([0., 0., 0.])\n",
        "            loss = criterion(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step() # t = t - lr * t.grad\n",
        "            end_time = time.time()\n",
        "            execution_time = end_time - start_time\n",
        "\n",
        "            # save loss and accuracy\n",
        "            train_loss += loss.detach().cpu().numpy()\n",
        "            print(f'batch: {it+1}/{len(train_dataloader)}, loss: {train_loss/(it+1):.4f}, time: {execution_time:.4f}')\n",
        "            it += 1\n",
        "            train_accuracy += (preds.argmax(-1).detach() == y).cpu().numpy().mean()\n",
        "\n",
        "\n",
        "        train_loss /= len(train_dataloader)\n",
        "        train_accuracy /= len(train_dataloader)\n",
        "        train_losses[i_epoch] = train_loss\n",
        "        train_accuracy_arr[i_epoch] = train_accuracy\n",
        "\n",
        "        # test step\n",
        "        model.eval()\n",
        "        for batch in test_dataloader:\n",
        "            X = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            h = model.init_hidden(y.size(0))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # model forward-pass\n",
        "                preds, h = model(X, h)\n",
        "                loss = criterion(preds, y)\n",
        "\n",
        "                # save loss and accuracy\n",
        "                test_loss += loss.detach().cpu().numpy()\n",
        "                test_accuracy += (preds.argmax(-1) == y).cpu().numpy().mean()\n",
        "\n",
        "\n",
        "        test_loss /= len(test_dataloader)\n",
        "        test_accuracy /= len(test_dataloader)\n",
        "\n",
        "        test_losses[i_epoch] = test_loss\n",
        "        test_accuracy_arr[i_epoch] = test_accuracy\n",
        "\n",
        "    return train_losses, test_losses, train_accuracy_arr, test_accuracy_arr\n",
        "\n",
        "def train_for_hqnn(model, criterion, optimizer,\n",
        "          train_dataloader, test_dataloader, num_epochs):\n",
        "\n",
        "    train_losses = np.zeros(num_epochs)\n",
        "    test_losses = np.zeros(num_epochs)\n",
        "\n",
        "    train_accuracy_arr = np.zeros(num_epochs)\n",
        "    test_accuracy_arr = np.zeros(num_epochs)\n",
        "\n",
        "    for i_epoch in tqdm(range(num_epochs)):\n",
        "        it = 0\n",
        "\n",
        "        train_loss = 0\n",
        "        test_loss = 0\n",
        "\n",
        "        train_accuracy = 0\n",
        "        test_accuracy = 0\n",
        "\n",
        "        # train step\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            start_time = time.time()\n",
        "            X = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            # model forward-pass\n",
        "            preds = model(X)\n",
        "\n",
        "            # model backward-pass\n",
        "            optimizer.zero_grad() # t.grad = torch.tensor([0., 0., 0.])\n",
        "            loss = criterion(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step() # t = t - lr * t.grad\n",
        "            end_time = time.time()\n",
        "            execution_time = end_time - start_time\n",
        "\n",
        "            # save loss and accuracy\n",
        "            train_loss += loss.detach().cpu().numpy()\n",
        "            print(f'it: {it+1}/{len(train_dataloader)}, loss: {train_loss/(it+1):.4f}, time: {execution_time:.4f}')\n",
        "            it += 1\n",
        "            train_accuracy += (preds.argmax(-1).detach() == y).cpu().numpy().mean()\n",
        "\n",
        "\n",
        "        train_loss /= len(train_dataloader)\n",
        "        train_accuracy /= len(train_dataloader)\n",
        "        train_losses[i_epoch] = train_loss\n",
        "\n",
        "        train_accuracy_arr[i_epoch] = train_accuracy\n",
        "\n",
        "        # test step\n",
        "        model.eval()\n",
        "        for batch in test_dataloader:\n",
        "            X = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # model forward-pass\n",
        "                preds = model(X)\n",
        "                loss = criterion(preds, y)\n",
        "\n",
        "                # save loss and accuracy\n",
        "                test_loss += loss.detach().cpu().numpy()\n",
        "                test_accuracy += (preds.argmax(-1) == y).cpu().numpy().mean()\n",
        "\n",
        "\n",
        "        test_loss /= len(test_dataloader)\n",
        "        test_accuracy /= len(test_dataloader)\n",
        "\n",
        "        test_losses[i_epoch] = test_loss\n",
        "        test_accuracy_arr[i_epoch] = test_accuracy\n",
        "\n",
        "    return train_losses, test_losses, train_accuracy_arr, test_accuracy_arr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl_train_losses, \\\n",
        "    cl_test_losses, \\\n",
        "    cl_train_accuracy_arr, \\\n",
        "    cl_test_accuracy_arr = train(classical_model, criterion=criterion,\n",
        "                              optimizer=cl_optimizer,\n",
        "                              train_dataloader=train_loader,\n",
        "                              test_dataloader=test_loader,\n",
        "                              num_epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bbe2733aa8a944c1bd1a26e4dfa962db",
            "73ea96ba473d410aa4ada890ba0d9f21",
            "5006cfd87b7249d5a4831b38980a26eb",
            "76b318d61b674a7aad44fd4259561ddf",
            "89fee5e8b5e24462a52ac6a56f44b4bc",
            "e736b0c8fb9445fe892042b4afecd39d",
            "4498d76a83a94e2d846903a0142d900a",
            "151693e862e245d78cc828c4b5286141",
            "62ff2611cef243cfb95716cd49353050",
            "f17bb282bb90488f9f71675c2e9b9073",
            "6b328e39265244abb9300804ff9f6069"
          ]
        },
        "id": "1swJ9EmxZKAT",
        "outputId": "a6dff013-f914-4347-ebcb-9dd2c7beae81",
        "collapsed": true
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bbe2733aa8a944c1bd1a26e4dfa962db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "batch: 875/979, loss: 0.0168, time: 0.0028\n",
            "batch: 876/979, loss: 0.0168, time: 0.0044\n",
            "batch: 877/979, loss: 0.0168, time: 0.0041\n",
            "batch: 878/979, loss: 0.0168, time: 0.0030\n",
            "batch: 879/979, loss: 0.0170, time: 0.0029\n",
            "batch: 880/979, loss: 0.0170, time: 0.0031\n",
            "batch: 881/979, loss: 0.0170, time: 0.0029\n",
            "batch: 882/979, loss: 0.0170, time: 0.0028\n",
            "batch: 883/979, loss: 0.0170, time: 0.0028\n",
            "batch: 884/979, loss: 0.0170, time: 0.0029\n",
            "batch: 885/979, loss: 0.0170, time: 0.0029\n",
            "batch: 886/979, loss: 0.0170, time: 0.0030\n",
            "batch: 887/979, loss: 0.0170, time: 0.0029\n",
            "batch: 888/979, loss: 0.0170, time: 0.0029\n",
            "batch: 889/979, loss: 0.0170, time: 0.0029\n",
            "batch: 890/979, loss: 0.0169, time: 0.0029\n",
            "batch: 891/979, loss: 0.0170, time: 0.0029\n",
            "batch: 892/979, loss: 0.0170, time: 0.0030\n",
            "batch: 893/979, loss: 0.0169, time: 0.0032\n",
            "batch: 894/979, loss: 0.0170, time: 0.0036\n",
            "batch: 895/979, loss: 0.0170, time: 0.0036\n",
            "batch: 896/979, loss: 0.0170, time: 0.0036\n",
            "batch: 897/979, loss: 0.0171, time: 0.0038\n",
            "batch: 898/979, loss: 0.0171, time: 0.0029\n",
            "batch: 899/979, loss: 0.0172, time: 0.0029\n",
            "batch: 900/979, loss: 0.0171, time: 0.0029\n",
            "batch: 901/979, loss: 0.0171, time: 0.0029\n",
            "batch: 902/979, loss: 0.0171, time: 0.0028\n",
            "batch: 903/979, loss: 0.0171, time: 0.0032\n",
            "batch: 904/979, loss: 0.0171, time: 0.0029\n",
            "batch: 905/979, loss: 0.0171, time: 0.0029\n",
            "batch: 906/979, loss: 0.0171, time: 0.0029\n",
            "batch: 907/979, loss: 0.0171, time: 0.0029\n",
            "batch: 908/979, loss: 0.0172, time: 0.0030\n",
            "batch: 909/979, loss: 0.0172, time: 0.0042\n",
            "batch: 910/979, loss: 0.0171, time: 0.0034\n",
            "batch: 911/979, loss: 0.0171, time: 0.0028\n",
            "batch: 912/979, loss: 0.0171, time: 0.0029\n",
            "batch: 913/979, loss: 0.0171, time: 0.0029\n",
            "batch: 914/979, loss: 0.0171, time: 0.0028\n",
            "batch: 915/979, loss: 0.0172, time: 0.0028\n",
            "batch: 916/979, loss: 0.0172, time: 0.0030\n",
            "batch: 917/979, loss: 0.0171, time: 0.0029\n",
            "batch: 918/979, loss: 0.0171, time: 0.0028\n",
            "batch: 919/979, loss: 0.0174, time: 0.0029\n",
            "batch: 920/979, loss: 0.0174, time: 0.0028\n",
            "batch: 921/979, loss: 0.0174, time: 0.0028\n",
            "batch: 922/979, loss: 0.0174, time: 0.0028\n",
            "batch: 923/979, loss: 0.0174, time: 0.0029\n",
            "batch: 924/979, loss: 0.0173, time: 0.0029\n",
            "batch: 925/979, loss: 0.0174, time: 0.0028\n",
            "batch: 926/979, loss: 0.0173, time: 0.0029\n",
            "batch: 927/979, loss: 0.0173, time: 0.0028\n",
            "batch: 928/979, loss: 0.0173, time: 0.0029\n",
            "batch: 929/979, loss: 0.0173, time: 0.0028\n",
            "batch: 930/979, loss: 0.0173, time: 0.0028\n",
            "batch: 931/979, loss: 0.0173, time: 0.0028\n",
            "batch: 932/979, loss: 0.0173, time: 0.0029\n",
            "batch: 933/979, loss: 0.0173, time: 0.0048\n",
            "batch: 934/979, loss: 0.0173, time: 0.0029\n",
            "batch: 935/979, loss: 0.0175, time: 0.0029\n",
            "batch: 936/979, loss: 0.0176, time: 0.0029\n",
            "batch: 937/979, loss: 0.0176, time: 0.0029\n",
            "batch: 938/979, loss: 0.0176, time: 0.0029\n",
            "batch: 939/979, loss: 0.0176, time: 0.0033\n",
            "batch: 940/979, loss: 0.0177, time: 0.0034\n",
            "batch: 941/979, loss: 0.0177, time: 0.0059\n",
            "batch: 942/979, loss: 0.0177, time: 0.0049\n",
            "batch: 943/979, loss: 0.0177, time: 0.0033\n",
            "batch: 944/979, loss: 0.0177, time: 0.0030\n",
            "batch: 945/979, loss: 0.0176, time: 0.0030\n",
            "batch: 946/979, loss: 0.0177, time: 0.0029\n",
            "batch: 947/979, loss: 0.0177, time: 0.0029\n",
            "batch: 948/979, loss: 0.0178, time: 0.0030\n",
            "batch: 949/979, loss: 0.0178, time: 0.0028\n",
            "batch: 950/979, loss: 0.0178, time: 0.0028\n",
            "batch: 951/979, loss: 0.0177, time: 0.0029\n",
            "batch: 952/979, loss: 0.0177, time: 0.0029\n",
            "batch: 953/979, loss: 0.0178, time: 0.0029\n",
            "batch: 954/979, loss: 0.0178, time: 0.0029\n",
            "batch: 955/979, loss: 0.0178, time: 0.0028\n",
            "batch: 956/979, loss: 0.0178, time: 0.0029\n",
            "batch: 957/979, loss: 0.0179, time: 0.0028\n",
            "batch: 958/979, loss: 0.0179, time: 0.0029\n",
            "batch: 959/979, loss: 0.0179, time: 0.0029\n",
            "batch: 960/979, loss: 0.0178, time: 0.0029\n",
            "batch: 961/979, loss: 0.0178, time: 0.0028\n",
            "batch: 962/979, loss: 0.0178, time: 0.0028\n",
            "batch: 963/979, loss: 0.0179, time: 0.0029\n",
            "batch: 964/979, loss: 0.0179, time: 0.0029\n",
            "batch: 965/979, loss: 0.0179, time: 0.0032\n",
            "batch: 966/979, loss: 0.0179, time: 0.0030\n",
            "batch: 967/979, loss: 0.0179, time: 0.0029\n",
            "batch: 968/979, loss: 0.0179, time: 0.0030\n",
            "batch: 969/979, loss: 0.0180, time: 0.0029\n",
            "batch: 970/979, loss: 0.0180, time: 0.0045\n",
            "batch: 971/979, loss: 0.0180, time: 0.0044\n",
            "batch: 972/979, loss: 0.0181, time: 0.0045\n",
            "batch: 973/979, loss: 0.0180, time: 0.0055\n",
            "batch: 974/979, loss: 0.0180, time: 0.0034\n",
            "batch: 975/979, loss: 0.0181, time: 0.0031\n",
            "batch: 976/979, loss: 0.0181, time: 0.0029\n",
            "batch: 977/979, loss: 0.0180, time: 0.0028\n",
            "batch: 978/979, loss: 0.0180, time: 0.0028\n",
            "batch: 979/979, loss: 0.0180, time: 0.0028\n",
            "batch: 1/979, loss: 0.0095, time: 0.0045\n",
            "batch: 2/979, loss: 0.0062, time: 0.0030\n",
            "batch: 3/979, loss: 0.0117, time: 0.0029\n",
            "batch: 4/979, loss: 0.0100, time: 0.0030\n",
            "batch: 5/979, loss: 0.0098, time: 0.0028\n",
            "batch: 6/979, loss: 0.0360, time: 0.0028\n",
            "batch: 7/979, loss: 0.0314, time: 0.0048\n",
            "batch: 8/979, loss: 0.0288, time: 0.0036\n",
            "batch: 9/979, loss: 0.0321, time: 0.0031\n",
            "batch: 10/979, loss: 0.0298, time: 0.0029\n",
            "batch: 11/979, loss: 0.0273, time: 0.0029\n",
            "batch: 12/979, loss: 0.0253, time: 0.0029\n",
            "batch: 13/979, loss: 0.0237, time: 0.0029\n",
            "batch: 14/979, loss: 0.0224, time: 0.0029\n",
            "batch: 15/979, loss: 0.0211, time: 0.0028\n",
            "batch: 16/979, loss: 0.0202, time: 0.0029\n",
            "batch: 17/979, loss: 0.0191, time: 0.0029\n",
            "batch: 18/979, loss: 0.0186, time: 0.0030\n",
            "batch: 19/979, loss: 0.0182, time: 0.0029\n",
            "batch: 20/979, loss: 0.0176, time: 0.0029\n",
            "batch: 21/979, loss: 0.0172, time: 0.0032\n",
            "batch: 22/979, loss: 0.0166, time: 0.0038\n",
            "batch: 23/979, loss: 0.0161, time: 0.0038\n",
            "batch: 24/979, loss: 0.0155, time: 0.0039\n",
            "batch: 25/979, loss: 0.0150, time: 0.0033\n",
            "batch: 26/979, loss: 0.0145, time: 0.0029\n",
            "batch: 27/979, loss: 0.0145, time: 0.0029\n",
            "batch: 28/979, loss: 0.0144, time: 0.0029\n",
            "batch: 29/979, loss: 0.0156, time: 0.0029\n",
            "batch: 30/979, loss: 0.0152, time: 0.0029\n",
            "batch: 31/979, loss: 0.0151, time: 0.0029\n",
            "batch: 32/979, loss: 0.0148, time: 0.0029\n",
            "batch: 33/979, loss: 0.0162, time: 0.0049\n",
            "batch: 34/979, loss: 0.0177, time: 0.0030\n",
            "batch: 35/979, loss: 0.0172, time: 0.0029\n",
            "batch: 36/979, loss: 0.0168, time: 0.0030\n",
            "batch: 37/979, loss: 0.0164, time: 0.0029\n",
            "batch: 38/979, loss: 0.0161, time: 0.0029\n",
            "batch: 39/979, loss: 0.0158, time: 0.0029\n",
            "batch: 40/979, loss: 0.0159, time: 0.0028\n",
            "batch: 41/979, loss: 0.0156, time: 0.0029\n",
            "batch: 42/979, loss: 0.0153, time: 0.0029\n",
            "batch: 43/979, loss: 0.0150, time: 0.0028\n",
            "batch: 44/979, loss: 0.0148, time: 0.0028\n",
            "batch: 45/979, loss: 0.0148, time: 0.0029\n",
            "batch: 46/979, loss: 0.0145, time: 0.0028\n",
            "batch: 47/979, loss: 0.0142, time: 0.0028\n",
            "batch: 48/979, loss: 0.0140, time: 0.0028\n",
            "batch: 49/979, loss: 0.0143, time: 0.0028\n",
            "batch: 50/979, loss: 0.0141, time: 0.0028\n",
            "batch: 51/979, loss: 0.0138, time: 0.0028\n",
            "batch: 52/979, loss: 0.0139, time: 0.0028\n",
            "batch: 53/979, loss: 0.0137, time: 0.0028\n",
            "batch: 54/979, loss: 0.0136, time: 0.0028\n",
            "batch: 55/979, loss: 0.0136, time: 0.0029\n",
            "batch: 56/979, loss: 0.0136, time: 0.0029\n",
            "batch: 57/979, loss: 0.0143, time: 0.0028\n",
            "batch: 58/979, loss: 0.0141, time: 0.0030\n",
            "batch: 59/979, loss: 0.0142, time: 0.0029\n",
            "batch: 60/979, loss: 0.0140, time: 0.0028\n",
            "batch: 61/979, loss: 0.0138, time: 0.0037\n",
            "batch: 62/979, loss: 0.0136, time: 0.0029\n",
            "batch: 63/979, loss: 0.0135, time: 0.0029\n",
            "batch: 64/979, loss: 0.0137, time: 0.0048\n",
            "batch: 65/979, loss: 0.0135, time: 0.0044\n",
            "batch: 66/979, loss: 0.0141, time: 0.0031\n",
            "batch: 67/979, loss: 0.0139, time: 0.0029\n",
            "batch: 68/979, loss: 0.0138, time: 0.0031\n",
            "batch: 69/979, loss: 0.0137, time: 0.0029\n",
            "batch: 70/979, loss: 0.0135, time: 0.0029\n",
            "batch: 71/979, loss: 0.0134, time: 0.0029\n",
            "batch: 72/979, loss: 0.0132, time: 0.0029\n",
            "batch: 73/979, loss: 0.0131, time: 0.0028\n",
            "batch: 74/979, loss: 0.0130, time: 0.0029\n",
            "batch: 75/979, loss: 0.0133, time: 0.0029\n",
            "batch: 76/979, loss: 0.0132, time: 0.0029\n",
            "batch: 77/979, loss: 0.0131, time: 0.0029\n",
            "batch: 78/979, loss: 0.0129, time: 0.0028\n",
            "batch: 79/979, loss: 0.0129, time: 0.0029\n",
            "batch: 80/979, loss: 0.0129, time: 0.0028\n",
            "batch: 81/979, loss: 0.0135, time: 0.0028\n",
            "batch: 82/979, loss: 0.0133, time: 0.0030\n",
            "batch: 83/979, loss: 0.0132, time: 0.0029\n",
            "batch: 84/979, loss: 0.0131, time: 0.0029\n",
            "batch: 85/979, loss: 0.0130, time: 0.0029\n",
            "batch: 86/979, loss: 0.0129, time: 0.0029\n",
            "batch: 87/979, loss: 0.0129, time: 0.0028\n",
            "batch: 88/979, loss: 0.0128, time: 0.0028\n",
            "batch: 89/979, loss: 0.0129, time: 0.0028\n",
            "batch: 90/979, loss: 0.0127, time: 0.0028\n",
            "batch: 91/979, loss: 0.0126, time: 0.0029\n",
            "batch: 92/979, loss: 0.0125, time: 0.0029\n",
            "batch: 93/979, loss: 0.0125, time: 0.0031\n",
            "batch: 94/979, loss: 0.0127, time: 0.0029\n",
            "batch: 95/979, loss: 0.0128, time: 0.0029\n",
            "batch: 96/979, loss: 0.0127, time: 0.0028\n",
            "batch: 97/979, loss: 0.0126, time: 0.0061\n",
            "batch: 98/979, loss: 0.0125, time: 0.0040\n",
            "batch: 99/979, loss: 0.0124, time: 0.0028\n",
            "batch: 100/979, loss: 0.0123, time: 0.0029\n",
            "batch: 101/979, loss: 0.0122, time: 0.0029\n",
            "batch: 102/979, loss: 0.0122, time: 0.0066\n",
            "batch: 103/979, loss: 0.0121, time: 0.0041\n",
            "batch: 104/979, loss: 0.0120, time: 0.0043\n",
            "batch: 105/979, loss: 0.0119, time: 0.0030\n",
            "batch: 106/979, loss: 0.0119, time: 0.0029\n",
            "batch: 107/979, loss: 0.0119, time: 0.0028\n",
            "batch: 108/979, loss: 0.0118, time: 0.0029\n",
            "batch: 109/979, loss: 0.0117, time: 0.0029\n",
            "batch: 110/979, loss: 0.0116, time: 0.0029\n",
            "batch: 111/979, loss: 0.0115, time: 0.0029\n",
            "batch: 112/979, loss: 0.0114, time: 0.0031\n",
            "batch: 113/979, loss: 0.0114, time: 0.0030\n",
            "batch: 114/979, loss: 0.0114, time: 0.0029\n",
            "batch: 115/979, loss: 0.0113, time: 0.0029\n",
            "batch: 116/979, loss: 0.0112, time: 0.0030\n",
            "batch: 117/979, loss: 0.0111, time: 0.0028\n",
            "batch: 118/979, loss: 0.0111, time: 0.0028\n",
            "batch: 119/979, loss: 0.0110, time: 0.0030\n",
            "batch: 120/979, loss: 0.0109, time: 0.0029\n",
            "batch: 121/979, loss: 0.0109, time: 0.0030\n",
            "batch: 122/979, loss: 0.0109, time: 0.0029\n",
            "batch: 123/979, loss: 0.0108, time: 0.0029\n",
            "batch: 124/979, loss: 0.0108, time: 0.0029\n",
            "batch: 125/979, loss: 0.0107, time: 0.0030\n",
            "batch: 126/979, loss: 0.0107, time: 0.0029\n",
            "batch: 127/979, loss: 0.0106, time: 0.0051\n",
            "batch: 128/979, loss: 0.0106, time: 0.0036\n",
            "batch: 129/979, loss: 0.0105, time: 0.0029\n",
            "batch: 130/979, loss: 0.0105, time: 0.0030\n",
            "batch: 131/979, loss: 0.0105, time: 0.0031\n",
            "batch: 132/979, loss: 0.0104, time: 0.0034\n",
            "batch: 133/979, loss: 0.0104, time: 0.0030\n",
            "batch: 134/979, loss: 0.0104, time: 0.0030\n",
            "batch: 135/979, loss: 0.0104, time: 0.0030\n",
            "batch: 136/979, loss: 0.0103, time: 0.0030\n",
            "batch: 137/979, loss: 0.0103, time: 0.0031\n",
            "batch: 138/979, loss: 0.0103, time: 0.0030\n",
            "batch: 139/979, loss: 0.0103, time: 0.0030\n",
            "batch: 140/979, loss: 0.0102, time: 0.0030\n",
            "batch: 141/979, loss: 0.0102, time: 0.0030\n",
            "batch: 142/979, loss: 0.0102, time: 0.0030\n",
            "batch: 143/979, loss: 0.0101, time: 0.0030\n",
            "batch: 144/979, loss: 0.0101, time: 0.0030\n",
            "batch: 145/979, loss: 0.0101, time: 0.0031\n",
            "batch: 146/979, loss: 0.0101, time: 0.0028\n",
            "batch: 147/979, loss: 0.0100, time: 0.0029\n",
            "batch: 148/979, loss: 0.0100, time: 0.0029\n",
            "batch: 149/979, loss: 0.0099, time: 0.0030\n",
            "batch: 150/979, loss: 0.0099, time: 0.0029\n",
            "batch: 151/979, loss: 0.0099, time: 0.0028\n",
            "batch: 152/979, loss: 0.0101, time: 0.0028\n",
            "batch: 153/979, loss: 0.0100, time: 0.0028\n",
            "batch: 154/979, loss: 0.0100, time: 0.0028\n",
            "batch: 155/979, loss: 0.0101, time: 0.0028\n",
            "batch: 156/979, loss: 0.0101, time: 0.0029\n",
            "batch: 157/979, loss: 0.0100, time: 0.0028\n",
            "batch: 158/979, loss: 0.0100, time: 0.0028\n",
            "batch: 159/979, loss: 0.0099, time: 0.0042\n",
            "batch: 160/979, loss: 0.0099, time: 0.0046\n",
            "batch: 161/979, loss: 0.0098, time: 0.0042\n",
            "batch: 162/979, loss: 0.0098, time: 0.0029\n",
            "batch: 163/979, loss: 0.0097, time: 0.0028\n",
            "batch: 164/979, loss: 0.0097, time: 0.0028\n",
            "batch: 165/979, loss: 0.0096, time: 0.0029\n",
            "batch: 166/979, loss: 0.0096, time: 0.0029\n",
            "batch: 167/979, loss: 0.0096, time: 0.0030\n",
            "batch: 168/979, loss: 0.0097, time: 0.0029\n",
            "batch: 169/979, loss: 0.0097, time: 0.0028\n",
            "batch: 170/979, loss: 0.0097, time: 0.0029\n",
            "batch: 171/979, loss: 0.0097, time: 0.0029\n",
            "batch: 172/979, loss: 0.0096, time: 0.0029\n",
            "batch: 173/979, loss: 0.0096, time: 0.0029\n",
            "batch: 174/979, loss: 0.0095, time: 0.0029\n",
            "batch: 175/979, loss: 0.0098, time: 0.0037\n",
            "batch: 176/979, loss: 0.0097, time: 0.0036\n",
            "batch: 177/979, loss: 0.0099, time: 0.0032\n",
            "batch: 178/979, loss: 0.0098, time: 0.0029\n",
            "batch: 179/979, loss: 0.0098, time: 0.0028\n",
            "batch: 180/979, loss: 0.0103, time: 0.0028\n",
            "batch: 181/979, loss: 0.0102, time: 0.0029\n",
            "batch: 182/979, loss: 0.0102, time: 0.0029\n",
            "batch: 183/979, loss: 0.0101, time: 0.0028\n",
            "batch: 184/979, loss: 0.0101, time: 0.0029\n",
            "batch: 185/979, loss: 0.0101, time: 0.0029\n",
            "batch: 186/979, loss: 0.0101, time: 0.0029\n",
            "batch: 187/979, loss: 0.0100, time: 0.0028\n",
            "batch: 188/979, loss: 0.0100, time: 0.0028\n",
            "batch: 189/979, loss: 0.0100, time: 0.0029\n",
            "batch: 190/979, loss: 0.0101, time: 0.0029\n",
            "batch: 191/979, loss: 0.0101, time: 0.0061\n",
            "batch: 192/979, loss: 0.0101, time: 0.0036\n",
            "batch: 193/979, loss: 0.0100, time: 0.0029\n",
            "batch: 194/979, loss: 0.0100, time: 0.0028\n",
            "batch: 195/979, loss: 0.0100, time: 0.0030\n",
            "batch: 196/979, loss: 0.0099, time: 0.0029\n",
            "batch: 197/979, loss: 0.0099, time: 0.0029\n",
            "batch: 198/979, loss: 0.0099, time: 0.0029\n",
            "batch: 199/979, loss: 0.0099, time: 0.0029\n",
            "batch: 200/979, loss: 0.0098, time: 0.0029\n",
            "batch: 201/979, loss: 0.0098, time: 0.0028\n",
            "batch: 202/979, loss: 0.0098, time: 0.0029\n",
            "batch: 203/979, loss: 0.0098, time: 0.0029\n",
            "batch: 204/979, loss: 0.0098, time: 0.0029\n",
            "batch: 205/979, loss: 0.0099, time: 0.0028\n",
            "batch: 206/979, loss: 0.0099, time: 0.0030\n",
            "batch: 207/979, loss: 0.0099, time: 0.0029\n",
            "batch: 208/979, loss: 0.0098, time: 0.0029\n",
            "batch: 209/979, loss: 0.0098, time: 0.0028\n",
            "batch: 210/979, loss: 0.0099, time: 0.0028\n",
            "batch: 211/979, loss: 0.0098, time: 0.0031\n",
            "batch: 212/979, loss: 0.0098, time: 0.0028\n",
            "batch: 213/979, loss: 0.0098, time: 0.0028\n",
            "batch: 214/979, loss: 0.0098, time: 0.0028\n",
            "batch: 215/979, loss: 0.0097, time: 0.0029\n",
            "batch: 216/979, loss: 0.0098, time: 0.0036\n",
            "batch: 217/979, loss: 0.0098, time: 0.0028\n",
            "batch: 218/979, loss: 0.0098, time: 0.0028\n",
            "batch: 219/979, loss: 0.0098, time: 0.0028\n",
            "batch: 220/979, loss: 0.0097, time: 0.0029\n",
            "batch: 221/979, loss: 0.0097, time: 0.0028\n",
            "batch: 222/979, loss: 0.0097, time: 0.0028\n",
            "batch: 223/979, loss: 0.0097, time: 0.0028\n",
            "batch: 224/979, loss: 0.0096, time: 0.0046\n",
            "batch: 225/979, loss: 0.0096, time: 0.0029\n",
            "batch: 226/979, loss: 0.0096, time: 0.0036\n",
            "batch: 227/979, loss: 0.0100, time: 0.0029\n",
            "batch: 228/979, loss: 0.0100, time: 0.0029\n",
            "batch: 229/979, loss: 0.0100, time: 0.0029\n",
            "batch: 230/979, loss: 0.0099, time: 0.0030\n",
            "batch: 231/979, loss: 0.0099, time: 0.0030\n",
            "batch: 232/979, loss: 0.0099, time: 0.0028\n",
            "batch: 233/979, loss: 0.0099, time: 0.0030\n",
            "batch: 234/979, loss: 0.0098, time: 0.0030\n",
            "batch: 235/979, loss: 0.0098, time: 0.0028\n",
            "batch: 236/979, loss: 0.0098, time: 0.0028\n",
            "batch: 237/979, loss: 0.0101, time: 0.0028\n",
            "batch: 238/979, loss: 0.0101, time: 0.0028\n",
            "batch: 239/979, loss: 0.0101, time: 0.0028\n",
            "batch: 240/979, loss: 0.0101, time: 0.0030\n",
            "batch: 241/979, loss: 0.0101, time: 0.0029\n",
            "batch: 242/979, loss: 0.0101, time: 0.0028\n",
            "batch: 243/979, loss: 0.0101, time: 0.0028\n",
            "batch: 244/979, loss: 0.0103, time: 0.0028\n",
            "batch: 245/979, loss: 0.0103, time: 0.0029\n",
            "batch: 246/979, loss: 0.0103, time: 0.0029\n",
            "batch: 247/979, loss: 0.0103, time: 0.0029\n",
            "batch: 248/979, loss: 0.0102, time: 0.0030\n",
            "batch: 249/979, loss: 0.0102, time: 0.0028\n",
            "batch: 250/979, loss: 0.0102, time: 0.0029\n",
            "batch: 251/979, loss: 0.0102, time: 0.0028\n",
            "batch: 252/979, loss: 0.0102, time: 0.0029\n",
            "batch: 253/979, loss: 0.0102, time: 0.0028\n",
            "batch: 254/979, loss: 0.0102, time: 0.0029\n",
            "batch: 255/979, loss: 0.0102, time: 0.0056\n",
            "batch: 256/979, loss: 0.0102, time: 0.0045\n",
            "batch: 257/979, loss: 0.0102, time: 0.0028\n",
            "batch: 258/979, loss: 0.0102, time: 0.0029\n",
            "batch: 259/979, loss: 0.0101, time: 0.0054\n",
            "batch: 260/979, loss: 0.0101, time: 0.0045\n",
            "batch: 261/979, loss: 0.0101, time: 0.0042\n",
            "batch: 262/979, loss: 0.0101, time: 0.0036\n",
            "batch: 263/979, loss: 0.0101, time: 0.0030\n",
            "batch: 264/979, loss: 0.0101, time: 0.0029\n",
            "batch: 265/979, loss: 0.0102, time: 0.0029\n",
            "batch: 266/979, loss: 0.0102, time: 0.0028\n",
            "batch: 267/979, loss: 0.0102, time: 0.0029\n",
            "batch: 268/979, loss: 0.0102, time: 0.0029\n",
            "batch: 269/979, loss: 0.0108, time: 0.0029\n",
            "batch: 270/979, loss: 0.0108, time: 0.0028\n",
            "batch: 271/979, loss: 0.0107, time: 0.0029\n",
            "batch: 272/979, loss: 0.0107, time: 0.0028\n",
            "batch: 273/979, loss: 0.0107, time: 0.0028\n",
            "batch: 274/979, loss: 0.0107, time: 0.0029\n",
            "batch: 275/979, loss: 0.0106, time: 0.0028\n",
            "batch: 276/979, loss: 0.0107, time: 0.0030\n",
            "batch: 277/979, loss: 0.0106, time: 0.0028\n",
            "batch: 278/979, loss: 0.0106, time: 0.0029\n",
            "batch: 279/979, loss: 0.0106, time: 0.0029\n",
            "batch: 280/979, loss: 0.0105, time: 0.0028\n",
            "batch: 281/979, loss: 0.0105, time: 0.0028\n",
            "batch: 282/979, loss: 0.0105, time: 0.0030\n",
            "batch: 283/979, loss: 0.0105, time: 0.0029\n",
            "batch: 284/979, loss: 0.0105, time: 0.0030\n",
            "batch: 285/979, loss: 0.0104, time: 0.0051\n",
            "batch: 286/979, loss: 0.0104, time: 0.0030\n",
            "batch: 287/979, loss: 0.0105, time: 0.0052\n",
            "batch: 288/979, loss: 0.0105, time: 0.0029\n",
            "batch: 289/979, loss: 0.0104, time: 0.0028\n",
            "batch: 290/979, loss: 0.0104, time: 0.0028\n",
            "batch: 291/979, loss: 0.0104, time: 0.0028\n",
            "batch: 292/979, loss: 0.0104, time: 0.0029\n",
            "batch: 293/979, loss: 0.0103, time: 0.0029\n",
            "batch: 294/979, loss: 0.0103, time: 0.0028\n",
            "batch: 295/979, loss: 0.0103, time: 0.0029\n",
            "batch: 296/979, loss: 0.0103, time: 0.0029\n",
            "batch: 297/979, loss: 0.0102, time: 0.0028\n",
            "batch: 298/979, loss: 0.0102, time: 0.0028\n",
            "batch: 299/979, loss: 0.0102, time: 0.0028\n",
            "batch: 300/979, loss: 0.0102, time: 0.0029\n",
            "batch: 301/979, loss: 0.0102, time: 0.0028\n",
            "batch: 302/979, loss: 0.0101, time: 0.0029\n",
            "batch: 303/979, loss: 0.0102, time: 0.0028\n",
            "batch: 304/979, loss: 0.0102, time: 0.0028\n",
            "batch: 305/979, loss: 0.0101, time: 0.0030\n",
            "batch: 306/979, loss: 0.0102, time: 0.0028\n",
            "batch: 307/979, loss: 0.0102, time: 0.0029\n",
            "batch: 308/979, loss: 0.0102, time: 0.0028\n",
            "batch: 309/979, loss: 0.0101, time: 0.0029\n",
            "batch: 310/979, loss: 0.0102, time: 0.0029\n",
            "batch: 311/979, loss: 0.0101, time: 0.0028\n",
            "batch: 312/979, loss: 0.0101, time: 0.0028\n",
            "batch: 313/979, loss: 0.0101, time: 0.0030\n",
            "batch: 314/979, loss: 0.0101, time: 0.0029\n",
            "batch: 315/979, loss: 0.0101, time: 0.0028\n",
            "batch: 316/979, loss: 0.0100, time: 0.0032\n",
            "batch: 317/979, loss: 0.0100, time: 0.0029\n",
            "batch: 318/979, loss: 0.0100, time: 0.0028\n",
            "batch: 319/979, loss: 0.0100, time: 0.0028\n",
            "batch: 320/979, loss: 0.0100, time: 0.0048\n",
            "batch: 321/979, loss: 0.0101, time: 0.0029\n",
            "batch: 322/979, loss: 0.0100, time: 0.0029\n",
            "batch: 323/979, loss: 0.0100, time: 0.0029\n",
            "batch: 324/979, loss: 0.0100, time: 0.0032\n",
            "batch: 325/979, loss: 0.0100, time: 0.0029\n",
            "batch: 326/979, loss: 0.0099, time: 0.0028\n",
            "batch: 327/979, loss: 0.0100, time: 0.0028\n",
            "batch: 328/979, loss: 0.0099, time: 0.0028\n",
            "batch: 329/979, loss: 0.0100, time: 0.0028\n",
            "batch: 330/979, loss: 0.0101, time: 0.0035\n",
            "batch: 331/979, loss: 0.0101, time: 0.0037\n",
            "batch: 332/979, loss: 0.0101, time: 0.0035\n",
            "batch: 333/979, loss: 0.0101, time: 0.0030\n",
            "batch: 334/979, loss: 0.0100, time: 0.0028\n",
            "batch: 335/979, loss: 0.0100, time: 0.0028\n",
            "batch: 336/979, loss: 0.0100, time: 0.0029\n",
            "batch: 337/979, loss: 0.0100, time: 0.0029\n",
            "batch: 338/979, loss: 0.0100, time: 0.0029\n",
            "batch: 339/979, loss: 0.0099, time: 0.0028\n",
            "batch: 340/979, loss: 0.0099, time: 0.0028\n",
            "batch: 341/979, loss: 0.0099, time: 0.0029\n",
            "batch: 342/979, loss: 0.0100, time: 0.0029\n",
            "batch: 343/979, loss: 0.0099, time: 0.0028\n",
            "batch: 344/979, loss: 0.0099, time: 0.0029\n",
            "batch: 345/979, loss: 0.0099, time: 0.0028\n",
            "batch: 346/979, loss: 0.0099, time: 0.0028\n",
            "batch: 347/979, loss: 0.0099, time: 0.0028\n",
            "batch: 348/979, loss: 0.0099, time: 0.0040\n",
            "batch: 349/979, loss: 0.0099, time: 0.0028\n",
            "batch: 350/979, loss: 0.0098, time: 0.0028\n",
            "batch: 351/979, loss: 0.0098, time: 0.0057\n",
            "batch: 352/979, loss: 0.0098, time: 0.0028\n",
            "batch: 353/979, loss: 0.0099, time: 0.0028\n",
            "batch: 354/979, loss: 0.0099, time: 0.0029\n",
            "batch: 355/979, loss: 0.0099, time: 0.0028\n",
            "batch: 356/979, loss: 0.0099, time: 0.0028\n",
            "batch: 357/979, loss: 0.0099, time: 0.0028\n",
            "batch: 358/979, loss: 0.0099, time: 0.0028\n",
            "batch: 359/979, loss: 0.0099, time: 0.0028\n",
            "batch: 360/979, loss: 0.0099, time: 0.0028\n",
            "batch: 361/979, loss: 0.0098, time: 0.0029\n",
            "batch: 362/979, loss: 0.0098, time: 0.0028\n",
            "batch: 363/979, loss: 0.0099, time: 0.0028\n",
            "batch: 364/979, loss: 0.0099, time: 0.0029\n",
            "batch: 365/979, loss: 0.0099, time: 0.0030\n",
            "batch: 366/979, loss: 0.0099, time: 0.0029\n",
            "batch: 367/979, loss: 0.0099, time: 0.0028\n",
            "batch: 368/979, loss: 0.0101, time: 0.0028\n",
            "batch: 369/979, loss: 0.0100, time: 0.0028\n",
            "batch: 370/979, loss: 0.0100, time: 0.0028\n",
            "batch: 371/979, loss: 0.0101, time: 0.0035\n",
            "batch: 372/979, loss: 0.0102, time: 0.0029\n",
            "batch: 373/979, loss: 0.0102, time: 0.0028\n",
            "batch: 374/979, loss: 0.0103, time: 0.0029\n",
            "batch: 375/979, loss: 0.0103, time: 0.0028\n",
            "batch: 376/979, loss: 0.0103, time: 0.0028\n",
            "batch: 377/979, loss: 0.0104, time: 0.0028\n",
            "batch: 378/979, loss: 0.0104, time: 0.0028\n",
            "batch: 379/979, loss: 0.0104, time: 0.0029\n",
            "batch: 380/979, loss: 0.0103, time: 0.0029\n",
            "batch: 381/979, loss: 0.0103, time: 0.0028\n",
            "batch: 382/979, loss: 0.0103, time: 0.0036\n",
            "batch: 383/979, loss: 0.0103, time: 0.0029\n",
            "batch: 384/979, loss: 0.0103, time: 0.0058\n",
            "batch: 385/979, loss: 0.0102, time: 0.0031\n",
            "batch: 386/979, loss: 0.0102, time: 0.0029\n",
            "batch: 387/979, loss: 0.0102, time: 0.0029\n",
            "batch: 388/979, loss: 0.0102, time: 0.0029\n",
            "batch: 389/979, loss: 0.0103, time: 0.0028\n",
            "batch: 390/979, loss: 0.0103, time: 0.0028\n",
            "batch: 391/979, loss: 0.0102, time: 0.0028\n",
            "batch: 392/979, loss: 0.0103, time: 0.0029\n",
            "batch: 393/979, loss: 0.0103, time: 0.0028\n",
            "batch: 394/979, loss: 0.0103, time: 0.0028\n",
            "batch: 395/979, loss: 0.0102, time: 0.0028\n",
            "batch: 396/979, loss: 0.0102, time: 0.0028\n",
            "batch: 397/979, loss: 0.0102, time: 0.0028\n",
            "batch: 398/979, loss: 0.0102, time: 0.0028\n",
            "batch: 399/979, loss: 0.0102, time: 0.0028\n",
            "batch: 400/979, loss: 0.0108, time: 0.0028\n",
            "batch: 401/979, loss: 0.0108, time: 0.0028\n",
            "batch: 402/979, loss: 0.0108, time: 0.0028\n",
            "batch: 403/979, loss: 0.0108, time: 0.0030\n",
            "batch: 404/979, loss: 0.0108, time: 0.0028\n",
            "batch: 405/979, loss: 0.0108, time: 0.0029\n",
            "batch: 406/979, loss: 0.0108, time: 0.0029\n",
            "batch: 407/979, loss: 0.0108, time: 0.0028\n",
            "batch: 408/979, loss: 0.0109, time: 0.0028\n",
            "batch: 409/979, loss: 0.0108, time: 0.0028\n",
            "batch: 410/979, loss: 0.0108, time: 0.0045\n",
            "batch: 411/979, loss: 0.0108, time: 0.0031\n",
            "batch: 412/979, loss: 0.0108, time: 0.0029\n",
            "batch: 413/979, loss: 0.0109, time: 0.0029\n",
            "batch: 414/979, loss: 0.0109, time: 0.0045\n",
            "batch: 415/979, loss: 0.0109, time: 0.0045\n",
            "batch: 416/979, loss: 0.0109, time: 0.0029\n",
            "batch: 417/979, loss: 0.0109, time: 0.0028\n",
            "batch: 418/979, loss: 0.0109, time: 0.0067\n",
            "batch: 419/979, loss: 0.0108, time: 0.0040\n",
            "batch: 420/979, loss: 0.0108, time: 0.0044\n",
            "batch: 421/979, loss: 0.0108, time: 0.0029\n",
            "batch: 422/979, loss: 0.0109, time: 0.0029\n",
            "batch: 423/979, loss: 0.0110, time: 0.0028\n",
            "batch: 424/979, loss: 0.0109, time: 0.0029\n",
            "batch: 425/979, loss: 0.0109, time: 0.0029\n",
            "batch: 426/979, loss: 0.0109, time: 0.0028\n",
            "batch: 427/979, loss: 0.0109, time: 0.0029\n",
            "batch: 428/979, loss: 0.0109, time: 0.0028\n",
            "batch: 429/979, loss: 0.0109, time: 0.0029\n",
            "batch: 430/979, loss: 0.0109, time: 0.0029\n",
            "batch: 431/979, loss: 0.0110, time: 0.0029\n",
            "batch: 432/979, loss: 0.0110, time: 0.0029\n",
            "batch: 433/979, loss: 0.0110, time: 0.0028\n",
            "batch: 434/979, loss: 0.0109, time: 0.0029\n",
            "batch: 435/979, loss: 0.0109, time: 0.0028\n",
            "batch: 436/979, loss: 0.0110, time: 0.0030\n",
            "batch: 437/979, loss: 0.0109, time: 0.0028\n",
            "batch: 438/979, loss: 0.0109, time: 0.0028\n",
            "batch: 439/979, loss: 0.0109, time: 0.0031\n",
            "batch: 440/979, loss: 0.0109, time: 0.0029\n",
            "batch: 441/979, loss: 0.0109, time: 0.0029\n",
            "batch: 442/979, loss: 0.0108, time: 0.0029\n",
            "batch: 443/979, loss: 0.0109, time: 0.0029\n",
            "batch: 444/979, loss: 0.0108, time: 0.0029\n",
            "batch: 445/979, loss: 0.0108, time: 0.0037\n",
            "batch: 446/979, loss: 0.0109, time: 0.0045\n",
            "batch: 447/979, loss: 0.0109, time: 0.0029\n",
            "batch: 448/979, loss: 0.0109, time: 0.0028\n",
            "batch: 449/979, loss: 0.0109, time: 0.0029\n",
            "batch: 450/979, loss: 0.0109, time: 0.0028\n",
            "batch: 451/979, loss: 0.0108, time: 0.0028\n",
            "batch: 452/979, loss: 0.0108, time: 0.0029\n",
            "batch: 453/979, loss: 0.0108, time: 0.0029\n",
            "batch: 454/979, loss: 0.0109, time: 0.0029\n",
            "batch: 455/979, loss: 0.0110, time: 0.0028\n",
            "batch: 456/979, loss: 0.0112, time: 0.0029\n",
            "batch: 457/979, loss: 0.0112, time: 0.0028\n",
            "batch: 458/979, loss: 0.0112, time: 0.0034\n",
            "batch: 459/979, loss: 0.0111, time: 0.0028\n",
            "batch: 460/979, loss: 0.0111, time: 0.0028\n",
            "batch: 461/979, loss: 0.0111, time: 0.0028\n",
            "batch: 462/979, loss: 0.0111, time: 0.0028\n",
            "batch: 463/979, loss: 0.0111, time: 0.0028\n",
            "batch: 464/979, loss: 0.0112, time: 0.0028\n",
            "batch: 465/979, loss: 0.0111, time: 0.0028\n",
            "batch: 466/979, loss: 0.0111, time: 0.0028\n",
            "batch: 467/979, loss: 0.0111, time: 0.0028\n",
            "batch: 468/979, loss: 0.0112, time: 0.0029\n",
            "batch: 469/979, loss: 0.0112, time: 0.0028\n",
            "batch: 470/979, loss: 0.0111, time: 0.0028\n",
            "batch: 471/979, loss: 0.0111, time: 0.0028\n",
            "batch: 472/979, loss: 0.0111, time: 0.0028\n",
            "batch: 473/979, loss: 0.0111, time: 0.0037\n",
            "batch: 474/979, loss: 0.0111, time: 0.0029\n",
            "batch: 475/979, loss: 0.0111, time: 0.0029\n",
            "batch: 476/979, loss: 0.0111, time: 0.0028\n",
            "batch: 477/979, loss: 0.0111, time: 0.0038\n",
            "batch: 478/979, loss: 0.0111, time: 0.0040\n",
            "batch: 479/979, loss: 0.0111, time: 0.0029\n",
            "batch: 480/979, loss: 0.0111, time: 0.0029\n",
            "batch: 481/979, loss: 0.0111, time: 0.0030\n",
            "batch: 482/979, loss: 0.0111, time: 0.0031\n",
            "batch: 483/979, loss: 0.0112, time: 0.0029\n",
            "batch: 484/979, loss: 0.0112, time: 0.0030\n",
            "batch: 485/979, loss: 0.0111, time: 0.0038\n",
            "batch: 486/979, loss: 0.0111, time: 0.0039\n",
            "batch: 487/979, loss: 0.0111, time: 0.0029\n",
            "batch: 488/979, loss: 0.0111, time: 0.0030\n",
            "batch: 489/979, loss: 0.0111, time: 0.0028\n",
            "batch: 490/979, loss: 0.0111, time: 0.0028\n",
            "batch: 491/979, loss: 0.0111, time: 0.0028\n",
            "batch: 492/979, loss: 0.0111, time: 0.0029\n",
            "batch: 493/979, loss: 0.0110, time: 0.0028\n",
            "batch: 494/979, loss: 0.0110, time: 0.0028\n",
            "batch: 495/979, loss: 0.0110, time: 0.0028\n",
            "batch: 496/979, loss: 0.0110, time: 0.0028\n",
            "batch: 497/979, loss: 0.0110, time: 0.0028\n",
            "batch: 498/979, loss: 0.0110, time: 0.0028\n",
            "batch: 499/979, loss: 0.0110, time: 0.0030\n",
            "batch: 500/979, loss: 0.0110, time: 0.0029\n",
            "batch: 501/979, loss: 0.0110, time: 0.0028\n",
            "batch: 502/979, loss: 0.0110, time: 0.0029\n",
            "batch: 503/979, loss: 0.0109, time: 0.0028\n",
            "batch: 504/979, loss: 0.0109, time: 0.0029\n",
            "batch: 505/979, loss: 0.0109, time: 0.0028\n",
            "batch: 506/979, loss: 0.0110, time: 0.0028\n",
            "batch: 507/979, loss: 0.0110, time: 0.0029\n",
            "batch: 508/979, loss: 0.0109, time: 0.0027\n",
            "batch: 509/979, loss: 0.0109, time: 0.0036\n",
            "batch: 510/979, loss: 0.0110, time: 0.0056\n",
            "batch: 511/979, loss: 0.0110, time: 0.0029\n",
            "batch: 512/979, loss: 0.0110, time: 0.0028\n",
            "batch: 513/979, loss: 0.0111, time: 0.0028\n",
            "batch: 514/979, loss: 0.0110, time: 0.0028\n",
            "batch: 515/979, loss: 0.0110, time: 0.0028\n",
            "batch: 516/979, loss: 0.0111, time: 0.0028\n",
            "batch: 517/979, loss: 0.0111, time: 0.0028\n",
            "batch: 518/979, loss: 0.0110, time: 0.0031\n",
            "batch: 519/979, loss: 0.0110, time: 0.0028\n",
            "batch: 520/979, loss: 0.0110, time: 0.0029\n",
            "batch: 521/979, loss: 0.0110, time: 0.0028\n",
            "batch: 522/979, loss: 0.0110, time: 0.0028\n",
            "batch: 523/979, loss: 0.0110, time: 0.0028\n",
            "batch: 524/979, loss: 0.0110, time: 0.0028\n",
            "batch: 525/979, loss: 0.0110, time: 0.0029\n",
            "batch: 526/979, loss: 0.0110, time: 0.0028\n",
            "batch: 527/979, loss: 0.0110, time: 0.0037\n",
            "batch: 528/979, loss: 0.0110, time: 0.0029\n",
            "batch: 529/979, loss: 0.0110, time: 0.0028\n",
            "batch: 530/979, loss: 0.0109, time: 0.0028\n",
            "batch: 531/979, loss: 0.0109, time: 0.0030\n",
            "batch: 532/979, loss: 0.0109, time: 0.0028\n",
            "batch: 533/979, loss: 0.0109, time: 0.0028\n",
            "batch: 534/979, loss: 0.0109, time: 0.0028\n",
            "batch: 535/979, loss: 0.0109, time: 0.0028\n",
            "batch: 536/979, loss: 0.0110, time: 0.0029\n",
            "batch: 537/979, loss: 0.0110, time: 0.0028\n",
            "batch: 538/979, loss: 0.0109, time: 0.0035\n",
            "batch: 539/979, loss: 0.0109, time: 0.0028\n",
            "batch: 540/979, loss: 0.0109, time: 0.0028\n",
            "batch: 541/979, loss: 0.0109, time: 0.0028\n",
            "batch: 542/979, loss: 0.0109, time: 0.0038\n",
            "batch: 543/979, loss: 0.0109, time: 0.0046\n",
            "batch: 544/979, loss: 0.0110, time: 0.0038\n",
            "batch: 545/979, loss: 0.0110, time: 0.0029\n",
            "batch: 546/979, loss: 0.0112, time: 0.0029\n",
            "batch: 547/979, loss: 0.0112, time: 0.0028\n",
            "batch: 548/979, loss: 0.0112, time: 0.0029\n",
            "batch: 549/979, loss: 0.0112, time: 0.0028\n",
            "batch: 550/979, loss: 0.0112, time: 0.0029\n",
            "batch: 551/979, loss: 0.0112, time: 0.0028\n",
            "batch: 552/979, loss: 0.0112, time: 0.0028\n",
            "batch: 553/979, loss: 0.0112, time: 0.0028\n",
            "batch: 554/979, loss: 0.0111, time: 0.0028\n",
            "batch: 555/979, loss: 0.0111, time: 0.0027\n",
            "batch: 556/979, loss: 0.0111, time: 0.0028\n",
            "batch: 557/979, loss: 0.0111, time: 0.0028\n",
            "batch: 558/979, loss: 0.0111, time: 0.0029\n",
            "batch: 559/979, loss: 0.0111, time: 0.0030\n",
            "batch: 560/979, loss: 0.0111, time: 0.0028\n",
            "batch: 561/979, loss: 0.0111, time: 0.0028\n",
            "batch: 562/979, loss: 0.0111, time: 0.0029\n",
            "batch: 563/979, loss: 0.0110, time: 0.0028\n",
            "batch: 564/979, loss: 0.0110, time: 0.0028\n",
            "batch: 565/979, loss: 0.0110, time: 0.0029\n",
            "batch: 566/979, loss: 0.0110, time: 0.0029\n",
            "batch: 567/979, loss: 0.0110, time: 0.0028\n",
            "batch: 568/979, loss: 0.0113, time: 0.0028\n",
            "batch: 569/979, loss: 0.0113, time: 0.0027\n",
            "batch: 570/979, loss: 0.0113, time: 0.0028\n",
            "batch: 571/979, loss: 0.0113, time: 0.0029\n",
            "batch: 572/979, loss: 0.0113, time: 0.0028\n",
            "batch: 573/979, loss: 0.0113, time: 0.0028\n",
            "batch: 574/979, loss: 0.0113, time: 0.0028\n",
            "batch: 575/979, loss: 0.0114, time: 0.0067\n",
            "batch: 576/979, loss: 0.0114, time: 0.0028\n",
            "batch: 577/979, loss: 0.0114, time: 0.0028\n",
            "batch: 578/979, loss: 0.0114, time: 0.0042\n",
            "batch: 579/979, loss: 0.0114, time: 0.0041\n",
            "batch: 580/979, loss: 0.0113, time: 0.0043\n",
            "batch: 581/979, loss: 0.0113, time: 0.0029\n",
            "batch: 582/979, loss: 0.0113, time: 0.0029\n",
            "batch: 583/979, loss: 0.0113, time: 0.0028\n",
            "batch: 584/979, loss: 0.0113, time: 0.0028\n",
            "batch: 585/979, loss: 0.0113, time: 0.0028\n",
            "batch: 586/979, loss: 0.0113, time: 0.0029\n",
            "batch: 587/979, loss: 0.0113, time: 0.0029\n",
            "batch: 588/979, loss: 0.0113, time: 0.0028\n",
            "batch: 589/979, loss: 0.0113, time: 0.0030\n",
            "batch: 590/979, loss: 0.0112, time: 0.0028\n",
            "batch: 591/979, loss: 0.0112, time: 0.0028\n",
            "batch: 592/979, loss: 0.0112, time: 0.0030\n",
            "batch: 593/979, loss: 0.0113, time: 0.0030\n",
            "batch: 594/979, loss: 0.0113, time: 0.0028\n",
            "batch: 595/979, loss: 0.0113, time: 0.0028\n",
            "batch: 596/979, loss: 0.0113, time: 0.0029\n",
            "batch: 597/979, loss: 0.0113, time: 0.0030\n",
            "batch: 598/979, loss: 0.0113, time: 0.0032\n",
            "batch: 599/979, loss: 0.0113, time: 0.0028\n",
            "batch: 600/979, loss: 0.0113, time: 0.0028\n",
            "batch: 601/979, loss: 0.0113, time: 0.0029\n",
            "batch: 602/979, loss: 0.0114, time: 0.0028\n",
            "batch: 603/979, loss: 0.0114, time: 0.0030\n",
            "batch: 604/979, loss: 0.0117, time: 0.0028\n",
            "batch: 605/979, loss: 0.0117, time: 0.0028\n",
            "batch: 606/979, loss: 0.0117, time: 0.0040\n",
            "batch: 607/979, loss: 0.0116, time: 0.0038\n",
            "batch: 608/979, loss: 0.0116, time: 0.0028\n",
            "batch: 609/979, loss: 0.0116, time: 0.0028\n",
            "batch: 610/979, loss: 0.0116, time: 0.0028\n",
            "batch: 611/979, loss: 0.0116, time: 0.0028\n",
            "batch: 612/979, loss: 0.0117, time: 0.0028\n",
            "batch: 613/979, loss: 0.0117, time: 0.0028\n",
            "batch: 614/979, loss: 0.0116, time: 0.0028\n",
            "batch: 615/979, loss: 0.0116, time: 0.0028\n",
            "batch: 616/979, loss: 0.0116, time: 0.0028\n",
            "batch: 617/979, loss: 0.0116, time: 0.0028\n",
            "batch: 618/979, loss: 0.0116, time: 0.0028\n",
            "batch: 619/979, loss: 0.0116, time: 0.0028\n",
            "batch: 620/979, loss: 0.0117, time: 0.0028\n",
            "batch: 621/979, loss: 0.0117, time: 0.0028\n",
            "batch: 622/979, loss: 0.0116, time: 0.0028\n",
            "batch: 623/979, loss: 0.0116, time: 0.0029\n",
            "batch: 624/979, loss: 0.0116, time: 0.0029\n",
            "batch: 625/979, loss: 0.0117, time: 0.0030\n",
            "batch: 626/979, loss: 0.0117, time: 0.0028\n",
            "batch: 627/979, loss: 0.0117, time: 0.0032\n",
            "batch: 628/979, loss: 0.0116, time: 0.0038\n",
            "batch: 629/979, loss: 0.0117, time: 0.0029\n",
            "batch: 630/979, loss: 0.0117, time: 0.0029\n",
            "batch: 631/979, loss: 0.0117, time: 0.0029\n",
            "batch: 632/979, loss: 0.0116, time: 0.0029\n",
            "batch: 633/979, loss: 0.0117, time: 0.0031\n",
            "batch: 634/979, loss: 0.0117, time: 0.0029\n",
            "batch: 635/979, loss: 0.0117, time: 0.0028\n",
            "batch: 636/979, loss: 0.0117, time: 0.0029\n",
            "batch: 637/979, loss: 0.0118, time: 0.0029\n",
            "batch: 638/979, loss: 0.0118, time: 0.0028\n",
            "batch: 639/979, loss: 0.0120, time: 0.0042\n",
            "batch: 640/979, loss: 0.0120, time: 0.0029\n",
            "batch: 641/979, loss: 0.0120, time: 0.0032\n",
            "batch: 642/979, loss: 0.0120, time: 0.0036\n",
            "batch: 643/979, loss: 0.0120, time: 0.0038\n",
            "batch: 644/979, loss: 0.0120, time: 0.0030\n",
            "batch: 645/979, loss: 0.0120, time: 0.0030\n",
            "batch: 646/979, loss: 0.0120, time: 0.0029\n",
            "batch: 647/979, loss: 0.0119, time: 0.0028\n",
            "batch: 648/979, loss: 0.0120, time: 0.0028\n",
            "batch: 649/979, loss: 0.0120, time: 0.0031\n",
            "batch: 650/979, loss: 0.0120, time: 0.0028\n",
            "batch: 651/979, loss: 0.0120, time: 0.0028\n",
            "batch: 652/979, loss: 0.0120, time: 0.0028\n",
            "batch: 653/979, loss: 0.0119, time: 0.0043\n",
            "batch: 654/979, loss: 0.0119, time: 0.0028\n",
            "batch: 655/979, loss: 0.0119, time: 0.0028\n",
            "batch: 656/979, loss: 0.0119, time: 0.0029\n",
            "batch: 657/979, loss: 0.0119, time: 0.0028\n",
            "batch: 658/979, loss: 0.0120, time: 0.0028\n",
            "batch: 659/979, loss: 0.0120, time: 0.0028\n",
            "batch: 660/979, loss: 0.0120, time: 0.0038\n",
            "batch: 661/979, loss: 0.0122, time: 0.0028\n",
            "batch: 662/979, loss: 0.0122, time: 0.0028\n",
            "batch: 663/979, loss: 0.0123, time: 0.0036\n",
            "batch: 664/979, loss: 0.0123, time: 0.0033\n",
            "batch: 665/979, loss: 0.0123, time: 0.0029\n",
            "batch: 666/979, loss: 0.0123, time: 0.0029\n",
            "batch: 667/979, loss: 0.0123, time: 0.0029\n",
            "batch: 668/979, loss: 0.0123, time: 0.0028\n",
            "batch: 669/979, loss: 0.0124, time: 0.0030\n",
            "batch: 670/979, loss: 0.0124, time: 0.0045\n",
            "batch: 671/979, loss: 0.0124, time: 0.0029\n",
            "batch: 672/979, loss: 0.0124, time: 0.0028\n",
            "batch: 673/979, loss: 0.0124, time: 0.0028\n",
            "batch: 674/979, loss: 0.0124, time: 0.0029\n",
            "batch: 675/979, loss: 0.0123, time: 0.0028\n",
            "batch: 676/979, loss: 0.0123, time: 0.0030\n",
            "batch: 677/979, loss: 0.0123, time: 0.0028\n",
            "batch: 678/979, loss: 0.0123, time: 0.0028\n",
            "batch: 679/979, loss: 0.0123, time: 0.0028\n",
            "batch: 680/979, loss: 0.0124, time: 0.0028\n",
            "batch: 681/979, loss: 0.0123, time: 0.0028\n",
            "batch: 682/979, loss: 0.0123, time: 0.0028\n",
            "batch: 683/979, loss: 0.0123, time: 0.0029\n",
            "batch: 684/979, loss: 0.0123, time: 0.0053\n",
            "batch: 685/979, loss: 0.0123, time: 0.0029\n",
            "batch: 686/979, loss: 0.0123, time: 0.0029\n",
            "batch: 687/979, loss: 0.0123, time: 0.0030\n",
            "batch: 688/979, loss: 0.0122, time: 0.0028\n",
            "batch: 689/979, loss: 0.0122, time: 0.0028\n",
            "batch: 690/979, loss: 0.0122, time: 0.0028\n",
            "batch: 691/979, loss: 0.0122, time: 0.0030\n",
            "batch: 692/979, loss: 0.0122, time: 0.0028\n",
            "batch: 693/979, loss: 0.0122, time: 0.0031\n",
            "batch: 694/979, loss: 0.0122, time: 0.0038\n",
            "batch: 695/979, loss: 0.0122, time: 0.0029\n",
            "batch: 696/979, loss: 0.0122, time: 0.0028\n",
            "batch: 697/979, loss: 0.0122, time: 0.0030\n",
            "batch: 698/979, loss: 0.0122, time: 0.0029\n",
            "batch: 699/979, loss: 0.0122, time: 0.0029\n",
            "batch: 700/979, loss: 0.0122, time: 0.0028\n",
            "batch: 701/979, loss: 0.0122, time: 0.0029\n",
            "batch: 702/979, loss: 0.0122, time: 0.0045\n",
            "batch: 703/979, loss: 0.0122, time: 0.0035\n",
            "batch: 704/979, loss: 0.0122, time: 0.0035\n",
            "batch: 705/979, loss: 0.0122, time: 0.0028\n",
            "batch: 706/979, loss: 0.0122, time: 0.0028\n",
            "batch: 707/979, loss: 0.0122, time: 0.0028\n",
            "batch: 708/979, loss: 0.0122, time: 0.0029\n",
            "batch: 709/979, loss: 0.0122, time: 0.0028\n",
            "batch: 710/979, loss: 0.0122, time: 0.0028\n",
            "batch: 711/979, loss: 0.0122, time: 0.0028\n",
            "batch: 712/979, loss: 0.0122, time: 0.0029\n",
            "batch: 713/979, loss: 0.0123, time: 0.0028\n",
            "batch: 714/979, loss: 0.0123, time: 0.0029\n",
            "batch: 715/979, loss: 0.0123, time: 0.0028\n",
            "batch: 716/979, loss: 0.0123, time: 0.0029\n",
            "batch: 717/979, loss: 0.0123, time: 0.0028\n",
            "batch: 718/979, loss: 0.0123, time: 0.0028\n",
            "batch: 719/979, loss: 0.0123, time: 0.0029\n",
            "batch: 720/979, loss: 0.0123, time: 0.0028\n",
            "batch: 721/979, loss: 0.0123, time: 0.0029\n",
            "batch: 722/979, loss: 0.0123, time: 0.0028\n",
            "batch: 723/979, loss: 0.0123, time: 0.0029\n",
            "batch: 724/979, loss: 0.0123, time: 0.0029\n",
            "batch: 725/979, loss: 0.0122, time: 0.0030\n",
            "batch: 726/979, loss: 0.0123, time: 0.0029\n",
            "batch: 727/979, loss: 0.0123, time: 0.0029\n",
            "batch: 728/979, loss: 0.0123, time: 0.0042\n",
            "batch: 729/979, loss: 0.0123, time: 0.0030\n",
            "batch: 730/979, loss: 0.0122, time: 0.0028\n",
            "batch: 731/979, loss: 0.0122, time: 0.0028\n",
            "batch: 732/979, loss: 0.0122, time: 0.0029\n",
            "batch: 733/979, loss: 0.0122, time: 0.0028\n",
            "batch: 734/979, loss: 0.0123, time: 0.0028\n",
            "batch: 735/979, loss: 0.0122, time: 0.0050\n",
            "batch: 736/979, loss: 0.0122, time: 0.0030\n",
            "batch: 737/979, loss: 0.0122, time: 0.0028\n",
            "batch: 738/979, loss: 0.0122, time: 0.0043\n",
            "batch: 739/979, loss: 0.0122, time: 0.0041\n",
            "batch: 740/979, loss: 0.0122, time: 0.0044\n",
            "batch: 741/979, loss: 0.0122, time: 0.0029\n",
            "batch: 742/979, loss: 0.0123, time: 0.0030\n",
            "batch: 743/979, loss: 0.0123, time: 0.0029\n",
            "batch: 744/979, loss: 0.0123, time: 0.0028\n",
            "batch: 745/979, loss: 0.0123, time: 0.0028\n",
            "batch: 746/979, loss: 0.0123, time: 0.0031\n",
            "batch: 747/979, loss: 0.0123, time: 0.0028\n",
            "batch: 748/979, loss: 0.0123, time: 0.0028\n",
            "batch: 749/979, loss: 0.0123, time: 0.0029\n",
            "batch: 750/979, loss: 0.0123, time: 0.0029\n",
            "batch: 751/979, loss: 0.0123, time: 0.0029\n",
            "batch: 752/979, loss: 0.0124, time: 0.0029\n",
            "batch: 753/979, loss: 0.0124, time: 0.0029\n",
            "batch: 754/979, loss: 0.0124, time: 0.0029\n",
            "batch: 755/979, loss: 0.0124, time: 0.0028\n",
            "batch: 756/979, loss: 0.0124, time: 0.0029\n",
            "batch: 757/979, loss: 0.0124, time: 0.0029\n",
            "batch: 758/979, loss: 0.0124, time: 0.0028\n",
            "batch: 759/979, loss: 0.0124, time: 0.0029\n",
            "batch: 760/979, loss: 0.0124, time: 0.0028\n",
            "batch: 761/979, loss: 0.0124, time: 0.0028\n",
            "batch: 762/979, loss: 0.0124, time: 0.0030\n",
            "batch: 763/979, loss: 0.0124, time: 0.0028\n",
            "batch: 764/979, loss: 0.0124, time: 0.0028\n",
            "batch: 765/979, loss: 0.0124, time: 0.0029\n",
            "batch: 766/979, loss: 0.0124, time: 0.0054\n",
            "batch: 767/979, loss: 0.0125, time: 0.0034\n",
            "batch: 768/979, loss: 0.0125, time: 0.0029\n",
            "batch: 769/979, loss: 0.0125, time: 0.0029\n",
            "batch: 770/979, loss: 0.0125, time: 0.0028\n",
            "batch: 771/979, loss: 0.0125, time: 0.0028\n",
            "batch: 772/979, loss: 0.0124, time: 0.0028\n",
            "batch: 773/979, loss: 0.0124, time: 0.0028\n",
            "batch: 774/979, loss: 0.0124, time: 0.0029\n",
            "batch: 775/979, loss: 0.0124, time: 0.0029\n",
            "batch: 776/979, loss: 0.0124, time: 0.0028\n",
            "batch: 777/979, loss: 0.0124, time: 0.0028\n",
            "batch: 778/979, loss: 0.0124, time: 0.0029\n",
            "batch: 779/979, loss: 0.0124, time: 0.0029\n",
            "batch: 780/979, loss: 0.0124, time: 0.0028\n",
            "batch: 781/979, loss: 0.0124, time: 0.0028\n",
            "batch: 782/979, loss: 0.0124, time: 0.0028\n",
            "batch: 783/979, loss: 0.0124, time: 0.0029\n",
            "batch: 784/979, loss: 0.0125, time: 0.0029\n",
            "batch: 785/979, loss: 0.0125, time: 0.0028\n",
            "batch: 786/979, loss: 0.0125, time: 0.0028\n",
            "batch: 787/979, loss: 0.0125, time: 0.0028\n",
            "batch: 788/979, loss: 0.0125, time: 0.0028\n",
            "batch: 789/979, loss: 0.0125, time: 0.0028\n",
            "batch: 790/979, loss: 0.0125, time: 0.0029\n",
            "batch: 791/979, loss: 0.0125, time: 0.0029\n",
            "batch: 792/979, loss: 0.0125, time: 0.0028\n",
            "batch: 793/979, loss: 0.0124, time: 0.0029\n",
            "batch: 794/979, loss: 0.0124, time: 0.0029\n",
            "batch: 795/979, loss: 0.0124, time: 0.0028\n",
            "batch: 796/979, loss: 0.0124, time: 0.0029\n",
            "batch: 797/979, loss: 0.0124, time: 0.0075\n",
            "batch: 798/979, loss: 0.0124, time: 0.0067\n",
            "batch: 799/979, loss: 0.0124, time: 0.0030\n",
            "batch: 800/979, loss: 0.0124, time: 0.0029\n",
            "batch: 801/979, loss: 0.0124, time: 0.0029\n",
            "batch: 802/979, loss: 0.0124, time: 0.0031\n",
            "batch: 803/979, loss: 0.0124, time: 0.0028\n",
            "batch: 804/979, loss: 0.0124, time: 0.0029\n",
            "batch: 805/979, loss: 0.0124, time: 0.0028\n",
            "batch: 806/979, loss: 0.0124, time: 0.0028\n",
            "batch: 807/979, loss: 0.0124, time: 0.0028\n",
            "batch: 808/979, loss: 0.0124, time: 0.0028\n",
            "batch: 809/979, loss: 0.0124, time: 0.0028\n",
            "batch: 810/979, loss: 0.0124, time: 0.0028\n",
            "batch: 811/979, loss: 0.0123, time: 0.0030\n",
            "batch: 812/979, loss: 0.0123, time: 0.0028\n",
            "batch: 813/979, loss: 0.0123, time: 0.0028\n",
            "batch: 814/979, loss: 0.0123, time: 0.0028\n",
            "batch: 815/979, loss: 0.0123, time: 0.0029\n",
            "batch: 816/979, loss: 0.0123, time: 0.0029\n",
            "batch: 817/979, loss: 0.0123, time: 0.0028\n",
            "batch: 818/979, loss: 0.0123, time: 0.0028\n",
            "batch: 819/979, loss: 0.0123, time: 0.0028\n",
            "batch: 820/979, loss: 0.0123, time: 0.0029\n",
            "batch: 821/979, loss: 0.0123, time: 0.0029\n",
            "batch: 822/979, loss: 0.0123, time: 0.0028\n",
            "batch: 823/979, loss: 0.0123, time: 0.0028\n",
            "batch: 824/979, loss: 0.0123, time: 0.0031\n",
            "batch: 825/979, loss: 0.0122, time: 0.0028\n",
            "batch: 826/979, loss: 0.0122, time: 0.0028\n",
            "batch: 827/979, loss: 0.0122, time: 0.0028\n",
            "batch: 828/979, loss: 0.0122, time: 0.0028\n",
            "batch: 829/979, loss: 0.0122, time: 0.0028\n",
            "batch: 830/979, loss: 0.0122, time: 0.0029\n",
            "batch: 831/979, loss: 0.0122, time: 0.0040\n",
            "batch: 832/979, loss: 0.0122, time: 0.0029\n",
            "batch: 833/979, loss: 0.0122, time: 0.0028\n",
            "batch: 834/979, loss: 0.0122, time: 0.0028\n",
            "batch: 835/979, loss: 0.0122, time: 0.0028\n",
            "batch: 836/979, loss: 0.0122, time: 0.0028\n",
            "batch: 837/979, loss: 0.0122, time: 0.0028\n",
            "batch: 838/979, loss: 0.0122, time: 0.0028\n",
            "batch: 839/979, loss: 0.0122, time: 0.0028\n",
            "batch: 840/979, loss: 0.0122, time: 0.0048\n",
            "batch: 841/979, loss: 0.0122, time: 0.0029\n",
            "batch: 842/979, loss: 0.0122, time: 0.0030\n",
            "batch: 843/979, loss: 0.0122, time: 0.0028\n",
            "batch: 844/979, loss: 0.0124, time: 0.0028\n",
            "batch: 845/979, loss: 0.0123, time: 0.0028\n",
            "batch: 846/979, loss: 0.0123, time: 0.0028\n",
            "batch: 847/979, loss: 0.0123, time: 0.0033\n",
            "batch: 848/979, loss: 0.0123, time: 0.0029\n",
            "batch: 849/979, loss: 0.0123, time: 0.0028\n",
            "batch: 850/979, loss: 0.0123, time: 0.0035\n",
            "batch: 851/979, loss: 0.0123, time: 0.0029\n",
            "batch: 852/979, loss: 0.0125, time: 0.0029\n",
            "batch: 853/979, loss: 0.0126, time: 0.0028\n",
            "batch: 854/979, loss: 0.0126, time: 0.0028\n",
            "batch: 855/979, loss: 0.0126, time: 0.0028\n",
            "batch: 856/979, loss: 0.0126, time: 0.0029\n",
            "batch: 857/979, loss: 0.0126, time: 0.0028\n",
            "batch: 858/979, loss: 0.0126, time: 0.0035\n",
            "batch: 859/979, loss: 0.0126, time: 0.0032\n",
            "batch: 860/979, loss: 0.0125, time: 0.0028\n",
            "batch: 861/979, loss: 0.0125, time: 0.0028\n",
            "batch: 862/979, loss: 0.0125, time: 0.0028\n",
            "batch: 863/979, loss: 0.0125, time: 0.0036\n",
            "batch: 864/979, loss: 0.0125, time: 0.0039\n",
            "batch: 865/979, loss: 0.0125, time: 0.0028\n",
            "batch: 866/979, loss: 0.0125, time: 0.0028\n",
            "batch: 867/979, loss: 0.0125, time: 0.0028\n",
            "batch: 868/979, loss: 0.0125, time: 0.0029\n",
            "batch: 869/979, loss: 0.0125, time: 0.0028\n",
            "batch: 870/979, loss: 0.0125, time: 0.0028\n",
            "batch: 871/979, loss: 0.0125, time: 0.0028\n",
            "batch: 872/979, loss: 0.0125, time: 0.0028\n",
            "batch: 873/979, loss: 0.0125, time: 0.0028\n",
            "batch: 874/979, loss: 0.0125, time: 0.0028\n",
            "batch: 875/979, loss: 0.0125, time: 0.0028\n",
            "batch: 876/979, loss: 0.0125, time: 0.0028\n",
            "batch: 877/979, loss: 0.0125, time: 0.0028\n",
            "batch: 878/979, loss: 0.0125, time: 0.0030\n",
            "batch: 879/979, loss: 0.0124, time: 0.0029\n",
            "batch: 880/979, loss: 0.0124, time: 0.0029\n",
            "batch: 881/979, loss: 0.0124, time: 0.0040\n",
            "batch: 882/979, loss: 0.0124, time: 0.0029\n",
            "batch: 883/979, loss: 0.0124, time: 0.0028\n",
            "batch: 884/979, loss: 0.0124, time: 0.0029\n",
            "batch: 885/979, loss: 0.0124, time: 0.0029\n",
            "batch: 886/979, loss: 0.0124, time: 0.0031\n",
            "batch: 887/979, loss: 0.0124, time: 0.0032\n",
            "batch: 888/979, loss: 0.0124, time: 0.0028\n",
            "batch: 889/979, loss: 0.0124, time: 0.0028\n",
            "batch: 890/979, loss: 0.0124, time: 0.0030\n",
            "batch: 891/979, loss: 0.0124, time: 0.0028\n",
            "batch: 892/979, loss: 0.0124, time: 0.0029\n",
            "batch: 893/979, loss: 0.0124, time: 0.0029\n",
            "batch: 894/979, loss: 0.0124, time: 0.0029\n",
            "batch: 895/979, loss: 0.0124, time: 0.0029\n",
            "batch: 896/979, loss: 0.0124, time: 0.0066\n",
            "batch: 897/979, loss: 0.0124, time: 0.0047\n",
            "batch: 898/979, loss: 0.0123, time: 0.0042\n",
            "batch: 899/979, loss: 0.0124, time: 0.0043\n",
            "batch: 900/979, loss: 0.0123, time: 0.0031\n",
            "batch: 901/979, loss: 0.0124, time: 0.0028\n",
            "batch: 902/979, loss: 0.0124, time: 0.0028\n",
            "batch: 903/979, loss: 0.0124, time: 0.0030\n",
            "batch: 904/979, loss: 0.0124, time: 0.0029\n",
            "batch: 905/979, loss: 0.0124, time: 0.0029\n",
            "batch: 906/979, loss: 0.0124, time: 0.0028\n",
            "batch: 907/979, loss: 0.0124, time: 0.0029\n",
            "batch: 908/979, loss: 0.0124, time: 0.0030\n",
            "batch: 909/979, loss: 0.0124, time: 0.0030\n",
            "batch: 910/979, loss: 0.0124, time: 0.0029\n",
            "batch: 911/979, loss: 0.0124, time: 0.0028\n",
            "batch: 912/979, loss: 0.0124, time: 0.0028\n",
            "batch: 913/979, loss: 0.0124, time: 0.0028\n",
            "batch: 914/979, loss: 0.0123, time: 0.0029\n",
            "batch: 915/979, loss: 0.0123, time: 0.0029\n",
            "batch: 916/979, loss: 0.0124, time: 0.0028\n",
            "batch: 917/979, loss: 0.0124, time: 0.0028\n",
            "batch: 918/979, loss: 0.0124, time: 0.0028\n",
            "batch: 919/979, loss: 0.0124, time: 0.0043\n",
            "batch: 920/979, loss: 0.0124, time: 0.0029\n",
            "batch: 921/979, loss: 0.0124, time: 0.0028\n",
            "batch: 922/979, loss: 0.0124, time: 0.0028\n",
            "batch: 923/979, loss: 0.0124, time: 0.0028\n",
            "batch: 924/979, loss: 0.0124, time: 0.0028\n",
            "batch: 925/979, loss: 0.0124, time: 0.0029\n",
            "batch: 926/979, loss: 0.0124, time: 0.0028\n",
            "batch: 927/979, loss: 0.0124, time: 0.0039\n",
            "batch: 928/979, loss: 0.0124, time: 0.0029\n",
            "batch: 929/979, loss: 0.0124, time: 0.0028\n",
            "batch: 930/979, loss: 0.0124, time: 0.0029\n",
            "batch: 931/979, loss: 0.0124, time: 0.0028\n",
            "batch: 932/979, loss: 0.0124, time: 0.0028\n",
            "batch: 933/979, loss: 0.0124, time: 0.0030\n",
            "batch: 934/979, loss: 0.0123, time: 0.0030\n",
            "batch: 935/979, loss: 0.0123, time: 0.0028\n",
            "batch: 936/979, loss: 0.0123, time: 0.0029\n",
            "batch: 937/979, loss: 0.0123, time: 0.0028\n",
            "batch: 938/979, loss: 0.0123, time: 0.0029\n",
            "batch: 939/979, loss: 0.0123, time: 0.0028\n",
            "batch: 940/979, loss: 0.0123, time: 0.0028\n",
            "batch: 941/979, loss: 0.0123, time: 0.0029\n",
            "batch: 942/979, loss: 0.0123, time: 0.0029\n",
            "batch: 943/979, loss: 0.0123, time: 0.0029\n",
            "batch: 944/979, loss: 0.0123, time: 0.0028\n",
            "batch: 945/979, loss: 0.0123, time: 0.0028\n",
            "batch: 946/979, loss: 0.0123, time: 0.0028\n",
            "batch: 947/979, loss: 0.0123, time: 0.0029\n",
            "batch: 948/979, loss: 0.0123, time: 0.0028\n",
            "batch: 949/979, loss: 0.0124, time: 0.0030\n",
            "batch: 950/979, loss: 0.0124, time: 0.0029\n",
            "batch: 951/979, loss: 0.0124, time: 0.0029\n",
            "batch: 952/979, loss: 0.0124, time: 0.0030\n",
            "batch: 953/979, loss: 0.0124, time: 0.0038\n",
            "batch: 954/979, loss: 0.0124, time: 0.0038\n",
            "batch: 955/979, loss: 0.0124, time: 0.0029\n",
            "batch: 956/979, loss: 0.0124, time: 0.0030\n",
            "batch: 957/979, loss: 0.0124, time: 0.0028\n",
            "batch: 958/979, loss: 0.0125, time: 0.0038\n",
            "batch: 959/979, loss: 0.0125, time: 0.0038\n",
            "batch: 960/979, loss: 0.0125, time: 0.0029\n",
            "batch: 961/979, loss: 0.0125, time: 0.0028\n",
            "batch: 962/979, loss: 0.0125, time: 0.0029\n",
            "batch: 963/979, loss: 0.0125, time: 0.0029\n",
            "batch: 964/979, loss: 0.0125, time: 0.0028\n",
            "batch: 965/979, loss: 0.0125, time: 0.0028\n",
            "batch: 966/979, loss: 0.0125, time: 0.0028\n",
            "batch: 967/979, loss: 0.0125, time: 0.0029\n",
            "batch: 968/979, loss: 0.0125, time: 0.0028\n",
            "batch: 969/979, loss: 0.0125, time: 0.0028\n",
            "batch: 970/979, loss: 0.0125, time: 0.0028\n",
            "batch: 971/979, loss: 0.0125, time: 0.0028\n",
            "batch: 972/979, loss: 0.0125, time: 0.0028\n",
            "batch: 973/979, loss: 0.0125, time: 0.0029\n",
            "batch: 974/979, loss: 0.0126, time: 0.0028\n",
            "batch: 975/979, loss: 0.0126, time: 0.0028\n",
            "batch: 976/979, loss: 0.0127, time: 0.0029\n",
            "batch: 977/979, loss: 0.0127, time: 0.0028\n",
            "batch: 978/979, loss: 0.0127, time: 0.0028\n",
            "batch: 979/979, loss: 0.0127, time: 0.0028\n",
            "batch: 1/979, loss: 0.0051, time: 0.0049\n",
            "batch: 2/979, loss: 0.0042, time: 0.0044\n",
            "batch: 3/979, loss: 0.0029, time: 0.0035\n",
            "batch: 4/979, loss: 0.0024, time: 0.0043\n",
            "batch: 5/979, loss: 0.0020, time: 0.0042\n",
            "batch: 6/979, loss: 0.0150, time: 0.0044\n",
            "batch: 7/979, loss: 0.0129, time: 0.0048\n",
            "batch: 8/979, loss: 0.0118, time: 0.0041\n",
            "batch: 9/979, loss: 0.0105, time: 0.0041\n",
            "batch: 10/979, loss: 0.0097, time: 0.0041\n",
            "batch: 11/979, loss: 0.0088, time: 0.0040\n",
            "batch: 12/979, loss: 0.0082, time: 0.0039\n",
            "batch: 13/979, loss: 0.0079, time: 0.0053\n",
            "batch: 14/979, loss: 0.0117, time: 0.0037\n",
            "batch: 15/979, loss: 0.0111, time: 0.0051\n",
            "batch: 16/979, loss: 0.0106, time: 0.0037\n",
            "batch: 17/979, loss: 0.0101, time: 0.0041\n",
            "batch: 18/979, loss: 0.0097, time: 0.0040\n",
            "batch: 19/979, loss: 0.0093, time: 0.0035\n",
            "batch: 20/979, loss: 0.0089, time: 0.0035\n",
            "batch: 21/979, loss: 0.0097, time: 0.0035\n",
            "batch: 22/979, loss: 0.0093, time: 0.0041\n",
            "batch: 23/979, loss: 0.0096, time: 0.0052\n",
            "batch: 24/979, loss: 0.0092, time: 0.0053\n",
            "batch: 25/979, loss: 0.0090, time: 0.0053\n",
            "batch: 26/979, loss: 0.0091, time: 0.0052\n",
            "batch: 27/979, loss: 0.0089, time: 0.0061\n",
            "batch: 28/979, loss: 0.0102, time: 0.0041\n",
            "batch: 29/979, loss: 0.0099, time: 0.0047\n",
            "batch: 30/979, loss: 0.0096, time: 0.0039\n",
            "batch: 31/979, loss: 0.0094, time: 0.0036\n",
            "batch: 32/979, loss: 0.0093, time: 0.0037\n",
            "batch: 33/979, loss: 0.0091, time: 0.0036\n",
            "batch: 34/979, loss: 0.0089, time: 0.0037\n",
            "batch: 35/979, loss: 0.0088, time: 0.0039\n",
            "batch: 36/979, loss: 0.0087, time: 0.0036\n",
            "batch: 37/979, loss: 0.0085, time: 0.0037\n",
            "batch: 38/979, loss: 0.0083, time: 0.0044\n",
            "batch: 39/979, loss: 0.0082, time: 0.0036\n",
            "batch: 40/979, loss: 0.0080, time: 0.0039\n",
            "batch: 41/979, loss: 0.0088, time: 0.0036\n",
            "batch: 42/979, loss: 0.0087, time: 0.0035\n",
            "batch: 43/979, loss: 0.0086, time: 0.0035\n",
            "batch: 44/979, loss: 0.0089, time: 0.0035\n",
            "batch: 45/979, loss: 0.0088, time: 0.0035\n",
            "batch: 46/979, loss: 0.0088, time: 0.0035\n",
            "batch: 47/979, loss: 0.0086, time: 0.0037\n",
            "batch: 48/979, loss: 0.0086, time: 0.0035\n",
            "batch: 49/979, loss: 0.0084, time: 0.0041\n",
            "batch: 50/979, loss: 0.0086, time: 0.0041\n",
            "batch: 51/979, loss: 0.0084, time: 0.0036\n",
            "batch: 52/979, loss: 0.0083, time: 0.0036\n",
            "batch: 53/979, loss: 0.0082, time: 0.0035\n",
            "batch: 54/979, loss: 0.0081, time: 0.0035\n",
            "batch: 55/979, loss: 0.0079, time: 0.0084\n",
            "batch: 56/979, loss: 0.0082, time: 0.0042\n",
            "batch: 57/979, loss: 0.0081, time: 0.0036\n",
            "batch: 58/979, loss: 0.0080, time: 0.0036\n",
            "batch: 59/979, loss: 0.0079, time: 0.0036\n",
            "batch: 60/979, loss: 0.0078, time: 0.0036\n",
            "batch: 61/979, loss: 0.0077, time: 0.0037\n",
            "batch: 62/979, loss: 0.0077, time: 0.0037\n",
            "batch: 63/979, loss: 0.0075, time: 0.0036\n",
            "batch: 64/979, loss: 0.0074, time: 0.0036\n",
            "batch: 65/979, loss: 0.0074, time: 0.0042\n",
            "batch: 66/979, loss: 0.0073, time: 0.0046\n",
            "batch: 67/979, loss: 0.0072, time: 0.0044\n",
            "batch: 68/979, loss: 0.0075, time: 0.0043\n",
            "batch: 69/979, loss: 0.0076, time: 0.0042\n",
            "batch: 70/979, loss: 0.0075, time: 0.0039\n",
            "batch: 71/979, loss: 0.0075, time: 0.0036\n",
            "batch: 72/979, loss: 0.0076, time: 0.0036\n",
            "batch: 73/979, loss: 0.0075, time: 0.0036\n",
            "batch: 74/979, loss: 0.0074, time: 0.0036\n",
            "batch: 75/979, loss: 0.0074, time: 0.0036\n",
            "batch: 76/979, loss: 0.0073, time: 0.0037\n",
            "batch: 77/979, loss: 0.0078, time: 0.0036\n",
            "batch: 78/979, loss: 0.0078, time: 0.0037\n",
            "batch: 79/979, loss: 0.0077, time: 0.0036\n",
            "batch: 80/979, loss: 0.0076, time: 0.0036\n",
            "batch: 81/979, loss: 0.0075, time: 0.0040\n",
            "batch: 82/979, loss: 0.0075, time: 0.0036\n",
            "batch: 83/979, loss: 0.0074, time: 0.0058\n",
            "batch: 84/979, loss: 0.0074, time: 0.0038\n",
            "batch: 85/979, loss: 0.0073, time: 0.0036\n",
            "batch: 86/979, loss: 0.0074, time: 0.0037\n",
            "batch: 87/979, loss: 0.0074, time: 0.0036\n",
            "batch: 88/979, loss: 0.0073, time: 0.0036\n",
            "batch: 89/979, loss: 0.0072, time: 0.0036\n",
            "batch: 90/979, loss: 0.0072, time: 0.0037\n",
            "batch: 91/979, loss: 0.0071, time: 0.0039\n",
            "batch: 92/979, loss: 0.0071, time: 0.0036\n",
            "batch: 93/979, loss: 0.0070, time: 0.0036\n",
            "batch: 94/979, loss: 0.0069, time: 0.0036\n",
            "batch: 95/979, loss: 0.0069, time: 0.0036\n",
            "batch: 96/979, loss: 0.0068, time: 0.0036\n",
            "batch: 97/979, loss: 0.0067, time: 0.0037\n",
            "batch: 98/979, loss: 0.0067, time: 0.0062\n",
            "batch: 99/979, loss: 0.0067, time: 0.0041\n",
            "batch: 100/979, loss: 0.0066, time: 0.0038\n",
            "batch: 101/979, loss: 0.0066, time: 0.0038\n",
            "batch: 102/979, loss: 0.0066, time: 0.0035\n",
            "batch: 103/979, loss: 0.0066, time: 0.0035\n",
            "batch: 104/979, loss: 0.0065, time: 0.0035\n",
            "batch: 105/979, loss: 0.0065, time: 0.0044\n",
            "batch: 106/979, loss: 0.0065, time: 0.0035\n",
            "batch: 107/979, loss: 0.0066, time: 0.0034\n",
            "batch: 108/979, loss: 0.0065, time: 0.0034\n",
            "batch: 109/979, loss: 0.0065, time: 0.0042\n",
            "batch: 110/979, loss: 0.0065, time: 0.0051\n",
            "batch: 111/979, loss: 0.0064, time: 0.0041\n",
            "batch: 112/979, loss: 0.0064, time: 0.0035\n",
            "batch: 113/979, loss: 0.0064, time: 0.0036\n",
            "batch: 114/979, loss: 0.0064, time: 0.0036\n",
            "batch: 115/979, loss: 0.0064, time: 0.0036\n",
            "batch: 116/979, loss: 0.0063, time: 0.0035\n",
            "batch: 117/979, loss: 0.0063, time: 0.0034\n",
            "batch: 118/979, loss: 0.0063, time: 0.0044\n",
            "batch: 119/979, loss: 0.0062, time: 0.0051\n",
            "batch: 120/979, loss: 0.0062, time: 0.0035\n",
            "batch: 121/979, loss: 0.0068, time: 0.0035\n",
            "batch: 122/979, loss: 0.0068, time: 0.0035\n",
            "batch: 123/979, loss: 0.0068, time: 0.0035\n",
            "batch: 124/979, loss: 0.0068, time: 0.0036\n",
            "batch: 125/979, loss: 0.0070, time: 0.0035\n",
            "batch: 126/979, loss: 0.0070, time: 0.0035\n",
            "batch: 127/979, loss: 0.0070, time: 0.0035\n",
            "batch: 128/979, loss: 0.0069, time: 0.0034\n",
            "batch: 129/979, loss: 0.0069, time: 0.0035\n",
            "batch: 130/979, loss: 0.0070, time: 0.0038\n",
            "batch: 131/979, loss: 0.0070, time: 0.0036\n",
            "batch: 132/979, loss: 0.0069, time: 0.0036\n",
            "batch: 133/979, loss: 0.0069, time: 0.0035\n",
            "batch: 134/979, loss: 0.0069, time: 0.0035\n",
            "batch: 135/979, loss: 0.0069, time: 0.0034\n",
            "batch: 136/979, loss: 0.0069, time: 0.0035\n",
            "batch: 137/979, loss: 0.0068, time: 0.0036\n",
            "batch: 138/979, loss: 0.0068, time: 0.0035\n",
            "batch: 139/979, loss: 0.0068, time: 0.0043\n",
            "batch: 140/979, loss: 0.0068, time: 0.0035\n",
            "batch: 141/979, loss: 0.0068, time: 0.0034\n",
            "batch: 142/979, loss: 0.0067, time: 0.0037\n",
            "batch: 143/979, loss: 0.0067, time: 0.0035\n",
            "batch: 144/979, loss: 0.0068, time: 0.0034\n",
            "batch: 145/979, loss: 0.0067, time: 0.0035\n",
            "batch: 146/979, loss: 0.0081, time: 0.0035\n",
            "batch: 147/979, loss: 0.0084, time: 0.0035\n",
            "batch: 148/979, loss: 0.0084, time: 0.0035\n",
            "batch: 149/979, loss: 0.0084, time: 0.0035\n",
            "batch: 150/979, loss: 0.0084, time: 0.0035\n",
            "batch: 151/979, loss: 0.0083, time: 0.0035\n",
            "batch: 152/979, loss: 0.0083, time: 0.0035\n",
            "batch: 153/979, loss: 0.0083, time: 0.0036\n",
            "batch: 154/979, loss: 0.0082, time: 0.0036\n",
            "batch: 155/979, loss: 0.0082, time: 0.0042\n",
            "batch: 156/979, loss: 0.0081, time: 0.0041\n",
            "batch: 157/979, loss: 0.0081, time: 0.0037\n",
            "batch: 158/979, loss: 0.0080, time: 0.0042\n",
            "batch: 159/979, loss: 0.0080, time: 0.0031\n",
            "batch: 160/979, loss: 0.0080, time: 0.0036\n",
            "batch: 161/979, loss: 0.0079, time: 0.0070\n",
            "batch: 162/979, loss: 0.0079, time: 0.0042\n",
            "batch: 163/979, loss: 0.0079, time: 0.0050\n",
            "batch: 164/979, loss: 0.0079, time: 0.0081\n",
            "batch: 165/979, loss: 0.0079, time: 0.0043\n",
            "batch: 166/979, loss: 0.0078, time: 0.0050\n",
            "batch: 167/979, loss: 0.0079, time: 0.0046\n",
            "batch: 168/979, loss: 0.0079, time: 0.0047\n",
            "batch: 169/979, loss: 0.0080, time: 0.0046\n",
            "batch: 170/979, loss: 0.0080, time: 0.0042\n",
            "batch: 171/979, loss: 0.0079, time: 0.0053\n",
            "batch: 172/979, loss: 0.0079, time: 0.0034\n",
            "batch: 173/979, loss: 0.0079, time: 0.0041\n",
            "batch: 174/979, loss: 0.0079, time: 0.0045\n",
            "batch: 175/979, loss: 0.0079, time: 0.0043\n",
            "batch: 176/979, loss: 0.0089, time: 0.0046\n",
            "batch: 177/979, loss: 0.0090, time: 0.0047\n",
            "batch: 178/979, loss: 0.0090, time: 0.0051\n",
            "batch: 179/979, loss: 0.0090, time: 0.0046\n",
            "batch: 180/979, loss: 0.0089, time: 0.0044\n",
            "batch: 181/979, loss: 0.0089, time: 0.0036\n",
            "batch: 182/979, loss: 0.0089, time: 0.0036\n",
            "batch: 183/979, loss: 0.0088, time: 0.0036\n",
            "batch: 184/979, loss: 0.0089, time: 0.0036\n",
            "batch: 185/979, loss: 0.0089, time: 0.0036\n",
            "batch: 186/979, loss: 0.0091, time: 0.0061\n",
            "batch: 187/979, loss: 0.0091, time: 0.0042\n",
            "batch: 188/979, loss: 0.0090, time: 0.0037\n",
            "batch: 189/979, loss: 0.0090, time: 0.0039\n",
            "batch: 190/979, loss: 0.0090, time: 0.0056\n",
            "batch: 191/979, loss: 0.0089, time: 0.0038\n",
            "batch: 192/979, loss: 0.0089, time: 0.0036\n",
            "batch: 193/979, loss: 0.0090, time: 0.0035\n",
            "batch: 194/979, loss: 0.0089, time: 0.0035\n",
            "batch: 195/979, loss: 0.0089, time: 0.0035\n",
            "batch: 196/979, loss: 0.0089, time: 0.0036\n",
            "batch: 197/979, loss: 0.0088, time: 0.0043\n",
            "batch: 198/979, loss: 0.0088, time: 0.0051\n",
            "batch: 199/979, loss: 0.0088, time: 0.0062\n",
            "batch: 200/979, loss: 0.0093, time: 0.0047\n",
            "batch: 201/979, loss: 0.0093, time: 0.0035\n",
            "batch: 202/979, loss: 0.0092, time: 0.0039\n",
            "batch: 203/979, loss: 0.0092, time: 0.0044\n",
            "batch: 204/979, loss: 0.0101, time: 0.0047\n",
            "batch: 205/979, loss: 0.0101, time: 0.0044\n",
            "batch: 206/979, loss: 0.0101, time: 0.0041\n",
            "batch: 207/979, loss: 0.0102, time: 0.0042\n",
            "batch: 208/979, loss: 0.0101, time: 0.0042\n",
            "batch: 209/979, loss: 0.0101, time: 0.0049\n",
            "batch: 210/979, loss: 0.0100, time: 0.0042\n",
            "batch: 211/979, loss: 0.0100, time: 0.0041\n",
            "batch: 212/979, loss: 0.0100, time: 0.0043\n",
            "batch: 213/979, loss: 0.0100, time: 0.0062\n",
            "batch: 214/979, loss: 0.0102, time: 0.0037\n",
            "batch: 215/979, loss: 0.0103, time: 0.0039\n",
            "batch: 216/979, loss: 0.0103, time: 0.0041\n",
            "batch: 217/979, loss: 0.0102, time: 0.0042\n",
            "batch: 218/979, loss: 0.0102, time: 0.0038\n",
            "batch: 219/979, loss: 0.0104, time: 0.0039\n",
            "batch: 220/979, loss: 0.0104, time: 0.0035\n",
            "batch: 221/979, loss: 0.0104, time: 0.0034\n",
            "batch: 222/979, loss: 0.0104, time: 0.0041\n",
            "batch: 223/979, loss: 0.0103, time: 0.0044\n",
            "batch: 224/979, loss: 0.0103, time: 0.0041\n",
            "batch: 225/979, loss: 0.0102, time: 0.0040\n",
            "batch: 226/979, loss: 0.0102, time: 0.0040\n",
            "batch: 227/979, loss: 0.0102, time: 0.0042\n",
            "batch: 228/979, loss: 0.0101, time: 0.0046\n",
            "batch: 229/979, loss: 0.0101, time: 0.0041\n",
            "batch: 230/979, loss: 0.0101, time: 0.0046\n",
            "batch: 231/979, loss: 0.0108, time: 0.0042\n",
            "batch: 232/979, loss: 0.0108, time: 0.0042\n",
            "batch: 233/979, loss: 0.0108, time: 0.0042\n",
            "batch: 234/979, loss: 0.0107, time: 0.0042\n",
            "batch: 235/979, loss: 0.0107, time: 0.0040\n",
            "batch: 236/979, loss: 0.0106, time: 0.0039\n",
            "batch: 237/979, loss: 0.0106, time: 0.0047\n",
            "batch: 238/979, loss: 0.0106, time: 0.0039\n",
            "batch: 239/979, loss: 0.0108, time: 0.0040\n",
            "batch: 240/979, loss: 0.0109, time: 0.0057\n",
            "batch: 241/979, loss: 0.0109, time: 0.0039\n",
            "batch: 242/979, loss: 0.0108, time: 0.0039\n",
            "batch: 243/979, loss: 0.0108, time: 0.0039\n",
            "batch: 244/979, loss: 0.0108, time: 0.0042\n",
            "batch: 245/979, loss: 0.0107, time: 0.0038\n",
            "batch: 246/979, loss: 0.0107, time: 0.0038\n",
            "batch: 247/979, loss: 0.0107, time: 0.0039\n",
            "batch: 248/979, loss: 0.0106, time: 0.0039\n",
            "batch: 249/979, loss: 0.0106, time: 0.0038\n",
            "batch: 250/979, loss: 0.0106, time: 0.0041\n",
            "batch: 251/979, loss: 0.0105, time: 0.0039\n",
            "batch: 252/979, loss: 0.0106, time: 0.0039\n",
            "batch: 253/979, loss: 0.0106, time: 0.0039\n",
            "batch: 254/979, loss: 0.0105, time: 0.0039\n",
            "batch: 255/979, loss: 0.0105, time: 0.0040\n",
            "batch: 256/979, loss: 0.0105, time: 0.0039\n",
            "batch: 257/979, loss: 0.0104, time: 0.0038\n",
            "batch: 258/979, loss: 0.0105, time: 0.0041\n",
            "batch: 259/979, loss: 0.0105, time: 0.0038\n",
            "batch: 260/979, loss: 0.0104, time: 0.0038\n",
            "batch: 261/979, loss: 0.0104, time: 0.0038\n",
            "batch: 262/979, loss: 0.0106, time: 0.0038\n",
            "batch: 263/979, loss: 0.0106, time: 0.0038\n",
            "batch: 264/979, loss: 0.0106, time: 0.0039\n",
            "batch: 265/979, loss: 0.0105, time: 0.0040\n",
            "batch: 266/979, loss: 0.0105, time: 0.0038\n",
            "batch: 267/979, loss: 0.0109, time: 0.0039\n",
            "batch: 268/979, loss: 0.0109, time: 0.0039\n",
            "batch: 269/979, loss: 0.0109, time: 0.0059\n",
            "batch: 270/979, loss: 0.0109, time: 0.0040\n",
            "batch: 271/979, loss: 0.0108, time: 0.0038\n",
            "batch: 272/979, loss: 0.0108, time: 0.0039\n",
            "batch: 273/979, loss: 0.0108, time: 0.0038\n",
            "batch: 274/979, loss: 0.0108, time: 0.0038\n",
            "batch: 275/979, loss: 0.0108, time: 0.0038\n",
            "batch: 276/979, loss: 0.0108, time: 0.0038\n",
            "batch: 277/979, loss: 0.0108, time: 0.0038\n",
            "batch: 278/979, loss: 0.0108, time: 0.0038\n",
            "batch: 279/979, loss: 0.0109, time: 0.0042\n",
            "batch: 280/979, loss: 0.0109, time: 0.0039\n",
            "batch: 281/979, loss: 0.0109, time: 0.0038\n",
            "batch: 282/979, loss: 0.0109, time: 0.0039\n",
            "batch: 283/979, loss: 0.0110, time: 0.0038\n",
            "batch: 284/979, loss: 0.0110, time: 0.0038\n",
            "batch: 285/979, loss: 0.0110, time: 0.0038\n",
            "batch: 286/979, loss: 0.0109, time: 0.0039\n",
            "batch: 287/979, loss: 0.0109, time: 0.0040\n",
            "batch: 288/979, loss: 0.0109, time: 0.0038\n",
            "batch: 289/979, loss: 0.0109, time: 0.0038\n",
            "batch: 290/979, loss: 0.0108, time: 0.0042\n",
            "batch: 291/979, loss: 0.0108, time: 0.0044\n",
            "batch: 292/979, loss: 0.0108, time: 0.0061\n",
            "batch: 293/979, loss: 0.0108, time: 0.0071\n",
            "batch: 294/979, loss: 0.0111, time: 0.0042\n",
            "batch: 295/979, loss: 0.0110, time: 0.0060\n",
            "batch: 296/979, loss: 0.0110, time: 0.0038\n",
            "batch: 297/979, loss: 0.0110, time: 0.0044\n",
            "batch: 298/979, loss: 0.0110, time: 0.0042\n",
            "batch: 299/979, loss: 0.0110, time: 0.0039\n",
            "batch: 300/979, loss: 0.0110, time: 0.0038\n",
            "batch: 301/979, loss: 0.0110, time: 0.0038\n",
            "batch: 302/979, loss: 0.0109, time: 0.0038\n",
            "batch: 303/979, loss: 0.0109, time: 0.0038\n",
            "batch: 304/979, loss: 0.0109, time: 0.0038\n",
            "batch: 305/979, loss: 0.0109, time: 0.0039\n",
            "batch: 306/979, loss: 0.0109, time: 0.0039\n",
            "batch: 307/979, loss: 0.0108, time: 0.0041\n",
            "batch: 308/979, loss: 0.0108, time: 0.0038\n",
            "batch: 309/979, loss: 0.0108, time: 0.0038\n",
            "batch: 310/979, loss: 0.0107, time: 0.0038\n",
            "batch: 311/979, loss: 0.0107, time: 0.0038\n",
            "batch: 312/979, loss: 0.0107, time: 0.0040\n",
            "batch: 313/979, loss: 0.0107, time: 0.0038\n",
            "batch: 314/979, loss: 0.0107, time: 0.0038\n",
            "batch: 315/979, loss: 0.0106, time: 0.0038\n",
            "batch: 316/979, loss: 0.0106, time: 0.0039\n",
            "batch: 317/979, loss: 0.0106, time: 0.0039\n",
            "batch: 318/979, loss: 0.0106, time: 0.0038\n",
            "batch: 319/979, loss: 0.0106, time: 0.0039\n",
            "batch: 320/979, loss: 0.0105, time: 0.0040\n",
            "batch: 321/979, loss: 0.0105, time: 0.0039\n",
            "batch: 322/979, loss: 0.0105, time: 0.0039\n",
            "batch: 323/979, loss: 0.0105, time: 0.0046\n",
            "batch: 324/979, loss: 0.0105, time: 0.0034\n",
            "batch: 325/979, loss: 0.0105, time: 0.0036\n",
            "batch: 326/979, loss: 0.0105, time: 0.0039\n",
            "batch: 327/979, loss: 0.0104, time: 0.0038\n",
            "batch: 328/979, loss: 0.0104, time: 0.0039\n",
            "batch: 329/979, loss: 0.0104, time: 0.0038\n",
            "batch: 330/979, loss: 0.0103, time: 0.0040\n",
            "batch: 331/979, loss: 0.0103, time: 0.0042\n",
            "batch: 332/979, loss: 0.0103, time: 0.0041\n",
            "batch: 333/979, loss: 0.0103, time: 0.0039\n",
            "batch: 334/979, loss: 0.0102, time: 0.0038\n",
            "batch: 335/979, loss: 0.0102, time: 0.0038\n",
            "batch: 336/979, loss: 0.0102, time: 0.0038\n",
            "batch: 337/979, loss: 0.0102, time: 0.0038\n",
            "batch: 338/979, loss: 0.0101, time: 0.0038\n",
            "batch: 339/979, loss: 0.0101, time: 0.0038\n",
            "batch: 340/979, loss: 0.0101, time: 0.0038\n",
            "batch: 341/979, loss: 0.0101, time: 0.0038\n",
            "batch: 342/979, loss: 0.0101, time: 0.0039\n",
            "batch: 343/979, loss: 0.0100, time: 0.0051\n",
            "batch: 344/979, loss: 0.0100, time: 0.0039\n",
            "batch: 345/979, loss: 0.0100, time: 0.0041\n",
            "batch: 346/979, loss: 0.0100, time: 0.0039\n",
            "batch: 347/979, loss: 0.0100, time: 0.0039\n",
            "batch: 348/979, loss: 0.0100, time: 0.0041\n",
            "batch: 349/979, loss: 0.0099, time: 0.0041\n",
            "batch: 350/979, loss: 0.0100, time: 0.0038\n",
            "batch: 351/979, loss: 0.0099, time: 0.0043\n",
            "batch: 352/979, loss: 0.0099, time: 0.0041\n",
            "batch: 353/979, loss: 0.0099, time: 0.0041\n",
            "batch: 354/979, loss: 0.0099, time: 0.0046\n",
            "batch: 355/979, loss: 0.0099, time: 0.0042\n",
            "batch: 356/979, loss: 0.0099, time: 0.0041\n",
            "batch: 357/979, loss: 0.0099, time: 0.0041\n",
            "batch: 358/979, loss: 0.0099, time: 0.0063\n",
            "batch: 359/979, loss: 0.0100, time: 0.0041\n",
            "batch: 360/979, loss: 0.0100, time: 0.0042\n",
            "batch: 361/979, loss: 0.0100, time: 0.0041\n",
            "batch: 362/979, loss: 0.0100, time: 0.0040\n",
            "batch: 363/979, loss: 0.0100, time: 0.0038\n",
            "batch: 364/979, loss: 0.0099, time: 0.0039\n",
            "batch: 365/979, loss: 0.0099, time: 0.0038\n",
            "batch: 366/979, loss: 0.0099, time: 0.0040\n",
            "batch: 367/979, loss: 0.0099, time: 0.0040\n",
            "batch: 368/979, loss: 0.0099, time: 0.0049\n",
            "batch: 369/979, loss: 0.0099, time: 0.0041\n",
            "batch: 370/979, loss: 0.0099, time: 0.0040\n",
            "batch: 371/979, loss: 0.0099, time: 0.0040\n",
            "batch: 372/979, loss: 0.0099, time: 0.0042\n",
            "batch: 373/979, loss: 0.0100, time: 0.0042\n",
            "batch: 374/979, loss: 0.0100, time: 0.0042\n",
            "batch: 375/979, loss: 0.0100, time: 0.0050\n",
            "batch: 376/979, loss: 0.0100, time: 0.0040\n",
            "batch: 377/979, loss: 0.0100, time: 0.0056\n",
            "batch: 378/979, loss: 0.0100, time: 0.0044\n",
            "batch: 379/979, loss: 0.0100, time: 0.0042\n",
            "batch: 380/979, loss: 0.0100, time: 0.0039\n",
            "batch: 381/979, loss: 0.0100, time: 0.0037\n",
            "batch: 382/979, loss: 0.0104, time: 0.0036\n",
            "batch: 383/979, loss: 0.0106, time: 0.0036\n",
            "batch: 384/979, loss: 0.0106, time: 0.0040\n",
            "batch: 385/979, loss: 0.0106, time: 0.0036\n",
            "batch: 386/979, loss: 0.0106, time: 0.0035\n",
            "batch: 387/979, loss: 0.0105, time: 0.0037\n",
            "batch: 388/979, loss: 0.0105, time: 0.0036\n",
            "batch: 389/979, loss: 0.0105, time: 0.0036\n",
            "batch: 390/979, loss: 0.0105, time: 0.0036\n",
            "batch: 391/979, loss: 0.0105, time: 0.0036\n",
            "batch: 392/979, loss: 0.0104, time: 0.0036\n",
            "batch: 393/979, loss: 0.0104, time: 0.0036\n",
            "batch: 394/979, loss: 0.0104, time: 0.0035\n",
            "batch: 395/979, loss: 0.0104, time: 0.0036\n",
            "batch: 396/979, loss: 0.0104, time: 0.0036\n",
            "batch: 397/979, loss: 0.0104, time: 0.0036\n",
            "batch: 398/979, loss: 0.0104, time: 0.0036\n",
            "batch: 399/979, loss: 0.0104, time: 0.0050\n",
            "batch: 400/979, loss: 0.0104, time: 0.0035\n",
            "batch: 401/979, loss: 0.0104, time: 0.0035\n",
            "batch: 402/979, loss: 0.0104, time: 0.0036\n",
            "batch: 403/979, loss: 0.0105, time: 0.0044\n",
            "batch: 404/979, loss: 0.0105, time: 0.0039\n",
            "batch: 405/979, loss: 0.0106, time: 0.0039\n",
            "batch: 406/979, loss: 0.0106, time: 0.0035\n",
            "batch: 407/979, loss: 0.0106, time: 0.0035\n",
            "batch: 408/979, loss: 0.0106, time: 0.0035\n",
            "batch: 409/979, loss: 0.0106, time: 0.0035\n",
            "batch: 410/979, loss: 0.0105, time: 0.0036\n",
            "batch: 411/979, loss: 0.0105, time: 0.0034\n",
            "batch: 412/979, loss: 0.0105, time: 0.0028\n",
            "batch: 413/979, loss: 0.0105, time: 0.0028\n",
            "batch: 414/979, loss: 0.0105, time: 0.0029\n",
            "batch: 415/979, loss: 0.0105, time: 0.0029\n",
            "batch: 416/979, loss: 0.0105, time: 0.0029\n",
            "batch: 417/979, loss: 0.0105, time: 0.0029\n",
            "batch: 418/979, loss: 0.0105, time: 0.0029\n",
            "batch: 419/979, loss: 0.0104, time: 0.0028\n",
            "batch: 420/979, loss: 0.0104, time: 0.0028\n",
            "batch: 421/979, loss: 0.0104, time: 0.0029\n",
            "batch: 422/979, loss: 0.0104, time: 0.0029\n",
            "batch: 423/979, loss: 0.0104, time: 0.0028\n",
            "batch: 424/979, loss: 0.0104, time: 0.0030\n",
            "batch: 425/979, loss: 0.0104, time: 0.0029\n",
            "batch: 426/979, loss: 0.0103, time: 0.0029\n",
            "batch: 427/979, loss: 0.0103, time: 0.0029\n",
            "batch: 428/979, loss: 0.0103, time: 0.0030\n",
            "batch: 429/979, loss: 0.0103, time: 0.0029\n",
            "batch: 430/979, loss: 0.0103, time: 0.0028\n",
            "batch: 431/979, loss: 0.0103, time: 0.0028\n",
            "batch: 432/979, loss: 0.0102, time: 0.0029\n",
            "batch: 433/979, loss: 0.0102, time: 0.0028\n",
            "batch: 434/979, loss: 0.0102, time: 0.0044\n",
            "batch: 435/979, loss: 0.0102, time: 0.0041\n",
            "batch: 436/979, loss: 0.0102, time: 0.0040\n",
            "batch: 437/979, loss: 0.0102, time: 0.0029\n",
            "batch: 438/979, loss: 0.0102, time: 0.0029\n",
            "batch: 439/979, loss: 0.0102, time: 0.0028\n",
            "batch: 440/979, loss: 0.0102, time: 0.0028\n",
            "batch: 441/979, loss: 0.0101, time: 0.0028\n",
            "batch: 442/979, loss: 0.0101, time: 0.0029\n",
            "batch: 443/979, loss: 0.0101, time: 0.0029\n",
            "batch: 444/979, loss: 0.0101, time: 0.0028\n",
            "batch: 445/979, loss: 0.0101, time: 0.0028\n",
            "batch: 446/979, loss: 0.0101, time: 0.0029\n",
            "batch: 447/979, loss: 0.0101, time: 0.0029\n",
            "batch: 448/979, loss: 0.0101, time: 0.0028\n",
            "batch: 449/979, loss: 0.0101, time: 0.0028\n",
            "batch: 450/979, loss: 0.0101, time: 0.0030\n",
            "batch: 451/979, loss: 0.0100, time: 0.0035\n",
            "batch: 452/979, loss: 0.0100, time: 0.0029\n",
            "batch: 453/979, loss: 0.0100, time: 0.0028\n",
            "batch: 454/979, loss: 0.0100, time: 0.0028\n",
            "batch: 455/979, loss: 0.0100, time: 0.0033\n",
            "batch: 456/979, loss: 0.0100, time: 0.0030\n",
            "batch: 457/979, loss: 0.0100, time: 0.0028\n",
            "batch: 458/979, loss: 0.0100, time: 0.0029\n",
            "batch: 459/979, loss: 0.0099, time: 0.0029\n",
            "batch: 460/979, loss: 0.0099, time: 0.0028\n",
            "batch: 461/979, loss: 0.0099, time: 0.0029\n",
            "batch: 462/979, loss: 0.0099, time: 0.0029\n",
            "batch: 463/979, loss: 0.0099, time: 0.0029\n",
            "batch: 464/979, loss: 0.0099, time: 0.0028\n",
            "batch: 465/979, loss: 0.0099, time: 0.0029\n",
            "batch: 466/979, loss: 0.0099, time: 0.0045\n",
            "batch: 467/979, loss: 0.0099, time: 0.0039\n",
            "batch: 468/979, loss: 0.0099, time: 0.0043\n",
            "batch: 469/979, loss: 0.0099, time: 0.0037\n",
            "batch: 470/979, loss: 0.0098, time: 0.0035\n",
            "batch: 471/979, loss: 0.0098, time: 0.0028\n",
            "batch: 472/979, loss: 0.0098, time: 0.0030\n",
            "batch: 473/979, loss: 0.0098, time: 0.0029\n",
            "batch: 474/979, loss: 0.0098, time: 0.0028\n",
            "batch: 475/979, loss: 0.0098, time: 0.0029\n",
            "batch: 476/979, loss: 0.0098, time: 0.0028\n",
            "batch: 477/979, loss: 0.0098, time: 0.0029\n",
            "batch: 478/979, loss: 0.0098, time: 0.0028\n",
            "batch: 479/979, loss: 0.0097, time: 0.0028\n",
            "batch: 480/979, loss: 0.0098, time: 0.0031\n",
            "batch: 481/979, loss: 0.0097, time: 0.0028\n",
            "batch: 482/979, loss: 0.0097, time: 0.0043\n",
            "batch: 483/979, loss: 0.0097, time: 0.0030\n",
            "batch: 484/979, loss: 0.0097, time: 0.0028\n",
            "batch: 485/979, loss: 0.0097, time: 0.0029\n",
            "batch: 486/979, loss: 0.0097, time: 0.0028\n",
            "batch: 487/979, loss: 0.0096, time: 0.0028\n",
            "batch: 488/979, loss: 0.0096, time: 0.0028\n",
            "batch: 489/979, loss: 0.0096, time: 0.0030\n",
            "batch: 490/979, loss: 0.0096, time: 0.0029\n",
            "batch: 491/979, loss: 0.0097, time: 0.0028\n",
            "batch: 492/979, loss: 0.0097, time: 0.0029\n",
            "batch: 493/979, loss: 0.0096, time: 0.0029\n",
            "batch: 494/979, loss: 0.0098, time: 0.0028\n",
            "batch: 495/979, loss: 0.0098, time: 0.0029\n",
            "batch: 496/979, loss: 0.0098, time: 0.0031\n",
            "batch: 497/979, loss: 0.0097, time: 0.0029\n",
            "batch: 498/979, loss: 0.0097, time: 0.0050\n",
            "batch: 499/979, loss: 0.0097, time: 0.0041\n",
            "batch: 500/979, loss: 0.0097, time: 0.0031\n",
            "batch: 501/979, loss: 0.0098, time: 0.0029\n",
            "batch: 502/979, loss: 0.0097, time: 0.0029\n",
            "batch: 503/979, loss: 0.0097, time: 0.0029\n",
            "batch: 504/979, loss: 0.0097, time: 0.0029\n",
            "batch: 505/979, loss: 0.0097, time: 0.0030\n",
            "batch: 506/979, loss: 0.0097, time: 0.0028\n",
            "batch: 507/979, loss: 0.0097, time: 0.0029\n",
            "batch: 508/979, loss: 0.0097, time: 0.0028\n",
            "batch: 509/979, loss: 0.0097, time: 0.0028\n",
            "batch: 510/979, loss: 0.0097, time: 0.0029\n",
            "batch: 511/979, loss: 0.0097, time: 0.0029\n",
            "batch: 512/979, loss: 0.0097, time: 0.0029\n",
            "batch: 513/979, loss: 0.0097, time: 0.0028\n",
            "batch: 514/979, loss: 0.0097, time: 0.0028\n",
            "batch: 515/979, loss: 0.0096, time: 0.0028\n",
            "batch: 516/979, loss: 0.0096, time: 0.0028\n",
            "batch: 517/979, loss: 0.0096, time: 0.0038\n",
            "batch: 518/979, loss: 0.0096, time: 0.0038\n",
            "batch: 519/979, loss: 0.0096, time: 0.0029\n",
            "batch: 520/979, loss: 0.0096, time: 0.0028\n",
            "batch: 521/979, loss: 0.0096, time: 0.0028\n",
            "batch: 522/979, loss: 0.0096, time: 0.0029\n",
            "batch: 523/979, loss: 0.0096, time: 0.0036\n",
            "batch: 524/979, loss: 0.0096, time: 0.0029\n",
            "batch: 525/979, loss: 0.0095, time: 0.0030\n",
            "batch: 526/979, loss: 0.0095, time: 0.0029\n",
            "batch: 527/979, loss: 0.0095, time: 0.0029\n",
            "batch: 528/979, loss: 0.0095, time: 0.0028\n",
            "batch: 529/979, loss: 0.0095, time: 0.0031\n",
            "batch: 530/979, loss: 0.0095, time: 0.0063\n",
            "batch: 531/979, loss: 0.0095, time: 0.0029\n",
            "batch: 532/979, loss: 0.0094, time: 0.0030\n",
            "batch: 533/979, loss: 0.0095, time: 0.0028\n",
            "batch: 534/979, loss: 0.0095, time: 0.0029\n",
            "batch: 535/979, loss: 0.0095, time: 0.0030\n",
            "batch: 536/979, loss: 0.0094, time: 0.0029\n",
            "batch: 537/979, loss: 0.0097, time: 0.0028\n",
            "batch: 538/979, loss: 0.0097, time: 0.0028\n",
            "batch: 539/979, loss: 0.0097, time: 0.0029\n",
            "batch: 540/979, loss: 0.0097, time: 0.0029\n",
            "batch: 541/979, loss: 0.0097, time: 0.0029\n",
            "batch: 542/979, loss: 0.0097, time: 0.0029\n",
            "batch: 543/979, loss: 0.0097, time: 0.0028\n",
            "batch: 544/979, loss: 0.0097, time: 0.0029\n",
            "batch: 545/979, loss: 0.0097, time: 0.0028\n",
            "batch: 546/979, loss: 0.0097, time: 0.0029\n",
            "batch: 547/979, loss: 0.0097, time: 0.0028\n",
            "batch: 548/979, loss: 0.0096, time: 0.0028\n",
            "batch: 549/979, loss: 0.0096, time: 0.0029\n",
            "batch: 550/979, loss: 0.0097, time: 0.0029\n",
            "batch: 551/979, loss: 0.0097, time: 0.0028\n",
            "batch: 552/979, loss: 0.0097, time: 0.0035\n",
            "batch: 553/979, loss: 0.0097, time: 0.0029\n",
            "batch: 554/979, loss: 0.0097, time: 0.0028\n",
            "batch: 555/979, loss: 0.0097, time: 0.0029\n",
            "batch: 556/979, loss: 0.0097, time: 0.0029\n",
            "batch: 557/979, loss: 0.0097, time: 0.0028\n",
            "batch: 558/979, loss: 0.0097, time: 0.0029\n",
            "batch: 559/979, loss: 0.0097, time: 0.0028\n",
            "batch: 560/979, loss: 0.0100, time: 0.0029\n",
            "batch: 561/979, loss: 0.0101, time: 0.0029\n",
            "batch: 562/979, loss: 0.0101, time: 0.0028\n",
            "batch: 563/979, loss: 0.0100, time: 0.0041\n",
            "batch: 564/979, loss: 0.0100, time: 0.0029\n",
            "batch: 565/979, loss: 0.0100, time: 0.0028\n",
            "batch: 566/979, loss: 0.0100, time: 0.0029\n",
            "batch: 567/979, loss: 0.0100, time: 0.0029\n",
            "batch: 568/979, loss: 0.0101, time: 0.0028\n",
            "batch: 569/979, loss: 0.0101, time: 0.0029\n",
            "batch: 570/979, loss: 0.0101, time: 0.0031\n",
            "batch: 571/979, loss: 0.0101, time: 0.0029\n",
            "batch: 572/979, loss: 0.0101, time: 0.0028\n",
            "batch: 573/979, loss: 0.0101, time: 0.0028\n",
            "batch: 574/979, loss: 0.0101, time: 0.0029\n",
            "batch: 575/979, loss: 0.0101, time: 0.0029\n",
            "batch: 576/979, loss: 0.0101, time: 0.0031\n",
            "batch: 577/979, loss: 0.0100, time: 0.0029\n",
            "batch: 578/979, loss: 0.0100, time: 0.0028\n",
            "batch: 579/979, loss: 0.0100, time: 0.0028\n",
            "batch: 580/979, loss: 0.0100, time: 0.0029\n",
            "batch: 581/979, loss: 0.0100, time: 0.0028\n",
            "batch: 582/979, loss: 0.0100, time: 0.0037\n",
            "batch: 583/979, loss: 0.0101, time: 0.0030\n",
            "batch: 584/979, loss: 0.0101, time: 0.0028\n",
            "batch: 585/979, loss: 0.0100, time: 0.0028\n",
            "batch: 586/979, loss: 0.0100, time: 0.0028\n",
            "batch: 587/979, loss: 0.0100, time: 0.0028\n",
            "batch: 588/979, loss: 0.0100, time: 0.0028\n",
            "batch: 589/979, loss: 0.0100, time: 0.0028\n",
            "batch: 590/979, loss: 0.0100, time: 0.0028\n",
            "batch: 591/979, loss: 0.0100, time: 0.0028\n",
            "batch: 592/979, loss: 0.0100, time: 0.0044\n",
            "batch: 593/979, loss: 0.0100, time: 0.0043\n",
            "batch: 594/979, loss: 0.0100, time: 0.0052\n",
            "batch: 595/979, loss: 0.0100, time: 0.0044\n",
            "batch: 596/979, loss: 0.0100, time: 0.0030\n",
            "batch: 597/979, loss: 0.0100, time: 0.0029\n",
            "batch: 598/979, loss: 0.0100, time: 0.0028\n",
            "batch: 599/979, loss: 0.0100, time: 0.0028\n",
            "batch: 600/979, loss: 0.0100, time: 0.0029\n",
            "batch: 601/979, loss: 0.0100, time: 0.0029\n",
            "batch: 602/979, loss: 0.0100, time: 0.0029\n",
            "batch: 603/979, loss: 0.0100, time: 0.0028\n",
            "batch: 604/979, loss: 0.0100, time: 0.0029\n",
            "batch: 605/979, loss: 0.0100, time: 0.0028\n",
            "batch: 606/979, loss: 0.0099, time: 0.0028\n",
            "batch: 607/979, loss: 0.0099, time: 0.0028\n",
            "batch: 608/979, loss: 0.0099, time: 0.0028\n",
            "batch: 609/979, loss: 0.0099, time: 0.0030\n",
            "batch: 610/979, loss: 0.0099, time: 0.0029\n",
            "batch: 611/979, loss: 0.0099, time: 0.0032\n",
            "batch: 612/979, loss: 0.0099, time: 0.0030\n",
            "batch: 613/979, loss: 0.0099, time: 0.0028\n",
            "batch: 614/979, loss: 0.0099, time: 0.0028\n",
            "batch: 615/979, loss: 0.0099, time: 0.0029\n",
            "batch: 616/979, loss: 0.0098, time: 0.0029\n",
            "batch: 617/979, loss: 0.0098, time: 0.0029\n",
            "batch: 618/979, loss: 0.0098, time: 0.0028\n",
            "batch: 619/979, loss: 0.0098, time: 0.0028\n",
            "batch: 620/979, loss: 0.0098, time: 0.0029\n",
            "batch: 621/979, loss: 0.0098, time: 0.0029\n",
            "batch: 622/979, loss: 0.0098, time: 0.0032\n",
            "batch: 623/979, loss: 0.0098, time: 0.0032\n",
            "batch: 624/979, loss: 0.0098, time: 0.0038\n",
            "batch: 625/979, loss: 0.0097, time: 0.0056\n",
            "batch: 626/979, loss: 0.0097, time: 0.0030\n",
            "batch: 627/979, loss: 0.0097, time: 0.0029\n",
            "batch: 628/979, loss: 0.0097, time: 0.0029\n",
            "batch: 629/979, loss: 0.0097, time: 0.0029\n",
            "batch: 630/979, loss: 0.0097, time: 0.0028\n",
            "batch: 631/979, loss: 0.0097, time: 0.0028\n",
            "batch: 632/979, loss: 0.0097, time: 0.0029\n",
            "batch: 633/979, loss: 0.0097, time: 0.0028\n",
            "batch: 634/979, loss: 0.0097, time: 0.0028\n",
            "batch: 635/979, loss: 0.0097, time: 0.0028\n",
            "batch: 636/979, loss: 0.0097, time: 0.0029\n",
            "batch: 637/979, loss: 0.0097, time: 0.0029\n",
            "batch: 638/979, loss: 0.0097, time: 0.0028\n",
            "batch: 639/979, loss: 0.0096, time: 0.0029\n",
            "batch: 640/979, loss: 0.0098, time: 0.0029\n",
            "batch: 641/979, loss: 0.0098, time: 0.0029\n",
            "batch: 642/979, loss: 0.0098, time: 0.0028\n",
            "batch: 643/979, loss: 0.0098, time: 0.0028\n",
            "batch: 644/979, loss: 0.0098, time: 0.0050\n",
            "batch: 645/979, loss: 0.0098, time: 0.0029\n",
            "batch: 646/979, loss: 0.0098, time: 0.0032\n",
            "batch: 647/979, loss: 0.0098, time: 0.0029\n",
            "batch: 648/979, loss: 0.0098, time: 0.0028\n",
            "batch: 649/979, loss: 0.0098, time: 0.0030\n",
            "batch: 650/979, loss: 0.0098, time: 0.0029\n",
            "batch: 651/979, loss: 0.0098, time: 0.0030\n",
            "batch: 652/979, loss: 0.0098, time: 0.0030\n",
            "batch: 653/979, loss: 0.0098, time: 0.0030\n",
            "batch: 654/979, loss: 0.0098, time: 0.0032\n",
            "batch: 655/979, loss: 0.0097, time: 0.0030\n",
            "batch: 656/979, loss: 0.0097, time: 0.0029\n",
            "batch: 657/979, loss: 0.0097, time: 0.0039\n",
            "batch: 658/979, loss: 0.0097, time: 0.0038\n",
            "batch: 659/979, loss: 0.0097, time: 0.0029\n",
            "batch: 660/979, loss: 0.0097, time: 0.0033\n",
            "batch: 661/979, loss: 0.0097, time: 0.0028\n",
            "batch: 662/979, loss: 0.0097, time: 0.0029\n",
            "batch: 663/979, loss: 0.0097, time: 0.0029\n",
            "batch: 664/979, loss: 0.0097, time: 0.0028\n",
            "batch: 665/979, loss: 0.0097, time: 0.0028\n",
            "batch: 666/979, loss: 0.0097, time: 0.0028\n",
            "batch: 667/979, loss: 0.0096, time: 0.0029\n",
            "batch: 668/979, loss: 0.0096, time: 0.0028\n",
            "batch: 669/979, loss: 0.0096, time: 0.0028\n",
            "batch: 670/979, loss: 0.0096, time: 0.0028\n",
            "batch: 671/979, loss: 0.0096, time: 0.0028\n",
            "batch: 672/979, loss: 0.0096, time: 0.0033\n",
            "batch: 673/979, loss: 0.0096, time: 0.0029\n",
            "batch: 674/979, loss: 0.0096, time: 0.0030\n",
            "batch: 675/979, loss: 0.0097, time: 0.0038\n",
            "batch: 676/979, loss: 0.0097, time: 0.0029\n",
            "batch: 677/979, loss: 0.0097, time: 0.0029\n",
            "batch: 678/979, loss: 0.0097, time: 0.0036\n",
            "batch: 679/979, loss: 0.0097, time: 0.0029\n",
            "batch: 680/979, loss: 0.0097, time: 0.0028\n",
            "batch: 681/979, loss: 0.0096, time: 0.0028\n",
            "batch: 682/979, loss: 0.0096, time: 0.0030\n",
            "batch: 683/979, loss: 0.0096, time: 0.0028\n",
            "batch: 684/979, loss: 0.0096, time: 0.0028\n",
            "batch: 685/979, loss: 0.0096, time: 0.0028\n",
            "batch: 686/979, loss: 0.0096, time: 0.0028\n",
            "batch: 687/979, loss: 0.0096, time: 0.0028\n",
            "batch: 688/979, loss: 0.0096, time: 0.0028\n",
            "batch: 689/979, loss: 0.0096, time: 0.0038\n",
            "batch: 690/979, loss: 0.0096, time: 0.0040\n",
            "batch: 691/979, loss: 0.0096, time: 0.0029\n",
            "batch: 692/979, loss: 0.0096, time: 0.0029\n",
            "batch: 693/979, loss: 0.0096, time: 0.0029\n",
            "batch: 694/979, loss: 0.0096, time: 0.0029\n",
            "batch: 695/979, loss: 0.0095, time: 0.0028\n",
            "batch: 696/979, loss: 0.0095, time: 0.0029\n",
            "batch: 697/979, loss: 0.0095, time: 0.0029\n",
            "batch: 698/979, loss: 0.0095, time: 0.0029\n",
            "batch: 699/979, loss: 0.0095, time: 0.0028\n",
            "batch: 700/979, loss: 0.0095, time: 0.0030\n",
            "batch: 701/979, loss: 0.0095, time: 0.0028\n",
            "batch: 702/979, loss: 0.0095, time: 0.0028\n",
            "batch: 703/979, loss: 0.0095, time: 0.0028\n",
            "batch: 704/979, loss: 0.0095, time: 0.0030\n",
            "batch: 705/979, loss: 0.0095, time: 0.0047\n",
            "batch: 706/979, loss: 0.0095, time: 0.0028\n",
            "batch: 707/979, loss: 0.0095, time: 0.0029\n",
            "batch: 708/979, loss: 0.0094, time: 0.0028\n",
            "batch: 709/979, loss: 0.0094, time: 0.0028\n",
            "batch: 710/979, loss: 0.0094, time: 0.0029\n",
            "batch: 711/979, loss: 0.0094, time: 0.0028\n",
            "batch: 712/979, loss: 0.0094, time: 0.0028\n",
            "batch: 713/979, loss: 0.0094, time: 0.0029\n",
            "batch: 714/979, loss: 0.0094, time: 0.0029\n",
            "batch: 715/979, loss: 0.0094, time: 0.0028\n",
            "batch: 716/979, loss: 0.0094, time: 0.0028\n",
            "batch: 717/979, loss: 0.0094, time: 0.0028\n",
            "batch: 718/979, loss: 0.0094, time: 0.0029\n",
            "batch: 719/979, loss: 0.0093, time: 0.0028\n",
            "batch: 720/979, loss: 0.0093, time: 0.0028\n",
            "batch: 721/979, loss: 0.0093, time: 0.0036\n",
            "batch: 722/979, loss: 0.0093, time: 0.0057\n",
            "batch: 723/979, loss: 0.0093, time: 0.0029\n",
            "batch: 724/979, loss: 0.0093, time: 0.0030\n",
            "batch: 725/979, loss: 0.0093, time: 0.0030\n",
            "batch: 726/979, loss: 0.0093, time: 0.0029\n",
            "batch: 727/979, loss: 0.0093, time: 0.0029\n",
            "batch: 728/979, loss: 0.0093, time: 0.0028\n",
            "batch: 729/979, loss: 0.0092, time: 0.0029\n",
            "batch: 730/979, loss: 0.0092, time: 0.0028\n",
            "batch: 731/979, loss: 0.0092, time: 0.0028\n",
            "batch: 732/979, loss: 0.0092, time: 0.0028\n",
            "batch: 733/979, loss: 0.0092, time: 0.0029\n",
            "batch: 734/979, loss: 0.0093, time: 0.0029\n",
            "batch: 735/979, loss: 0.0092, time: 0.0030\n",
            "batch: 736/979, loss: 0.0092, time: 0.0029\n",
            "batch: 737/979, loss: 0.0092, time: 0.0029\n",
            "batch: 738/979, loss: 0.0092, time: 0.0030\n",
            "batch: 739/979, loss: 0.0094, time: 0.0028\n",
            "batch: 740/979, loss: 0.0093, time: 0.0028\n",
            "batch: 741/979, loss: 0.0094, time: 0.0029\n",
            "batch: 742/979, loss: 0.0094, time: 0.0028\n",
            "batch: 743/979, loss: 0.0094, time: 0.0028\n",
            "batch: 744/979, loss: 0.0094, time: 0.0029\n",
            "batch: 745/979, loss: 0.0094, time: 0.0028\n",
            "batch: 746/979, loss: 0.0094, time: 0.0028\n",
            "batch: 747/979, loss: 0.0094, time: 0.0028\n",
            "batch: 748/979, loss: 0.0094, time: 0.0028\n",
            "batch: 749/979, loss: 0.0094, time: 0.0028\n",
            "batch: 750/979, loss: 0.0094, time: 0.0028\n",
            "batch: 751/979, loss: 0.0094, time: 0.0028\n",
            "batch: 752/979, loss: 0.0094, time: 0.0035\n",
            "batch: 753/979, loss: 0.0094, time: 0.0045\n",
            "batch: 754/979, loss: 0.0094, time: 0.0061\n",
            "batch: 755/979, loss: 0.0094, time: 0.0044\n",
            "batch: 756/979, loss: 0.0094, time: 0.0037\n",
            "batch: 757/979, loss: 0.0094, time: 0.0029\n",
            "batch: 758/979, loss: 0.0096, time: 0.0030\n",
            "batch: 759/979, loss: 0.0096, time: 0.0029\n",
            "batch: 760/979, loss: 0.0096, time: 0.0028\n",
            "batch: 761/979, loss: 0.0096, time: 0.0029\n",
            "batch: 762/979, loss: 0.0096, time: 0.0029\n",
            "batch: 763/979, loss: 0.0096, time: 0.0029\n",
            "batch: 764/979, loss: 0.0096, time: 0.0030\n",
            "batch: 765/979, loss: 0.0096, time: 0.0029\n",
            "batch: 766/979, loss: 0.0096, time: 0.0030\n",
            "batch: 767/979, loss: 0.0096, time: 0.0029\n",
            "batch: 768/979, loss: 0.0096, time: 0.0029\n",
            "batch: 769/979, loss: 0.0097, time: 0.0030\n",
            "batch: 770/979, loss: 0.0097, time: 0.0028\n",
            "batch: 771/979, loss: 0.0096, time: 0.0029\n",
            "batch: 772/979, loss: 0.0096, time: 0.0029\n",
            "batch: 773/979, loss: 0.0096, time: 0.0028\n",
            "batch: 774/979, loss: 0.0096, time: 0.0029\n",
            "batch: 775/979, loss: 0.0096, time: 0.0028\n",
            "batch: 776/979, loss: 0.0096, time: 0.0028\n",
            "batch: 777/979, loss: 0.0096, time: 0.0029\n",
            "batch: 778/979, loss: 0.0096, time: 0.0034\n",
            "batch: 779/979, loss: 0.0096, time: 0.0038\n",
            "batch: 780/979, loss: 0.0096, time: 0.0038\n",
            "batch: 781/979, loss: 0.0096, time: 0.0029\n",
            "batch: 782/979, loss: 0.0096, time: 0.0032\n",
            "batch: 783/979, loss: 0.0096, time: 0.0029\n",
            "batch: 784/979, loss: 0.0096, time: 0.0037\n",
            "batch: 785/979, loss: 0.0096, time: 0.0046\n",
            "batch: 786/979, loss: 0.0096, time: 0.0029\n",
            "batch: 787/979, loss: 0.0096, time: 0.0029\n",
            "batch: 788/979, loss: 0.0096, time: 0.0028\n",
            "batch: 789/979, loss: 0.0095, time: 0.0029\n",
            "batch: 790/979, loss: 0.0095, time: 0.0028\n",
            "batch: 791/979, loss: 0.0095, time: 0.0028\n",
            "batch: 792/979, loss: 0.0095, time: 0.0029\n",
            "batch: 793/979, loss: 0.0095, time: 0.0030\n",
            "batch: 794/979, loss: 0.0095, time: 0.0029\n",
            "batch: 795/979, loss: 0.0095, time: 0.0029\n",
            "batch: 796/979, loss: 0.0095, time: 0.0028\n",
            "batch: 797/979, loss: 0.0095, time: 0.0029\n",
            "batch: 798/979, loss: 0.0095, time: 0.0029\n",
            "batch: 799/979, loss: 0.0095, time: 0.0028\n",
            "batch: 800/979, loss: 0.0095, time: 0.0029\n",
            "batch: 801/979, loss: 0.0095, time: 0.0029\n",
            "batch: 802/979, loss: 0.0095, time: 0.0029\n",
            "batch: 803/979, loss: 0.0095, time: 0.0028\n",
            "batch: 804/979, loss: 0.0095, time: 0.0030\n",
            "batch: 805/979, loss: 0.0095, time: 0.0028\n",
            "batch: 806/979, loss: 0.0095, time: 0.0028\n",
            "batch: 807/979, loss: 0.0095, time: 0.0028\n",
            "batch: 808/979, loss: 0.0094, time: 0.0028\n",
            "batch: 809/979, loss: 0.0094, time: 0.0029\n",
            "batch: 810/979, loss: 0.0094, time: 0.0029\n",
            "batch: 811/979, loss: 0.0094, time: 0.0029\n",
            "batch: 812/979, loss: 0.0094, time: 0.0028\n",
            "batch: 813/979, loss: 0.0096, time: 0.0032\n",
            "batch: 814/979, loss: 0.0096, time: 0.0029\n",
            "batch: 815/979, loss: 0.0096, time: 0.0028\n",
            "batch: 816/979, loss: 0.0098, time: 0.0038\n",
            "batch: 817/979, loss: 0.0098, time: 0.0041\n",
            "batch: 818/979, loss: 0.0098, time: 0.0029\n",
            "batch: 819/979, loss: 0.0098, time: 0.0028\n",
            "batch: 820/979, loss: 0.0098, time: 0.0028\n",
            "batch: 821/979, loss: 0.0098, time: 0.0031\n",
            "batch: 822/979, loss: 0.0098, time: 0.0028\n",
            "batch: 823/979, loss: 0.0098, time: 0.0028\n",
            "batch: 824/979, loss: 0.0098, time: 0.0029\n",
            "batch: 825/979, loss: 0.0098, time: 0.0029\n",
            "batch: 826/979, loss: 0.0098, time: 0.0029\n",
            "batch: 827/979, loss: 0.0098, time: 0.0029\n",
            "batch: 828/979, loss: 0.0098, time: 0.0037\n",
            "batch: 829/979, loss: 0.0098, time: 0.0029\n",
            "batch: 830/979, loss: 0.0098, time: 0.0029\n",
            "batch: 831/979, loss: 0.0098, time: 0.0028\n",
            "batch: 832/979, loss: 0.0098, time: 0.0028\n",
            "batch: 833/979, loss: 0.0098, time: 0.0042\n",
            "batch: 834/979, loss: 0.0098, time: 0.0033\n",
            "batch: 835/979, loss: 0.0098, time: 0.0030\n",
            "batch: 836/979, loss: 0.0098, time: 0.0028\n",
            "batch: 837/979, loss: 0.0098, time: 0.0028\n",
            "batch: 838/979, loss: 0.0097, time: 0.0029\n",
            "batch: 839/979, loss: 0.0100, time: 0.0030\n",
            "batch: 840/979, loss: 0.0100, time: 0.0028\n",
            "batch: 841/979, loss: 0.0100, time: 0.0028\n",
            "batch: 842/979, loss: 0.0100, time: 0.0029\n",
            "batch: 843/979, loss: 0.0101, time: 0.0029\n",
            "batch: 844/979, loss: 0.0101, time: 0.0029\n",
            "batch: 845/979, loss: 0.0101, time: 0.0028\n",
            "batch: 846/979, loss: 0.0101, time: 0.0028\n",
            "batch: 847/979, loss: 0.0101, time: 0.0029\n",
            "batch: 848/979, loss: 0.0101, time: 0.0055\n",
            "batch: 849/979, loss: 0.0101, time: 0.0039\n",
            "batch: 850/979, loss: 0.0101, time: 0.0030\n",
            "batch: 851/979, loss: 0.0101, time: 0.0029\n",
            "batch: 852/979, loss: 0.0101, time: 0.0029\n",
            "batch: 853/979, loss: 0.0101, time: 0.0030\n",
            "batch: 854/979, loss: 0.0101, time: 0.0029\n",
            "batch: 855/979, loss: 0.0101, time: 0.0031\n",
            "batch: 856/979, loss: 0.0102, time: 0.0028\n",
            "batch: 857/979, loss: 0.0102, time: 0.0029\n",
            "batch: 858/979, loss: 0.0101, time: 0.0029\n",
            "batch: 859/979, loss: 0.0101, time: 0.0028\n",
            "batch: 860/979, loss: 0.0101, time: 0.0030\n",
            "batch: 861/979, loss: 0.0102, time: 0.0028\n",
            "batch: 862/979, loss: 0.0101, time: 0.0030\n",
            "batch: 863/979, loss: 0.0101, time: 0.0028\n",
            "batch: 864/979, loss: 0.0102, time: 0.0028\n",
            "batch: 865/979, loss: 0.0102, time: 0.0029\n",
            "batch: 866/979, loss: 0.0101, time: 0.0029\n",
            "batch: 867/979, loss: 0.0102, time: 0.0028\n",
            "batch: 868/979, loss: 0.0102, time: 0.0030\n",
            "batch: 869/979, loss: 0.0102, time: 0.0028\n",
            "batch: 870/979, loss: 0.0102, time: 0.0029\n",
            "batch: 871/979, loss: 0.0102, time: 0.0028\n",
            "batch: 872/979, loss: 0.0102, time: 0.0028\n",
            "batch: 873/979, loss: 0.0102, time: 0.0029\n",
            "batch: 874/979, loss: 0.0102, time: 0.0030\n",
            "batch: 875/979, loss: 0.0102, time: 0.0028\n",
            "batch: 876/979, loss: 0.0102, time: 0.0029\n",
            "batch: 877/979, loss: 0.0102, time: 0.0028\n",
            "batch: 878/979, loss: 0.0102, time: 0.0030\n",
            "batch: 879/979, loss: 0.0102, time: 0.0028\n",
            "batch: 880/979, loss: 0.0102, time: 0.0038\n",
            "batch: 881/979, loss: 0.0102, time: 0.0044\n",
            "batch: 882/979, loss: 0.0102, time: 0.0036\n",
            "batch: 883/979, loss: 0.0102, time: 0.0029\n",
            "batch: 884/979, loss: 0.0102, time: 0.0032\n",
            "batch: 885/979, loss: 0.0103, time: 0.0029\n",
            "batch: 886/979, loss: 0.0103, time: 0.0029\n",
            "batch: 887/979, loss: 0.0103, time: 0.0028\n",
            "batch: 888/979, loss: 0.0102, time: 0.0029\n",
            "batch: 889/979, loss: 0.0102, time: 0.0031\n",
            "batch: 890/979, loss: 0.0102, time: 0.0031\n",
            "batch: 891/979, loss: 0.0103, time: 0.0060\n",
            "batch: 892/979, loss: 0.0103, time: 0.0029\n",
            "batch: 893/979, loss: 0.0103, time: 0.0029\n",
            "batch: 894/979, loss: 0.0103, time: 0.0029\n",
            "batch: 895/979, loss: 0.0103, time: 0.0028\n",
            "batch: 896/979, loss: 0.0103, time: 0.0029\n",
            "batch: 897/979, loss: 0.0102, time: 0.0029\n",
            "batch: 898/979, loss: 0.0102, time: 0.0030\n",
            "batch: 899/979, loss: 0.0102, time: 0.0029\n",
            "batch: 900/979, loss: 0.0102, time: 0.0029\n",
            "batch: 901/979, loss: 0.0102, time: 0.0028\n",
            "batch: 902/979, loss: 0.0102, time: 0.0032\n",
            "batch: 903/979, loss: 0.0102, time: 0.0028\n",
            "batch: 904/979, loss: 0.0104, time: 0.0029\n",
            "batch: 905/979, loss: 0.0104, time: 0.0029\n",
            "batch: 906/979, loss: 0.0104, time: 0.0031\n",
            "batch: 907/979, loss: 0.0104, time: 0.0029\n",
            "batch: 908/979, loss: 0.0103, time: 0.0029\n",
            "batch: 909/979, loss: 0.0103, time: 0.0028\n",
            "batch: 910/979, loss: 0.0103, time: 0.0028\n",
            "batch: 911/979, loss: 0.0103, time: 0.0055\n",
            "batch: 912/979, loss: 0.0103, time: 0.0089\n",
            "batch: 913/979, loss: 0.0103, time: 0.0043\n",
            "batch: 914/979, loss: 0.0103, time: 0.0045\n",
            "batch: 915/979, loss: 0.0103, time: 0.0029\n",
            "batch: 916/979, loss: 0.0103, time: 0.0028\n",
            "batch: 917/979, loss: 0.0103, time: 0.0036\n",
            "batch: 918/979, loss: 0.0103, time: 0.0030\n",
            "batch: 919/979, loss: 0.0103, time: 0.0029\n",
            "batch: 920/979, loss: 0.0103, time: 0.0030\n",
            "batch: 921/979, loss: 0.0103, time: 0.0030\n",
            "batch: 922/979, loss: 0.0103, time: 0.0029\n",
            "batch: 923/979, loss: 0.0103, time: 0.0029\n",
            "batch: 924/979, loss: 0.0103, time: 0.0030\n",
            "batch: 925/979, loss: 0.0103, time: 0.0030\n",
            "batch: 926/979, loss: 0.0102, time: 0.0030\n",
            "batch: 927/979, loss: 0.0102, time: 0.0031\n",
            "batch: 928/979, loss: 0.0103, time: 0.0030\n",
            "batch: 929/979, loss: 0.0103, time: 0.0033\n",
            "batch: 930/979, loss: 0.0103, time: 0.0038\n",
            "batch: 931/979, loss: 0.0103, time: 0.0037\n",
            "batch: 932/979, loss: 0.0103, time: 0.0038\n",
            "batch: 933/979, loss: 0.0103, time: 0.0029\n",
            "batch: 934/979, loss: 0.0103, time: 0.0034\n",
            "batch: 935/979, loss: 0.0102, time: 0.0033\n",
            "batch: 936/979, loss: 0.0103, time: 0.0029\n",
            "batch: 937/979, loss: 0.0104, time: 0.0029\n",
            "batch: 938/979, loss: 0.0104, time: 0.0031\n",
            "batch: 939/979, loss: 0.0104, time: 0.0031\n",
            "batch: 940/979, loss: 0.0104, time: 0.0029\n",
            "batch: 941/979, loss: 0.0104, time: 0.0048\n",
            "batch: 942/979, loss: 0.0104, time: 0.0033\n",
            "batch: 943/979, loss: 0.0104, time: 0.0032\n",
            "batch: 944/979, loss: 0.0104, time: 0.0031\n",
            "batch: 945/979, loss: 0.0104, time: 0.0030\n",
            "batch: 946/979, loss: 0.0104, time: 0.0029\n",
            "batch: 947/979, loss: 0.0104, time: 0.0028\n",
            "batch: 948/979, loss: 0.0104, time: 0.0029\n",
            "batch: 949/979, loss: 0.0104, time: 0.0029\n",
            "batch: 950/979, loss: 0.0104, time: 0.0029\n",
            "batch: 951/979, loss: 0.0104, time: 0.0039\n",
            "batch: 952/979, loss: 0.0104, time: 0.0033\n",
            "batch: 953/979, loss: 0.0104, time: 0.0033\n",
            "batch: 954/979, loss: 0.0104, time: 0.0030\n",
            "batch: 955/979, loss: 0.0105, time: 0.0029\n",
            "batch: 956/979, loss: 0.0104, time: 0.0028\n",
            "batch: 957/979, loss: 0.0104, time: 0.0028\n",
            "batch: 958/979, loss: 0.0104, time: 0.0029\n",
            "batch: 959/979, loss: 0.0104, time: 0.0028\n",
            "batch: 960/979, loss: 0.0104, time: 0.0028\n",
            "batch: 961/979, loss: 0.0104, time: 0.0028\n",
            "batch: 962/979, loss: 0.0104, time: 0.0028\n",
            "batch: 963/979, loss: 0.0104, time: 0.0028\n",
            "batch: 964/979, loss: 0.0104, time: 0.0028\n",
            "batch: 965/979, loss: 0.0105, time: 0.0028\n",
            "batch: 966/979, loss: 0.0105, time: 0.0028\n",
            "batch: 967/979, loss: 0.0105, time: 0.0028\n",
            "batch: 968/979, loss: 0.0105, time: 0.0028\n",
            "batch: 969/979, loss: 0.0106, time: 0.0029\n",
            "batch: 970/979, loss: 0.0106, time: 0.0028\n",
            "batch: 971/979, loss: 0.0106, time: 0.0028\n",
            "batch: 972/979, loss: 0.0106, time: 0.0049\n",
            "batch: 973/979, loss: 0.0106, time: 0.0035\n",
            "batch: 974/979, loss: 0.0105, time: 0.0028\n",
            "batch: 975/979, loss: 0.0105, time: 0.0029\n",
            "batch: 976/979, loss: 0.0105, time: 0.0029\n",
            "batch: 977/979, loss: 0.0105, time: 0.0028\n",
            "batch: 978/979, loss: 0.0105, time: 0.0043\n",
            "batch: 979/979, loss: 0.0106, time: 0.0029\n",
            "batch: 1/979, loss: 0.0245, time: 0.0043\n",
            "batch: 2/979, loss: 0.0142, time: 0.0030\n",
            "batch: 3/979, loss: 0.0104, time: 0.0030\n",
            "batch: 4/979, loss: 0.0085, time: 0.0029\n",
            "batch: 5/979, loss: 0.0261, time: 0.0029\n",
            "batch: 6/979, loss: 0.0221, time: 0.0028\n",
            "batch: 7/979, loss: 0.0191, time: 0.0028\n",
            "batch: 8/979, loss: 0.0171, time: 0.0029\n",
            "batch: 9/979, loss: 0.0154, time: 0.0030\n",
            "batch: 10/979, loss: 0.0140, time: 0.0029\n",
            "batch: 11/979, loss: 0.0133, time: 0.0029\n",
            "batch: 12/979, loss: 0.0123, time: 0.0031\n",
            "batch: 13/979, loss: 0.0114, time: 0.0028\n",
            "batch: 14/979, loss: 0.0108, time: 0.0028\n",
            "batch: 15/979, loss: 0.0102, time: 0.0032\n",
            "batch: 16/979, loss: 0.0100, time: 0.0031\n",
            "batch: 17/979, loss: 0.0105, time: 0.0030\n",
            "batch: 18/979, loss: 0.0102, time: 0.0029\n",
            "batch: 19/979, loss: 0.0106, time: 0.0030\n",
            "batch: 20/979, loss: 0.0102, time: 0.0029\n",
            "batch: 21/979, loss: 0.0104, time: 0.0029\n",
            "batch: 22/979, loss: 0.0106, time: 0.0029\n",
            "batch: 23/979, loss: 0.0165, time: 0.0029\n",
            "batch: 24/979, loss: 0.0160, time: 0.0029\n",
            "batch: 25/979, loss: 0.0154, time: 0.0029\n",
            "batch: 26/979, loss: 0.0151, time: 0.0028\n",
            "batch: 27/979, loss: 0.0146, time: 0.0029\n",
            "batch: 28/979, loss: 0.0143, time: 0.0029\n",
            "batch: 29/979, loss: 0.0139, time: 0.0029\n",
            "batch: 30/979, loss: 0.0145, time: 0.0028\n",
            "batch: 31/979, loss: 0.0147, time: 0.0028\n",
            "batch: 32/979, loss: 0.0145, time: 0.0028\n",
            "batch: 33/979, loss: 0.0143, time: 0.0029\n",
            "batch: 34/979, loss: 0.0140, time: 0.0063\n",
            "batch: 35/979, loss: 0.0136, time: 0.0030\n",
            "batch: 36/979, loss: 0.0134, time: 0.0029\n",
            "batch: 37/979, loss: 0.0131, time: 0.0028\n",
            "batch: 38/979, loss: 0.0128, time: 0.0028\n",
            "batch: 39/979, loss: 0.0125, time: 0.0029\n",
            "batch: 40/979, loss: 0.0122, time: 0.0028\n",
            "batch: 41/979, loss: 0.0119, time: 0.0029\n",
            "batch: 42/979, loss: 0.0117, time: 0.0028\n",
            "batch: 43/979, loss: 0.0116, time: 0.0039\n",
            "batch: 44/979, loss: 0.0116, time: 0.0045\n",
            "batch: 45/979, loss: 0.0114, time: 0.0044\n",
            "batch: 46/979, loss: 0.0112, time: 0.0042\n",
            "batch: 47/979, loss: 0.0111, time: 0.0029\n",
            "batch: 48/979, loss: 0.0109, time: 0.0029\n",
            "batch: 49/979, loss: 0.0116, time: 0.0029\n",
            "batch: 50/979, loss: 0.0114, time: 0.0029\n",
            "batch: 51/979, loss: 0.0115, time: 0.0029\n",
            "batch: 52/979, loss: 0.0114, time: 0.0029\n",
            "batch: 53/979, loss: 0.0113, time: 0.0029\n",
            "batch: 54/979, loss: 0.0111, time: 0.0045\n",
            "batch: 55/979, loss: 0.0109, time: 0.0029\n",
            "batch: 56/979, loss: 0.0109, time: 0.0028\n",
            "batch: 57/979, loss: 0.0108, time: 0.0041\n",
            "batch: 58/979, loss: 0.0123, time: 0.0047\n",
            "batch: 59/979, loss: 0.0123, time: 0.0042\n",
            "batch: 60/979, loss: 0.0121, time: 0.0042\n",
            "batch: 61/979, loss: 0.0119, time: 0.0036\n",
            "batch: 62/979, loss: 0.0118, time: 0.0047\n",
            "batch: 63/979, loss: 0.0117, time: 0.0045\n",
            "batch: 64/979, loss: 0.0115, time: 0.0036\n",
            "batch: 65/979, loss: 0.0114, time: 0.0031\n",
            "batch: 66/979, loss: 0.0113, time: 0.0029\n",
            "batch: 67/979, loss: 0.0116, time: 0.0028\n",
            "batch: 68/979, loss: 0.0115, time: 0.0028\n",
            "batch: 69/979, loss: 0.0114, time: 0.0029\n",
            "batch: 70/979, loss: 0.0120, time: 0.0028\n",
            "batch: 71/979, loss: 0.0126, time: 0.0028\n",
            "batch: 72/979, loss: 0.0126, time: 0.0030\n",
            "batch: 73/979, loss: 0.0124, time: 0.0029\n",
            "batch: 74/979, loss: 0.0124, time: 0.0028\n",
            "batch: 75/979, loss: 0.0122, time: 0.0029\n",
            "batch: 76/979, loss: 0.0121, time: 0.0029\n",
            "batch: 77/979, loss: 0.0120, time: 0.0028\n",
            "batch: 78/979, loss: 0.0118, time: 0.0028\n",
            "batch: 79/979, loss: 0.0118, time: 0.0028\n",
            "batch: 80/979, loss: 0.0119, time: 0.0028\n",
            "batch: 81/979, loss: 0.0117, time: 0.0029\n",
            "batch: 82/979, loss: 0.0118, time: 0.0029\n",
            "batch: 83/979, loss: 0.0117, time: 0.0028\n",
            "batch: 84/979, loss: 0.0117, time: 0.0028\n",
            "batch: 85/979, loss: 0.0116, time: 0.0029\n",
            "batch: 86/979, loss: 0.0115, time: 0.0029\n",
            "batch: 87/979, loss: 0.0123, time: 0.0029\n",
            "batch: 88/979, loss: 0.0122, time: 0.0028\n",
            "batch: 89/979, loss: 0.0121, time: 0.0028\n",
            "batch: 90/979, loss: 0.0119, time: 0.0029\n",
            "batch: 91/979, loss: 0.0118, time: 0.0029\n",
            "batch: 92/979, loss: 0.0118, time: 0.0028\n",
            "batch: 93/979, loss: 0.0117, time: 0.0029\n",
            "batch: 94/979, loss: 0.0115, time: 0.0028\n",
            "batch: 95/979, loss: 0.0115, time: 0.0028\n",
            "batch: 96/979, loss: 0.0114, time: 0.0049\n",
            "batch: 97/979, loss: 0.0113, time: 0.0037\n",
            "batch: 98/979, loss: 0.0112, time: 0.0032\n",
            "batch: 99/979, loss: 0.0111, time: 0.0029\n",
            "batch: 100/979, loss: 0.0111, time: 0.0029\n",
            "batch: 101/979, loss: 0.0110, time: 0.0029\n",
            "batch: 102/979, loss: 0.0110, time: 0.0029\n",
            "batch: 103/979, loss: 0.0109, time: 0.0029\n",
            "batch: 104/979, loss: 0.0108, time: 0.0038\n",
            "batch: 105/979, loss: 0.0107, time: 0.0028\n",
            "batch: 106/979, loss: 0.0106, time: 0.0028\n",
            "batch: 107/979, loss: 0.0105, time: 0.0029\n",
            "batch: 108/979, loss: 0.0105, time: 0.0028\n",
            "batch: 109/979, loss: 0.0104, time: 0.0036\n",
            "batch: 110/979, loss: 0.0103, time: 0.0028\n",
            "batch: 111/979, loss: 0.0103, time: 0.0029\n",
            "batch: 112/979, loss: 0.0102, time: 0.0030\n",
            "batch: 113/979, loss: 0.0102, time: 0.0029\n",
            "batch: 114/979, loss: 0.0101, time: 0.0050\n",
            "batch: 115/979, loss: 0.0100, time: 0.0029\n",
            "batch: 116/979, loss: 0.0100, time: 0.0029\n",
            "batch: 117/979, loss: 0.0099, time: 0.0028\n",
            "batch: 118/979, loss: 0.0104, time: 0.0029\n",
            "batch: 119/979, loss: 0.0103, time: 0.0031\n",
            "batch: 120/979, loss: 0.0103, time: 0.0030\n",
            "batch: 121/979, loss: 0.0102, time: 0.0028\n",
            "batch: 122/979, loss: 0.0101, time: 0.0029\n",
            "batch: 123/979, loss: 0.0101, time: 0.0028\n",
            "batch: 124/979, loss: 0.0101, time: 0.0028\n",
            "batch: 125/979, loss: 0.0100, time: 0.0028\n",
            "batch: 126/979, loss: 0.0099, time: 0.0028\n",
            "batch: 127/979, loss: 0.0100, time: 0.0028\n",
            "batch: 128/979, loss: 0.0100, time: 0.0037\n",
            "batch: 129/979, loss: 0.0099, time: 0.0029\n",
            "batch: 130/979, loss: 0.0098, time: 0.0029\n",
            "batch: 131/979, loss: 0.0098, time: 0.0030\n",
            "batch: 132/979, loss: 0.0100, time: 0.0028\n",
            "batch: 133/979, loss: 0.0100, time: 0.0028\n",
            "batch: 134/979, loss: 0.0099, time: 0.0028\n",
            "batch: 135/979, loss: 0.0098, time: 0.0029\n",
            "batch: 136/979, loss: 0.0098, time: 0.0028\n",
            "batch: 137/979, loss: 0.0098, time: 0.0028\n",
            "batch: 138/979, loss: 0.0097, time: 0.0029\n",
            "batch: 139/979, loss: 0.0097, time: 0.0028\n",
            "batch: 140/979, loss: 0.0096, time: 0.0029\n",
            "batch: 141/979, loss: 0.0095, time: 0.0043\n",
            "batch: 142/979, loss: 0.0095, time: 0.0029\n",
            "batch: 143/979, loss: 0.0095, time: 0.0030\n",
            "batch: 144/979, loss: 0.0095, time: 0.0029\n",
            "batch: 145/979, loss: 0.0094, time: 0.0030\n",
            "batch: 146/979, loss: 0.0093, time: 0.0028\n",
            "batch: 147/979, loss: 0.0093, time: 0.0028\n",
            "batch: 148/979, loss: 0.0092, time: 0.0028\n",
            "batch: 149/979, loss: 0.0092, time: 0.0028\n",
            "batch: 150/979, loss: 0.0096, time: 0.0030\n",
            "batch: 151/979, loss: 0.0096, time: 0.0031\n",
            "batch: 152/979, loss: 0.0095, time: 0.0028\n",
            "batch: 153/979, loss: 0.0095, time: 0.0028\n",
            "batch: 154/979, loss: 0.0094, time: 0.0028\n",
            "batch: 155/979, loss: 0.0095, time: 0.0028\n",
            "batch: 156/979, loss: 0.0094, time: 0.0029\n",
            "batch: 157/979, loss: 0.0094, time: 0.0029\n",
            "batch: 158/979, loss: 0.0093, time: 0.0028\n",
            "batch: 159/979, loss: 0.0093, time: 0.0031\n",
            "batch: 160/979, loss: 0.0093, time: 0.0059\n",
            "batch: 161/979, loss: 0.0092, time: 0.0029\n",
            "batch: 162/979, loss: 0.0092, time: 0.0028\n",
            "batch: 163/979, loss: 0.0091, time: 0.0028\n",
            "batch: 164/979, loss: 0.0091, time: 0.0029\n",
            "batch: 165/979, loss: 0.0093, time: 0.0029\n",
            "batch: 166/979, loss: 0.0093, time: 0.0029\n",
            "batch: 167/979, loss: 0.0092, time: 0.0028\n",
            "batch: 168/979, loss: 0.0092, time: 0.0028\n",
            "batch: 169/979, loss: 0.0091, time: 0.0030\n",
            "batch: 170/979, loss: 0.0091, time: 0.0029\n",
            "batch: 171/979, loss: 0.0090, time: 0.0028\n",
            "batch: 172/979, loss: 0.0093, time: 0.0028\n",
            "batch: 173/979, loss: 0.0093, time: 0.0028\n",
            "batch: 174/979, loss: 0.0093, time: 0.0028\n",
            "batch: 175/979, loss: 0.0092, time: 0.0028\n",
            "batch: 176/979, loss: 0.0092, time: 0.0028\n",
            "batch: 177/979, loss: 0.0092, time: 0.0028\n",
            "batch: 178/979, loss: 0.0091, time: 0.0028\n",
            "batch: 179/979, loss: 0.0091, time: 0.0045\n",
            "batch: 180/979, loss: 0.0090, time: 0.0028\n",
            "batch: 181/979, loss: 0.0091, time: 0.0028\n",
            "batch: 182/979, loss: 0.0091, time: 0.0029\n",
            "batch: 183/979, loss: 0.0090, time: 0.0028\n",
            "batch: 184/979, loss: 0.0090, time: 0.0028\n",
            "batch: 185/979, loss: 0.0090, time: 0.0028\n",
            "batch: 186/979, loss: 0.0089, time: 0.0028\n",
            "batch: 187/979, loss: 0.0089, time: 0.0028\n",
            "batch: 188/979, loss: 0.0089, time: 0.0028\n",
            "batch: 189/979, loss: 0.0089, time: 0.0028\n",
            "batch: 190/979, loss: 0.0088, time: 0.0028\n",
            "batch: 191/979, loss: 0.0088, time: 0.0028\n",
            "batch: 192/979, loss: 0.0088, time: 0.0050\n",
            "batch: 193/979, loss: 0.0087, time: 0.0029\n",
            "batch: 194/979, loss: 0.0087, time: 0.0029\n",
            "batch: 195/979, loss: 0.0087, time: 0.0028\n",
            "batch: 196/979, loss: 0.0087, time: 0.0028\n",
            "batch: 197/979, loss: 0.0087, time: 0.0028\n",
            "batch: 198/979, loss: 0.0087, time: 0.0029\n",
            "batch: 199/979, loss: 0.0086, time: 0.0029\n",
            "batch: 200/979, loss: 0.0086, time: 0.0030\n",
            "batch: 201/979, loss: 0.0086, time: 0.0043\n",
            "batch: 202/979, loss: 0.0085, time: 0.0042\n",
            "batch: 203/979, loss: 0.0085, time: 0.0043\n",
            "batch: 204/979, loss: 0.0085, time: 0.0032\n",
            "batch: 205/979, loss: 0.0085, time: 0.0029\n",
            "batch: 206/979, loss: 0.0084, time: 0.0028\n",
            "batch: 207/979, loss: 0.0084, time: 0.0028\n",
            "batch: 208/979, loss: 0.0084, time: 0.0029\n",
            "batch: 209/979, loss: 0.0085, time: 0.0029\n",
            "batch: 210/979, loss: 0.0085, time: 0.0029\n",
            "batch: 211/979, loss: 0.0085, time: 0.0029\n",
            "batch: 212/979, loss: 0.0084, time: 0.0031\n",
            "batch: 213/979, loss: 0.0084, time: 0.0038\n",
            "batch: 214/979, loss: 0.0084, time: 0.0036\n",
            "batch: 215/979, loss: 0.0083, time: 0.0037\n",
            "batch: 216/979, loss: 0.0083, time: 0.0029\n",
            "batch: 217/979, loss: 0.0083, time: 0.0028\n",
            "batch: 218/979, loss: 0.0082, time: 0.0028\n",
            "batch: 219/979, loss: 0.0082, time: 0.0028\n",
            "batch: 220/979, loss: 0.0082, time: 0.0029\n",
            "batch: 221/979, loss: 0.0082, time: 0.0028\n",
            "batch: 222/979, loss: 0.0082, time: 0.0028\n",
            "batch: 223/979, loss: 0.0081, time: 0.0028\n",
            "batch: 224/979, loss: 0.0081, time: 0.0049\n",
            "batch: 225/979, loss: 0.0081, time: 0.0033\n",
            "batch: 226/979, loss: 0.0081, time: 0.0029\n",
            "batch: 227/979, loss: 0.0080, time: 0.0029\n",
            "batch: 228/979, loss: 0.0080, time: 0.0029\n",
            "batch: 229/979, loss: 0.0080, time: 0.0028\n",
            "batch: 230/979, loss: 0.0080, time: 0.0028\n",
            "batch: 231/979, loss: 0.0080, time: 0.0029\n",
            "batch: 232/979, loss: 0.0080, time: 0.0029\n",
            "batch: 233/979, loss: 0.0079, time: 0.0028\n",
            "batch: 234/979, loss: 0.0079, time: 0.0030\n",
            "batch: 235/979, loss: 0.0079, time: 0.0031\n",
            "batch: 236/979, loss: 0.0083, time: 0.0029\n",
            "batch: 237/979, loss: 0.0083, time: 0.0028\n",
            "batch: 238/979, loss: 0.0083, time: 0.0029\n",
            "batch: 239/979, loss: 0.0082, time: 0.0028\n",
            "batch: 240/979, loss: 0.0082, time: 0.0028\n",
            "batch: 241/979, loss: 0.0082, time: 0.0029\n",
            "batch: 242/979, loss: 0.0082, time: 0.0047\n",
            "batch: 243/979, loss: 0.0084, time: 0.0029\n",
            "batch: 244/979, loss: 0.0084, time: 0.0029\n",
            "batch: 245/979, loss: 0.0084, time: 0.0028\n",
            "batch: 246/979, loss: 0.0084, time: 0.0028\n",
            "batch: 247/979, loss: 0.0084, time: 0.0029\n",
            "batch: 248/979, loss: 0.0083, time: 0.0029\n",
            "batch: 249/979, loss: 0.0083, time: 0.0029\n",
            "batch: 250/979, loss: 0.0083, time: 0.0033\n",
            "batch: 251/979, loss: 0.0083, time: 0.0029\n",
            "batch: 252/979, loss: 0.0082, time: 0.0028\n",
            "batch: 253/979, loss: 0.0082, time: 0.0029\n",
            "batch: 254/979, loss: 0.0082, time: 0.0029\n",
            "batch: 255/979, loss: 0.0082, time: 0.0048\n",
            "batch: 256/979, loss: 0.0082, time: 0.0030\n",
            "batch: 257/979, loss: 0.0081, time: 0.0032\n",
            "batch: 258/979, loss: 0.0081, time: 0.0030\n",
            "batch: 259/979, loss: 0.0081, time: 0.0030\n",
            "batch: 260/979, loss: 0.0081, time: 0.0029\n",
            "batch: 261/979, loss: 0.0081, time: 0.0028\n",
            "batch: 262/979, loss: 0.0081, time: 0.0029\n",
            "batch: 263/979, loss: 0.0081, time: 0.0029\n",
            "batch: 264/979, loss: 0.0081, time: 0.0042\n",
            "batch: 265/979, loss: 0.0081, time: 0.0029\n",
            "batch: 266/979, loss: 0.0081, time: 0.0030\n",
            "batch: 267/979, loss: 0.0080, time: 0.0029\n",
            "batch: 268/979, loss: 0.0080, time: 0.0028\n",
            "batch: 269/979, loss: 0.0080, time: 0.0037\n",
            "batch: 270/979, loss: 0.0080, time: 0.0029\n",
            "batch: 271/979, loss: 0.0080, time: 0.0029\n",
            "batch: 272/979, loss: 0.0079, time: 0.0030\n",
            "batch: 273/979, loss: 0.0079, time: 0.0029\n",
            "batch: 274/979, loss: 0.0079, time: 0.0029\n",
            "batch: 275/979, loss: 0.0079, time: 0.0029\n",
            "batch: 276/979, loss: 0.0079, time: 0.0029\n",
            "batch: 277/979, loss: 0.0078, time: 0.0029\n",
            "batch: 278/979, loss: 0.0078, time: 0.0029\n",
            "batch: 279/979, loss: 0.0078, time: 0.0029\n",
            "batch: 280/979, loss: 0.0078, time: 0.0029\n",
            "batch: 281/979, loss: 0.0078, time: 0.0031\n",
            "batch: 282/979, loss: 0.0078, time: 0.0029\n",
            "batch: 283/979, loss: 0.0078, time: 0.0029\n",
            "batch: 284/979, loss: 0.0077, time: 0.0029\n",
            "batch: 285/979, loss: 0.0078, time: 0.0029\n",
            "batch: 286/979, loss: 0.0078, time: 0.0030\n",
            "batch: 287/979, loss: 0.0079, time: 0.0038\n",
            "batch: 288/979, loss: 0.0079, time: 0.0040\n",
            "batch: 289/979, loss: 0.0079, time: 0.0038\n",
            "batch: 290/979, loss: 0.0078, time: 0.0031\n",
            "batch: 291/979, loss: 0.0078, time: 0.0029\n",
            "batch: 292/979, loss: 0.0078, time: 0.0030\n",
            "batch: 293/979, loss: 0.0078, time: 0.0029\n",
            "batch: 294/979, loss: 0.0077, time: 0.0028\n",
            "batch: 295/979, loss: 0.0077, time: 0.0030\n",
            "batch: 296/979, loss: 0.0077, time: 0.0029\n",
            "batch: 297/979, loss: 0.0077, time: 0.0029\n",
            "batch: 298/979, loss: 0.0077, time: 0.0028\n",
            "batch: 299/979, loss: 0.0077, time: 0.0029\n",
            "batch: 300/979, loss: 0.0077, time: 0.0028\n",
            "batch: 301/979, loss: 0.0077, time: 0.0029\n",
            "batch: 302/979, loss: 0.0077, time: 0.0029\n",
            "batch: 303/979, loss: 0.0076, time: 0.0029\n",
            "batch: 304/979, loss: 0.0077, time: 0.0028\n",
            "batch: 305/979, loss: 0.0077, time: 0.0029\n",
            "batch: 306/979, loss: 0.0077, time: 0.0028\n",
            "batch: 307/979, loss: 0.0077, time: 0.0062\n",
            "batch: 308/979, loss: 0.0077, time: 0.0045\n",
            "batch: 309/979, loss: 0.0079, time: 0.0029\n",
            "batch: 310/979, loss: 0.0079, time: 0.0030\n",
            "batch: 311/979, loss: 0.0079, time: 0.0029\n",
            "batch: 312/979, loss: 0.0079, time: 0.0028\n",
            "batch: 313/979, loss: 0.0078, time: 0.0029\n",
            "batch: 314/979, loss: 0.0078, time: 0.0028\n",
            "batch: 315/979, loss: 0.0078, time: 0.0028\n",
            "batch: 316/979, loss: 0.0078, time: 0.0030\n",
            "batch: 317/979, loss: 0.0078, time: 0.0030\n",
            "batch: 318/979, loss: 0.0078, time: 0.0031\n",
            "batch: 319/979, loss: 0.0078, time: 0.0068\n",
            "batch: 320/979, loss: 0.0078, time: 0.0029\n",
            "batch: 321/979, loss: 0.0077, time: 0.0028\n",
            "batch: 322/979, loss: 0.0077, time: 0.0029\n",
            "batch: 323/979, loss: 0.0077, time: 0.0031\n",
            "batch: 324/979, loss: 0.0080, time: 0.0030\n",
            "batch: 325/979, loss: 0.0080, time: 0.0031\n",
            "batch: 326/979, loss: 0.0082, time: 0.0031\n",
            "batch: 327/979, loss: 0.0082, time: 0.0030\n",
            "batch: 328/979, loss: 0.0082, time: 0.0031\n",
            "batch: 329/979, loss: 0.0081, time: 0.0031\n",
            "batch: 330/979, loss: 0.0081, time: 0.0030\n",
            "batch: 331/979, loss: 0.0081, time: 0.0029\n",
            "batch: 332/979, loss: 0.0081, time: 0.0029\n",
            "batch: 333/979, loss: 0.0081, time: 0.0029\n",
            "batch: 334/979, loss: 0.0081, time: 0.0029\n",
            "batch: 335/979, loss: 0.0081, time: 0.0028\n",
            "batch: 336/979, loss: 0.0081, time: 0.0032\n",
            "batch: 337/979, loss: 0.0082, time: 0.0030\n",
            "batch: 338/979, loss: 0.0082, time: 0.0031\n",
            "batch: 339/979, loss: 0.0082, time: 0.0028\n",
            "batch: 340/979, loss: 0.0081, time: 0.0028\n",
            "batch: 341/979, loss: 0.0081, time: 0.0028\n",
            "batch: 342/979, loss: 0.0081, time: 0.0029\n",
            "batch: 343/979, loss: 0.0081, time: 0.0029\n",
            "batch: 344/979, loss: 0.0081, time: 0.0028\n",
            "batch: 345/979, loss: 0.0083, time: 0.0028\n",
            "batch: 346/979, loss: 0.0083, time: 0.0028\n",
            "batch: 347/979, loss: 0.0083, time: 0.0029\n",
            "batch: 348/979, loss: 0.0083, time: 0.0028\n",
            "batch: 349/979, loss: 0.0082, time: 0.0028\n",
            "batch: 350/979, loss: 0.0082, time: 0.0038\n",
            "batch: 351/979, loss: 0.0082, time: 0.0038\n",
            "batch: 352/979, loss: 0.0082, time: 0.0029\n",
            "batch: 353/979, loss: 0.0082, time: 0.0035\n",
            "batch: 354/979, loss: 0.0087, time: 0.0030\n",
            "batch: 355/979, loss: 0.0087, time: 0.0029\n",
            "batch: 356/979, loss: 0.0087, time: 0.0052\n",
            "batch: 357/979, loss: 0.0086, time: 0.0038\n",
            "batch: 358/979, loss: 0.0086, time: 0.0049\n",
            "batch: 359/979, loss: 0.0086, time: 0.0041\n",
            "batch: 360/979, loss: 0.0086, time: 0.0045\n",
            "batch: 361/979, loss: 0.0086, time: 0.0029\n",
            "batch: 362/979, loss: 0.0086, time: 0.0028\n",
            "batch: 363/979, loss: 0.0086, time: 0.0029\n",
            "batch: 364/979, loss: 0.0086, time: 0.0029\n",
            "batch: 365/979, loss: 0.0086, time: 0.0034\n",
            "batch: 366/979, loss: 0.0085, time: 0.0035\n",
            "batch: 367/979, loss: 0.0085, time: 0.0038\n",
            "batch: 368/979, loss: 0.0085, time: 0.0029\n",
            "batch: 369/979, loss: 0.0088, time: 0.0031\n",
            "batch: 370/979, loss: 0.0088, time: 0.0029\n",
            "batch: 371/979, loss: 0.0087, time: 0.0028\n",
            "batch: 372/979, loss: 0.0087, time: 0.0044\n",
            "batch: 373/979, loss: 0.0087, time: 0.0029\n",
            "batch: 374/979, loss: 0.0087, time: 0.0028\n",
            "batch: 375/979, loss: 0.0087, time: 0.0028\n",
            "batch: 376/979, loss: 0.0086, time: 0.0029\n",
            "batch: 377/979, loss: 0.0086, time: 0.0028\n",
            "batch: 378/979, loss: 0.0086, time: 0.0029\n",
            "batch: 379/979, loss: 0.0086, time: 0.0028\n",
            "batch: 380/979, loss: 0.0086, time: 0.0028\n",
            "batch: 381/979, loss: 0.0086, time: 0.0063\n",
            "batch: 382/979, loss: 0.0086, time: 0.0031\n",
            "batch: 383/979, loss: 0.0086, time: 0.0030\n",
            "batch: 384/979, loss: 0.0086, time: 0.0029\n",
            "batch: 385/979, loss: 0.0085, time: 0.0029\n",
            "batch: 386/979, loss: 0.0085, time: 0.0029\n",
            "batch: 387/979, loss: 0.0086, time: 0.0031\n",
            "batch: 388/979, loss: 0.0085, time: 0.0029\n",
            "batch: 389/979, loss: 0.0085, time: 0.0029\n",
            "batch: 390/979, loss: 0.0085, time: 0.0029\n",
            "batch: 391/979, loss: 0.0085, time: 0.0029\n",
            "batch: 392/979, loss: 0.0085, time: 0.0029\n",
            "batch: 393/979, loss: 0.0085, time: 0.0029\n",
            "batch: 394/979, loss: 0.0084, time: 0.0028\n",
            "batch: 395/979, loss: 0.0084, time: 0.0028\n",
            "batch: 396/979, loss: 0.0084, time: 0.0030\n",
            "batch: 397/979, loss: 0.0084, time: 0.0028\n",
            "batch: 398/979, loss: 0.0084, time: 0.0028\n",
            "batch: 399/979, loss: 0.0084, time: 0.0029\n",
            "batch: 400/979, loss: 0.0084, time: 0.0031\n",
            "batch: 401/979, loss: 0.0084, time: 0.0029\n",
            "batch: 402/979, loss: 0.0084, time: 0.0028\n",
            "batch: 403/979, loss: 0.0084, time: 0.0028\n",
            "batch: 404/979, loss: 0.0084, time: 0.0029\n",
            "batch: 405/979, loss: 0.0085, time: 0.0029\n",
            "batch: 406/979, loss: 0.0086, time: 0.0030\n",
            "batch: 407/979, loss: 0.0086, time: 0.0029\n",
            "batch: 408/979, loss: 0.0086, time: 0.0030\n",
            "batch: 409/979, loss: 0.0086, time: 0.0028\n",
            "batch: 410/979, loss: 0.0086, time: 0.0028\n",
            "batch: 411/979, loss: 0.0086, time: 0.0028\n",
            "batch: 412/979, loss: 0.0086, time: 0.0065\n",
            "batch: 413/979, loss: 0.0086, time: 0.0038\n",
            "batch: 414/979, loss: 0.0085, time: 0.0031\n",
            "batch: 415/979, loss: 0.0086, time: 0.0028\n",
            "batch: 416/979, loss: 0.0085, time: 0.0029\n",
            "batch: 417/979, loss: 0.0085, time: 0.0040\n",
            "batch: 418/979, loss: 0.0085, time: 0.0029\n",
            "batch: 419/979, loss: 0.0085, time: 0.0029\n",
            "batch: 420/979, loss: 0.0085, time: 0.0028\n",
            "batch: 421/979, loss: 0.0085, time: 0.0028\n",
            "batch: 422/979, loss: 0.0085, time: 0.0037\n",
            "batch: 423/979, loss: 0.0085, time: 0.0032\n",
            "batch: 424/979, loss: 0.0085, time: 0.0029\n",
            "batch: 425/979, loss: 0.0085, time: 0.0028\n",
            "batch: 426/979, loss: 0.0085, time: 0.0028\n",
            "batch: 427/979, loss: 0.0084, time: 0.0030\n",
            "batch: 428/979, loss: 0.0084, time: 0.0029\n",
            "batch: 429/979, loss: 0.0085, time: 0.0029\n",
            "batch: 430/979, loss: 0.0086, time: 0.0028\n",
            "batch: 431/979, loss: 0.0085, time: 0.0028\n",
            "batch: 432/979, loss: 0.0085, time: 0.0028\n",
            "batch: 433/979, loss: 0.0085, time: 0.0029\n",
            "batch: 434/979, loss: 0.0085, time: 0.0030\n",
            "batch: 435/979, loss: 0.0085, time: 0.0043\n",
            "batch: 436/979, loss: 0.0085, time: 0.0029\n",
            "batch: 437/979, loss: 0.0084, time: 0.0029\n",
            "batch: 438/979, loss: 0.0087, time: 0.0028\n",
            "batch: 439/979, loss: 0.0086, time: 0.0028\n",
            "batch: 440/979, loss: 0.0086, time: 0.0029\n",
            "batch: 441/979, loss: 0.0086, time: 0.0029\n",
            "batch: 442/979, loss: 0.0086, time: 0.0028\n",
            "batch: 443/979, loss: 0.0086, time: 0.0028\n",
            "batch: 444/979, loss: 0.0086, time: 0.0042\n",
            "batch: 445/979, loss: 0.0086, time: 0.0042\n",
            "batch: 446/979, loss: 0.0086, time: 0.0029\n",
            "batch: 447/979, loss: 0.0086, time: 0.0030\n",
            "batch: 448/979, loss: 0.0086, time: 0.0041\n",
            "batch: 449/979, loss: 0.0085, time: 0.0029\n",
            "batch: 450/979, loss: 0.0085, time: 0.0029\n",
            "batch: 451/979, loss: 0.0086, time: 0.0029\n",
            "batch: 452/979, loss: 0.0086, time: 0.0030\n",
            "batch: 453/979, loss: 0.0086, time: 0.0028\n",
            "batch: 454/979, loss: 0.0086, time: 0.0028\n",
            "batch: 455/979, loss: 0.0086, time: 0.0029\n",
            "batch: 456/979, loss: 0.0085, time: 0.0029\n",
            "batch: 457/979, loss: 0.0085, time: 0.0029\n",
            "batch: 458/979, loss: 0.0085, time: 0.0028\n",
            "batch: 459/979, loss: 0.0085, time: 0.0028\n",
            "batch: 460/979, loss: 0.0085, time: 0.0028\n",
            "batch: 461/979, loss: 0.0085, time: 0.0028\n",
            "batch: 462/979, loss: 0.0085, time: 0.0028\n",
            "batch: 463/979, loss: 0.0085, time: 0.0029\n",
            "batch: 464/979, loss: 0.0085, time: 0.0028\n",
            "batch: 465/979, loss: 0.0085, time: 0.0028\n",
            "batch: 466/979, loss: 0.0085, time: 0.0029\n",
            "batch: 467/979, loss: 0.0085, time: 0.0030\n",
            "batch: 468/979, loss: 0.0085, time: 0.0028\n",
            "batch: 469/979, loss: 0.0084, time: 0.0028\n",
            "batch: 470/979, loss: 0.0084, time: 0.0029\n",
            "batch: 471/979, loss: 0.0084, time: 0.0028\n",
            "batch: 472/979, loss: 0.0084, time: 0.0028\n",
            "batch: 473/979, loss: 0.0084, time: 0.0028\n",
            "batch: 474/979, loss: 0.0084, time: 0.0028\n",
            "batch: 475/979, loss: 0.0085, time: 0.0029\n",
            "batch: 476/979, loss: 0.0084, time: 0.0030\n",
            "batch: 477/979, loss: 0.0084, time: 0.0057\n",
            "batch: 478/979, loss: 0.0085, time: 0.0029\n",
            "batch: 479/979, loss: 0.0085, time: 0.0029\n",
            "batch: 480/979, loss: 0.0085, time: 0.0028\n",
            "batch: 481/979, loss: 0.0085, time: 0.0028\n",
            "batch: 482/979, loss: 0.0085, time: 0.0028\n",
            "batch: 483/979, loss: 0.0085, time: 0.0029\n",
            "batch: 484/979, loss: 0.0084, time: 0.0029\n",
            "batch: 485/979, loss: 0.0084, time: 0.0028\n",
            "batch: 486/979, loss: 0.0084, time: 0.0030\n",
            "batch: 487/979, loss: 0.0084, time: 0.0029\n",
            "batch: 488/979, loss: 0.0088, time: 0.0029\n",
            "batch: 489/979, loss: 0.0088, time: 0.0028\n",
            "batch: 490/979, loss: 0.0088, time: 0.0028\n",
            "batch: 491/979, loss: 0.0088, time: 0.0029\n",
            "batch: 492/979, loss: 0.0088, time: 0.0028\n",
            "batch: 493/979, loss: 0.0087, time: 0.0028\n",
            "batch: 494/979, loss: 0.0087, time: 0.0028\n",
            "batch: 495/979, loss: 0.0087, time: 0.0028\n",
            "batch: 496/979, loss: 0.0087, time: 0.0030\n",
            "batch: 497/979, loss: 0.0087, time: 0.0028\n",
            "batch: 498/979, loss: 0.0087, time: 0.0028\n",
            "batch: 499/979, loss: 0.0087, time: 0.0028\n",
            "batch: 500/979, loss: 0.0087, time: 0.0028\n",
            "batch: 501/979, loss: 0.0087, time: 0.0029\n",
            "batch: 502/979, loss: 0.0087, time: 0.0044\n",
            "batch: 503/979, loss: 0.0087, time: 0.0030\n",
            "batch: 504/979, loss: 0.0087, time: 0.0029\n",
            "batch: 505/979, loss: 0.0087, time: 0.0028\n",
            "batch: 506/979, loss: 0.0087, time: 0.0028\n",
            "batch: 507/979, loss: 0.0088, time: 0.0034\n",
            "batch: 508/979, loss: 0.0088, time: 0.0028\n",
            "batch: 509/979, loss: 0.0088, time: 0.0040\n",
            "batch: 510/979, loss: 0.0088, time: 0.0029\n",
            "batch: 511/979, loss: 0.0087, time: 0.0030\n",
            "batch: 512/979, loss: 0.0087, time: 0.0029\n",
            "batch: 513/979, loss: 0.0087, time: 0.0030\n",
            "batch: 514/979, loss: 0.0087, time: 0.0029\n",
            "batch: 515/979, loss: 0.0087, time: 0.0029\n",
            "batch: 516/979, loss: 0.0088, time: 0.0047\n",
            "batch: 517/979, loss: 0.0087, time: 0.0043\n",
            "batch: 518/979, loss: 0.0087, time: 0.0046\n",
            "batch: 519/979, loss: 0.0087, time: 0.0032\n",
            "batch: 520/979, loss: 0.0087, time: 0.0040\n",
            "batch: 521/979, loss: 0.0087, time: 0.0038\n",
            "batch: 522/979, loss: 0.0087, time: 0.0031\n",
            "batch: 523/979, loss: 0.0087, time: 0.0028\n",
            "batch: 524/979, loss: 0.0087, time: 0.0028\n",
            "batch: 525/979, loss: 0.0087, time: 0.0042\n",
            "batch: 526/979, loss: 0.0086, time: 0.0049\n",
            "batch: 527/979, loss: 0.0087, time: 0.0040\n",
            "batch: 528/979, loss: 0.0087, time: 0.0030\n",
            "batch: 529/979, loss: 0.0087, time: 0.0039\n",
            "batch: 530/979, loss: 0.0087, time: 0.0046\n",
            "batch: 531/979, loss: 0.0087, time: 0.0044\n",
            "batch: 532/979, loss: 0.0087, time: 0.0038\n",
            "batch: 533/979, loss: 0.0087, time: 0.0042\n",
            "batch: 534/979, loss: 0.0086, time: 0.0029\n",
            "batch: 535/979, loss: 0.0087, time: 0.0029\n",
            "batch: 536/979, loss: 0.0087, time: 0.0028\n",
            "batch: 537/979, loss: 0.0086, time: 0.0055\n",
            "batch: 538/979, loss: 0.0086, time: 0.0029\n",
            "batch: 539/979, loss: 0.0086, time: 0.0029\n",
            "batch: 540/979, loss: 0.0086, time: 0.0030\n",
            "batch: 541/979, loss: 0.0087, time: 0.0030\n",
            "batch: 542/979, loss: 0.0087, time: 0.0029\n",
            "batch: 543/979, loss: 0.0088, time: 0.0029\n",
            "batch: 544/979, loss: 0.0087, time: 0.0029\n",
            "batch: 545/979, loss: 0.0087, time: 0.0029\n",
            "batch: 546/979, loss: 0.0087, time: 0.0029\n",
            "batch: 547/979, loss: 0.0087, time: 0.0028\n",
            "batch: 548/979, loss: 0.0087, time: 0.0029\n",
            "batch: 549/979, loss: 0.0087, time: 0.0029\n",
            "batch: 550/979, loss: 0.0087, time: 0.0029\n",
            "batch: 551/979, loss: 0.0087, time: 0.0029\n",
            "batch: 552/979, loss: 0.0087, time: 0.0029\n",
            "batch: 553/979, loss: 0.0087, time: 0.0029\n",
            "batch: 554/979, loss: 0.0086, time: 0.0029\n",
            "batch: 555/979, loss: 0.0086, time: 0.0029\n",
            "batch: 556/979, loss: 0.0086, time: 0.0029\n",
            "batch: 557/979, loss: 0.0086, time: 0.0029\n",
            "batch: 558/979, loss: 0.0086, time: 0.0029\n",
            "batch: 559/979, loss: 0.0086, time: 0.0028\n",
            "batch: 560/979, loss: 0.0086, time: 0.0028\n",
            "batch: 561/979, loss: 0.0086, time: 0.0028\n",
            "batch: 562/979, loss: 0.0086, time: 0.0028\n",
            "batch: 563/979, loss: 0.0086, time: 0.0029\n",
            "batch: 564/979, loss: 0.0086, time: 0.0028\n",
            "batch: 565/979, loss: 0.0086, time: 0.0039\n",
            "batch: 566/979, loss: 0.0086, time: 0.0029\n",
            "batch: 567/979, loss: 0.0086, time: 0.0029\n",
            "batch: 568/979, loss: 0.0086, time: 0.0028\n",
            "batch: 569/979, loss: 0.0086, time: 0.0028\n",
            "batch: 570/979, loss: 0.0086, time: 0.0056\n",
            "batch: 571/979, loss: 0.0086, time: 0.0029\n",
            "batch: 572/979, loss: 0.0086, time: 0.0029\n",
            "batch: 573/979, loss: 0.0086, time: 0.0030\n",
            "batch: 574/979, loss: 0.0086, time: 0.0028\n",
            "batch: 575/979, loss: 0.0086, time: 0.0037\n",
            "batch: 576/979, loss: 0.0085, time: 0.0029\n",
            "batch: 577/979, loss: 0.0086, time: 0.0029\n",
            "batch: 578/979, loss: 0.0086, time: 0.0029\n",
            "batch: 579/979, loss: 0.0086, time: 0.0028\n",
            "batch: 580/979, loss: 0.0086, time: 0.0028\n",
            "batch: 581/979, loss: 0.0086, time: 0.0028\n",
            "batch: 582/979, loss: 0.0086, time: 0.0029\n",
            "batch: 583/979, loss: 0.0086, time: 0.0028\n",
            "batch: 584/979, loss: 0.0086, time: 0.0028\n",
            "batch: 585/979, loss: 0.0086, time: 0.0028\n",
            "batch: 586/979, loss: 0.0086, time: 0.0028\n",
            "batch: 587/979, loss: 0.0086, time: 0.0028\n",
            "batch: 588/979, loss: 0.0085, time: 0.0028\n",
            "batch: 589/979, loss: 0.0085, time: 0.0028\n",
            "batch: 590/979, loss: 0.0085, time: 0.0029\n",
            "batch: 591/979, loss: 0.0085, time: 0.0030\n",
            "batch: 592/979, loss: 0.0085, time: 0.0029\n",
            "batch: 593/979, loss: 0.0085, time: 0.0030\n",
            "batch: 594/979, loss: 0.0085, time: 0.0029\n",
            "batch: 595/979, loss: 0.0085, time: 0.0028\n",
            "batch: 596/979, loss: 0.0085, time: 0.0028\n",
            "batch: 597/979, loss: 0.0085, time: 0.0029\n",
            "batch: 598/979, loss: 0.0085, time: 0.0029\n",
            "batch: 599/979, loss: 0.0084, time: 0.0029\n",
            "batch: 600/979, loss: 0.0084, time: 0.0029\n",
            "batch: 601/979, loss: 0.0084, time: 0.0029\n",
            "batch: 602/979, loss: 0.0084, time: 0.0051\n",
            "batch: 603/979, loss: 0.0084, time: 0.0029\n",
            "batch: 604/979, loss: 0.0084, time: 0.0028\n",
            "batch: 605/979, loss: 0.0084, time: 0.0030\n",
            "batch: 606/979, loss: 0.0084, time: 0.0028\n",
            "batch: 607/979, loss: 0.0084, time: 0.0028\n",
            "batch: 608/979, loss: 0.0084, time: 0.0028\n",
            "batch: 609/979, loss: 0.0083, time: 0.0028\n",
            "batch: 610/979, loss: 0.0083, time: 0.0029\n",
            "batch: 611/979, loss: 0.0083, time: 0.0028\n",
            "batch: 612/979, loss: 0.0083, time: 0.0028\n",
            "batch: 613/979, loss: 0.0083, time: 0.0030\n",
            "batch: 614/979, loss: 0.0083, time: 0.0029\n",
            "batch: 615/979, loss: 0.0083, time: 0.0028\n",
            "batch: 616/979, loss: 0.0083, time: 0.0028\n",
            "batch: 617/979, loss: 0.0083, time: 0.0029\n",
            "batch: 618/979, loss: 0.0083, time: 0.0028\n",
            "batch: 619/979, loss: 0.0083, time: 0.0029\n",
            "batch: 620/979, loss: 0.0083, time: 0.0028\n",
            "batch: 621/979, loss: 0.0083, time: 0.0028\n",
            "batch: 622/979, loss: 0.0083, time: 0.0028\n",
            "batch: 623/979, loss: 0.0083, time: 0.0028\n",
            "batch: 624/979, loss: 0.0083, time: 0.0029\n",
            "batch: 625/979, loss: 0.0083, time: 0.0029\n",
            "batch: 626/979, loss: 0.0083, time: 0.0029\n",
            "batch: 627/979, loss: 0.0083, time: 0.0045\n",
            "batch: 628/979, loss: 0.0083, time: 0.0028\n",
            "batch: 629/979, loss: 0.0082, time: 0.0029\n",
            "batch: 630/979, loss: 0.0082, time: 0.0029\n",
            "batch: 631/979, loss: 0.0082, time: 0.0028\n",
            "batch: 632/979, loss: 0.0082, time: 0.0030\n",
            "batch: 633/979, loss: 0.0082, time: 0.0038\n",
            "batch: 634/979, loss: 0.0082, time: 0.0054\n",
            "batch: 635/979, loss: 0.0082, time: 0.0035\n",
            "batch: 636/979, loss: 0.0082, time: 0.0028\n",
            "batch: 637/979, loss: 0.0082, time: 0.0029\n",
            "batch: 638/979, loss: 0.0082, time: 0.0028\n",
            "batch: 639/979, loss: 0.0082, time: 0.0028\n",
            "batch: 640/979, loss: 0.0081, time: 0.0028\n",
            "batch: 641/979, loss: 0.0081, time: 0.0028\n",
            "batch: 642/979, loss: 0.0081, time: 0.0028\n",
            "batch: 643/979, loss: 0.0081, time: 0.0028\n",
            "batch: 644/979, loss: 0.0081, time: 0.0029\n",
            "batch: 645/979, loss: 0.0081, time: 0.0028\n",
            "batch: 646/979, loss: 0.0081, time: 0.0028\n",
            "batch: 647/979, loss: 0.0081, time: 0.0028\n",
            "batch: 648/979, loss: 0.0081, time: 0.0028\n",
            "batch: 649/979, loss: 0.0081, time: 0.0028\n",
            "batch: 650/979, loss: 0.0080, time: 0.0028\n",
            "batch: 651/979, loss: 0.0081, time: 0.0028\n",
            "batch: 652/979, loss: 0.0081, time: 0.0028\n",
            "batch: 653/979, loss: 0.0081, time: 0.0028\n",
            "batch: 654/979, loss: 0.0081, time: 0.0029\n",
            "batch: 655/979, loss: 0.0081, time: 0.0029\n",
            "batch: 656/979, loss: 0.0081, time: 0.0028\n",
            "batch: 657/979, loss: 0.0081, time: 0.0028\n",
            "batch: 658/979, loss: 0.0081, time: 0.0028\n",
            "batch: 659/979, loss: 0.0081, time: 0.0028\n",
            "batch: 660/979, loss: 0.0081, time: 0.0034\n",
            "batch: 661/979, loss: 0.0081, time: 0.0029\n",
            "batch: 662/979, loss: 0.0081, time: 0.0030\n",
            "batch: 663/979, loss: 0.0081, time: 0.0028\n",
            "batch: 664/979, loss: 0.0081, time: 0.0028\n",
            "batch: 665/979, loss: 0.0081, time: 0.0028\n",
            "batch: 666/979, loss: 0.0082, time: 0.0050\n",
            "batch: 667/979, loss: 0.0082, time: 0.0029\n",
            "batch: 668/979, loss: 0.0082, time: 0.0028\n",
            "batch: 669/979, loss: 0.0082, time: 0.0028\n",
            "batch: 670/979, loss: 0.0082, time: 0.0028\n",
            "batch: 671/979, loss: 0.0082, time: 0.0029\n",
            "batch: 672/979, loss: 0.0082, time: 0.0030\n",
            "batch: 673/979, loss: 0.0082, time: 0.0046\n",
            "batch: 674/979, loss: 0.0082, time: 0.0044\n",
            "batch: 675/979, loss: 0.0082, time: 0.0043\n",
            "batch: 676/979, loss: 0.0082, time: 0.0044\n",
            "batch: 677/979, loss: 0.0082, time: 0.0042\n",
            "batch: 678/979, loss: 0.0082, time: 0.0035\n",
            "batch: 679/979, loss: 0.0081, time: 0.0031\n",
            "batch: 680/979, loss: 0.0081, time: 0.0029\n",
            "batch: 681/979, loss: 0.0081, time: 0.0029\n",
            "batch: 682/979, loss: 0.0081, time: 0.0029\n",
            "batch: 683/979, loss: 0.0081, time: 0.0028\n",
            "batch: 684/979, loss: 0.0081, time: 0.0028\n",
            "batch: 685/979, loss: 0.0081, time: 0.0028\n",
            "batch: 686/979, loss: 0.0081, time: 0.0029\n",
            "batch: 687/979, loss: 0.0081, time: 0.0031\n",
            "batch: 688/979, loss: 0.0081, time: 0.0030\n",
            "batch: 689/979, loss: 0.0081, time: 0.0027\n",
            "batch: 690/979, loss: 0.0081, time: 0.0028\n",
            "batch: 691/979, loss: 0.0081, time: 0.0028\n",
            "batch: 692/979, loss: 0.0081, time: 0.0029\n",
            "batch: 693/979, loss: 0.0081, time: 0.0029\n",
            "batch: 694/979, loss: 0.0081, time: 0.0028\n",
            "batch: 695/979, loss: 0.0081, time: 0.0027\n",
            "batch: 696/979, loss: 0.0081, time: 0.0027\n",
            "batch: 697/979, loss: 0.0080, time: 0.0028\n",
            "batch: 698/979, loss: 0.0080, time: 0.0054\n",
            "batch: 699/979, loss: 0.0080, time: 0.0028\n",
            "batch: 700/979, loss: 0.0080, time: 0.0027\n",
            "batch: 701/979, loss: 0.0080, time: 0.0027\n",
            "batch: 702/979, loss: 0.0080, time: 0.0028\n",
            "batch: 703/979, loss: 0.0080, time: 0.0028\n",
            "batch: 704/979, loss: 0.0080, time: 0.0028\n",
            "batch: 705/979, loss: 0.0080, time: 0.0028\n",
            "batch: 706/979, loss: 0.0080, time: 0.0029\n",
            "batch: 707/979, loss: 0.0080, time: 0.0028\n",
            "batch: 708/979, loss: 0.0080, time: 0.0027\n",
            "batch: 709/979, loss: 0.0080, time: 0.0027\n",
            "batch: 710/979, loss: 0.0080, time: 0.0027\n",
            "batch: 711/979, loss: 0.0080, time: 0.0028\n",
            "batch: 712/979, loss: 0.0080, time: 0.0028\n",
            "batch: 713/979, loss: 0.0080, time: 0.0041\n",
            "batch: 714/979, loss: 0.0079, time: 0.0028\n",
            "batch: 715/979, loss: 0.0079, time: 0.0028\n",
            "batch: 716/979, loss: 0.0079, time: 0.0028\n",
            "batch: 717/979, loss: 0.0079, time: 0.0028\n",
            "batch: 718/979, loss: 0.0079, time: 0.0041\n",
            "batch: 719/979, loss: 0.0079, time: 0.0028\n",
            "batch: 720/979, loss: 0.0079, time: 0.0028\n",
            "batch: 721/979, loss: 0.0079, time: 0.0028\n",
            "batch: 722/979, loss: 0.0080, time: 0.0027\n",
            "batch: 723/979, loss: 0.0080, time: 0.0028\n",
            "batch: 724/979, loss: 0.0079, time: 0.0028\n",
            "batch: 725/979, loss: 0.0079, time: 0.0028\n",
            "batch: 726/979, loss: 0.0082, time: 0.0028\n",
            "batch: 727/979, loss: 0.0081, time: 0.0030\n",
            "batch: 728/979, loss: 0.0081, time: 0.0028\n",
            "batch: 729/979, loss: 0.0081, time: 0.0028\n",
            "batch: 730/979, loss: 0.0081, time: 0.0038\n",
            "batch: 731/979, loss: 0.0081, time: 0.0032\n",
            "batch: 732/979, loss: 0.0081, time: 0.0028\n",
            "batch: 733/979, loss: 0.0081, time: 0.0028\n",
            "batch: 734/979, loss: 0.0081, time: 0.0028\n",
            "batch: 735/979, loss: 0.0081, time: 0.0028\n",
            "batch: 736/979, loss: 0.0081, time: 0.0028\n",
            "batch: 737/979, loss: 0.0081, time: 0.0028\n",
            "batch: 738/979, loss: 0.0081, time: 0.0028\n",
            "batch: 739/979, loss: 0.0081, time: 0.0027\n",
            "batch: 740/979, loss: 0.0081, time: 0.0027\n",
            "batch: 741/979, loss: 0.0081, time: 0.0027\n",
            "batch: 742/979, loss: 0.0080, time: 0.0028\n",
            "batch: 743/979, loss: 0.0080, time: 0.0028\n",
            "batch: 744/979, loss: 0.0081, time: 0.0028\n",
            "batch: 745/979, loss: 0.0080, time: 0.0028\n",
            "batch: 746/979, loss: 0.0080, time: 0.0028\n",
            "batch: 747/979, loss: 0.0080, time: 0.0028\n",
            "batch: 748/979, loss: 0.0080, time: 0.0028\n",
            "batch: 749/979, loss: 0.0080, time: 0.0028\n",
            "batch: 750/979, loss: 0.0080, time: 0.0028\n",
            "batch: 751/979, loss: 0.0080, time: 0.0028\n",
            "batch: 752/979, loss: 0.0080, time: 0.0028\n",
            "batch: 753/979, loss: 0.0080, time: 0.0028\n",
            "batch: 754/979, loss: 0.0080, time: 0.0028\n",
            "batch: 755/979, loss: 0.0080, time: 0.0029\n",
            "batch: 756/979, loss: 0.0080, time: 0.0028\n",
            "batch: 757/979, loss: 0.0080, time: 0.0028\n",
            "batch: 758/979, loss: 0.0079, time: 0.0040\n",
            "batch: 759/979, loss: 0.0079, time: 0.0027\n",
            "batch: 760/979, loss: 0.0079, time: 0.0027\n",
            "batch: 761/979, loss: 0.0079, time: 0.0028\n",
            "batch: 762/979, loss: 0.0079, time: 0.0045\n",
            "batch: 763/979, loss: 0.0079, time: 0.0038\n",
            "batch: 764/979, loss: 0.0079, time: 0.0033\n",
            "batch: 765/979, loss: 0.0079, time: 0.0031\n",
            "batch: 766/979, loss: 0.0079, time: 0.0027\n",
            "batch: 767/979, loss: 0.0079, time: 0.0028\n",
            "batch: 768/979, loss: 0.0079, time: 0.0028\n",
            "batch: 769/979, loss: 0.0079, time: 0.0028\n",
            "batch: 770/979, loss: 0.0079, time: 0.0028\n",
            "batch: 771/979, loss: 0.0079, time: 0.0028\n",
            "batch: 772/979, loss: 0.0079, time: 0.0028\n",
            "batch: 773/979, loss: 0.0079, time: 0.0028\n",
            "batch: 774/979, loss: 0.0079, time: 0.0027\n",
            "batch: 775/979, loss: 0.0079, time: 0.0029\n",
            "batch: 776/979, loss: 0.0079, time: 0.0029\n",
            "batch: 777/979, loss: 0.0079, time: 0.0028\n",
            "batch: 778/979, loss: 0.0078, time: 0.0027\n",
            "batch: 779/979, loss: 0.0078, time: 0.0027\n",
            "batch: 780/979, loss: 0.0078, time: 0.0028\n",
            "batch: 781/979, loss: 0.0078, time: 0.0029\n",
            "batch: 782/979, loss: 0.0078, time: 0.0029\n",
            "batch: 783/979, loss: 0.0078, time: 0.0028\n",
            "batch: 784/979, loss: 0.0078, time: 0.0027\n",
            "batch: 785/979, loss: 0.0078, time: 0.0030\n",
            "batch: 786/979, loss: 0.0078, time: 0.0029\n",
            "batch: 787/979, loss: 0.0078, time: 0.0037\n",
            "batch: 788/979, loss: 0.0079, time: 0.0035\n",
            "batch: 789/979, loss: 0.0079, time: 0.0029\n",
            "batch: 790/979, loss: 0.0079, time: 0.0028\n",
            "batch: 791/979, loss: 0.0079, time: 0.0028\n",
            "batch: 792/979, loss: 0.0079, time: 0.0028\n",
            "batch: 793/979, loss: 0.0079, time: 0.0028\n",
            "batch: 794/979, loss: 0.0079, time: 0.0028\n",
            "batch: 795/979, loss: 0.0078, time: 0.0046\n",
            "batch: 796/979, loss: 0.0078, time: 0.0028\n",
            "batch: 797/979, loss: 0.0078, time: 0.0028\n",
            "batch: 798/979, loss: 0.0078, time: 0.0027\n",
            "batch: 799/979, loss: 0.0078, time: 0.0028\n",
            "batch: 800/979, loss: 0.0078, time: 0.0028\n",
            "batch: 801/979, loss: 0.0078, time: 0.0028\n",
            "batch: 802/979, loss: 0.0078, time: 0.0028\n",
            "batch: 803/979, loss: 0.0078, time: 0.0028\n",
            "batch: 804/979, loss: 0.0078, time: 0.0028\n",
            "batch: 805/979, loss: 0.0078, time: 0.0028\n",
            "batch: 806/979, loss: 0.0078, time: 0.0030\n",
            "batch: 807/979, loss: 0.0078, time: 0.0028\n",
            "batch: 808/979, loss: 0.0078, time: 0.0028\n",
            "batch: 809/979, loss: 0.0079, time: 0.0028\n",
            "batch: 810/979, loss: 0.0079, time: 0.0028\n",
            "batch: 811/979, loss: 0.0079, time: 0.0027\n",
            "batch: 812/979, loss: 0.0079, time: 0.0028\n",
            "batch: 813/979, loss: 0.0079, time: 0.0028\n",
            "batch: 814/979, loss: 0.0079, time: 0.0029\n",
            "batch: 815/979, loss: 0.0079, time: 0.0028\n",
            "batch: 816/979, loss: 0.0079, time: 0.0027\n",
            "batch: 817/979, loss: 0.0079, time: 0.0044\n",
            "batch: 818/979, loss: 0.0079, time: 0.0036\n",
            "batch: 819/979, loss: 0.0079, time: 0.0029\n",
            "batch: 820/979, loss: 0.0079, time: 0.0029\n",
            "batch: 821/979, loss: 0.0079, time: 0.0029\n",
            "batch: 822/979, loss: 0.0079, time: 0.0028\n",
            "batch: 823/979, loss: 0.0079, time: 0.0029\n",
            "batch: 824/979, loss: 0.0080, time: 0.0029\n",
            "batch: 825/979, loss: 0.0080, time: 0.0029\n",
            "batch: 826/979, loss: 0.0082, time: 0.0028\n",
            "batch: 827/979, loss: 0.0082, time: 0.0029\n",
            "batch: 828/979, loss: 0.0082, time: 0.0055\n",
            "batch: 829/979, loss: 0.0082, time: 0.0030\n",
            "batch: 830/979, loss: 0.0082, time: 0.0040\n",
            "batch: 831/979, loss: 0.0082, time: 0.0037\n",
            "batch: 832/979, loss: 0.0082, time: 0.0035\n",
            "batch: 833/979, loss: 0.0082, time: 0.0029\n",
            "batch: 834/979, loss: 0.0082, time: 0.0045\n",
            "batch: 835/979, loss: 0.0083, time: 0.0048\n",
            "batch: 836/979, loss: 0.0083, time: 0.0043\n",
            "batch: 837/979, loss: 0.0083, time: 0.0029\n",
            "batch: 838/979, loss: 0.0084, time: 0.0029\n",
            "batch: 839/979, loss: 0.0084, time: 0.0028\n",
            "batch: 840/979, loss: 0.0086, time: 0.0028\n",
            "batch: 841/979, loss: 0.0086, time: 0.0028\n",
            "batch: 842/979, loss: 0.0086, time: 0.0028\n",
            "batch: 843/979, loss: 0.0086, time: 0.0029\n",
            "batch: 844/979, loss: 0.0086, time: 0.0028\n",
            "batch: 845/979, loss: 0.0086, time: 0.0028\n",
            "batch: 846/979, loss: 0.0086, time: 0.0029\n",
            "batch: 847/979, loss: 0.0086, time: 0.0029\n",
            "batch: 848/979, loss: 0.0086, time: 0.0029\n",
            "batch: 849/979, loss: 0.0086, time: 0.0028\n",
            "batch: 850/979, loss: 0.0086, time: 0.0030\n",
            "batch: 851/979, loss: 0.0086, time: 0.0029\n",
            "batch: 852/979, loss: 0.0086, time: 0.0028\n",
            "batch: 853/979, loss: 0.0086, time: 0.0030\n",
            "batch: 854/979, loss: 0.0086, time: 0.0029\n",
            "batch: 855/979, loss: 0.0086, time: 0.0028\n",
            "batch: 856/979, loss: 0.0086, time: 0.0029\n",
            "batch: 857/979, loss: 0.0086, time: 0.0029\n",
            "batch: 858/979, loss: 0.0086, time: 0.0029\n",
            "batch: 859/979, loss: 0.0086, time: 0.0029\n",
            "batch: 860/979, loss: 0.0086, time: 0.0058\n",
            "batch: 861/979, loss: 0.0085, time: 0.0031\n",
            "batch: 862/979, loss: 0.0085, time: 0.0030\n",
            "batch: 863/979, loss: 0.0085, time: 0.0029\n",
            "batch: 864/979, loss: 0.0085, time: 0.0028\n",
            "batch: 865/979, loss: 0.0085, time: 0.0027\n",
            "batch: 866/979, loss: 0.0085, time: 0.0028\n",
            "batch: 867/979, loss: 0.0085, time: 0.0028\n",
            "batch: 868/979, loss: 0.0085, time: 0.0029\n",
            "batch: 869/979, loss: 0.0085, time: 0.0028\n",
            "batch: 870/979, loss: 0.0085, time: 0.0028\n",
            "batch: 871/979, loss: 0.0085, time: 0.0027\n",
            "batch: 872/979, loss: 0.0085, time: 0.0027\n",
            "batch: 873/979, loss: 0.0085, time: 0.0028\n",
            "batch: 874/979, loss: 0.0085, time: 0.0028\n",
            "batch: 875/979, loss: 0.0085, time: 0.0028\n",
            "batch: 876/979, loss: 0.0085, time: 0.0028\n",
            "batch: 877/979, loss: 0.0085, time: 0.0028\n",
            "batch: 878/979, loss: 0.0085, time: 0.0028\n",
            "batch: 879/979, loss: 0.0085, time: 0.0028\n",
            "batch: 880/979, loss: 0.0085, time: 0.0028\n",
            "batch: 881/979, loss: 0.0084, time: 0.0028\n",
            "batch: 882/979, loss: 0.0085, time: 0.0028\n",
            "batch: 883/979, loss: 0.0085, time: 0.0028\n",
            "batch: 884/979, loss: 0.0084, time: 0.0032\n",
            "batch: 885/979, loss: 0.0084, time: 0.0029\n",
            "batch: 886/979, loss: 0.0084, time: 0.0028\n",
            "batch: 887/979, loss: 0.0084, time: 0.0028\n",
            "batch: 888/979, loss: 0.0084, time: 0.0028\n",
            "batch: 889/979, loss: 0.0084, time: 0.0031\n",
            "batch: 890/979, loss: 0.0084, time: 0.0028\n",
            "batch: 891/979, loss: 0.0085, time: 0.0028\n",
            "batch: 892/979, loss: 0.0085, time: 0.0045\n",
            "batch: 893/979, loss: 0.0084, time: 0.0037\n",
            "batch: 894/979, loss: 0.0084, time: 0.0030\n",
            "batch: 895/979, loss: 0.0085, time: 0.0027\n",
            "batch: 896/979, loss: 0.0084, time: 0.0033\n",
            "batch: 897/979, loss: 0.0084, time: 0.0089\n",
            "batch: 898/979, loss: 0.0086, time: 0.0027\n",
            "batch: 899/979, loss: 0.0086, time: 0.0028\n",
            "batch: 900/979, loss: 0.0086, time: 0.0028\n",
            "batch: 901/979, loss: 0.0086, time: 0.0040\n",
            "batch: 902/979, loss: 0.0086, time: 0.0035\n",
            "batch: 903/979, loss: 0.0086, time: 0.0029\n",
            "batch: 904/979, loss: 0.0086, time: 0.0031\n",
            "batch: 905/979, loss: 0.0087, time: 0.0028\n",
            "batch: 906/979, loss: 0.0087, time: 0.0028\n",
            "batch: 907/979, loss: 0.0087, time: 0.0028\n",
            "batch: 908/979, loss: 0.0087, time: 0.0027\n",
            "batch: 909/979, loss: 0.0087, time: 0.0028\n",
            "batch: 910/979, loss: 0.0087, time: 0.0028\n",
            "batch: 911/979, loss: 0.0087, time: 0.0028\n",
            "batch: 912/979, loss: 0.0086, time: 0.0029\n",
            "batch: 913/979, loss: 0.0087, time: 0.0028\n",
            "batch: 914/979, loss: 0.0087, time: 0.0028\n",
            "batch: 915/979, loss: 0.0087, time: 0.0028\n",
            "batch: 916/979, loss: 0.0087, time: 0.0028\n",
            "batch: 917/979, loss: 0.0087, time: 0.0029\n",
            "batch: 918/979, loss: 0.0087, time: 0.0028\n",
            "batch: 919/979, loss: 0.0087, time: 0.0027\n",
            "batch: 920/979, loss: 0.0087, time: 0.0027\n",
            "batch: 921/979, loss: 0.0087, time: 0.0028\n",
            "batch: 922/979, loss: 0.0087, time: 0.0028\n",
            "batch: 923/979, loss: 0.0087, time: 0.0028\n",
            "batch: 924/979, loss: 0.0087, time: 0.0068\n",
            "batch: 925/979, loss: 0.0087, time: 0.0028\n",
            "batch: 926/979, loss: 0.0086, time: 0.0029\n",
            "batch: 927/979, loss: 0.0088, time: 0.0053\n",
            "batch: 928/979, loss: 0.0088, time: 0.0041\n",
            "batch: 929/979, loss: 0.0088, time: 0.0040\n",
            "batch: 930/979, loss: 0.0088, time: 0.0036\n",
            "batch: 931/979, loss: 0.0088, time: 0.0041\n",
            "batch: 932/979, loss: 0.0088, time: 0.0040\n",
            "batch: 933/979, loss: 0.0087, time: 0.0042\n",
            "batch: 934/979, loss: 0.0087, time: 0.0042\n",
            "batch: 935/979, loss: 0.0087, time: 0.0048\n",
            "batch: 936/979, loss: 0.0087, time: 0.0041\n",
            "batch: 937/979, loss: 0.0087, time: 0.0036\n",
            "batch: 938/979, loss: 0.0087, time: 0.0044\n",
            "batch: 939/979, loss: 0.0087, time: 0.0044\n",
            "batch: 940/979, loss: 0.0087, time: 0.0048\n",
            "batch: 941/979, loss: 0.0087, time: 0.0043\n",
            "batch: 942/979, loss: 0.0087, time: 0.0041\n",
            "batch: 943/979, loss: 0.0087, time: 0.0041\n",
            "batch: 944/979, loss: 0.0087, time: 0.0035\n",
            "batch: 945/979, loss: 0.0087, time: 0.0036\n",
            "batch: 946/979, loss: 0.0087, time: 0.0035\n",
            "batch: 947/979, loss: 0.0087, time: 0.0074\n",
            "batch: 948/979, loss: 0.0087, time: 0.0030\n",
            "batch: 949/979, loss: 0.0087, time: 0.0036\n",
            "batch: 950/979, loss: 0.0087, time: 0.0050\n",
            "batch: 951/979, loss: 0.0086, time: 0.0042\n",
            "batch: 952/979, loss: 0.0086, time: 0.0037\n",
            "batch: 953/979, loss: 0.0086, time: 0.0034\n",
            "batch: 954/979, loss: 0.0086, time: 0.0035\n",
            "batch: 955/979, loss: 0.0086, time: 0.0041\n",
            "batch: 956/979, loss: 0.0086, time: 0.0040\n",
            "batch: 957/979, loss: 0.0086, time: 0.0041\n",
            "batch: 958/979, loss: 0.0086, time: 0.0042\n",
            "batch: 959/979, loss: 0.0086, time: 0.0040\n",
            "batch: 960/979, loss: 0.0086, time: 0.0040\n",
            "batch: 961/979, loss: 0.0086, time: 0.0037\n",
            "batch: 962/979, loss: 0.0086, time: 0.0035\n",
            "batch: 963/979, loss: 0.0086, time: 0.0035\n",
            "batch: 964/979, loss: 0.0086, time: 0.0038\n",
            "batch: 965/979, loss: 0.0086, time: 0.0036\n",
            "batch: 966/979, loss: 0.0086, time: 0.0036\n",
            "batch: 967/979, loss: 0.0085, time: 0.0035\n",
            "batch: 968/979, loss: 0.0087, time: 0.0040\n",
            "batch: 969/979, loss: 0.0086, time: 0.0041\n",
            "batch: 970/979, loss: 0.0086, time: 0.0037\n",
            "batch: 971/979, loss: 0.0087, time: 0.0039\n",
            "batch: 972/979, loss: 0.0087, time: 0.0035\n",
            "batch: 973/979, loss: 0.0087, time: 0.0035\n",
            "batch: 974/979, loss: 0.0087, time: 0.0034\n",
            "batch: 975/979, loss: 0.0086, time: 0.0035\n",
            "batch: 976/979, loss: 0.0087, time: 0.0040\n",
            "batch: 977/979, loss: 0.0087, time: 0.0042\n",
            "batch: 978/979, loss: 0.0087, time: 0.0058\n",
            "batch: 979/979, loss: 0.0087, time: 0.0039\n",
            "batch: 1/979, loss: 0.0030, time: 0.0044\n",
            "batch: 2/979, loss: 0.0134, time: 0.0042\n",
            "batch: 3/979, loss: 0.0124, time: 0.0041\n",
            "batch: 4/979, loss: 0.0094, time: 0.0044\n",
            "batch: 5/979, loss: 0.0080, time: 0.0040\n",
            "batch: 6/979, loss: 0.0068, time: 0.0036\n",
            "batch: 7/979, loss: 0.0294, time: 0.0035\n",
            "batch: 8/979, loss: 0.0258, time: 0.0036\n",
            "batch: 9/979, loss: 0.0231, time: 0.0037\n",
            "batch: 10/979, loss: 0.0211, time: 0.0035\n",
            "batch: 11/979, loss: 0.0193, time: 0.0035\n",
            "batch: 12/979, loss: 0.0177, time: 0.0034\n",
            "batch: 13/979, loss: 0.0164, time: 0.0035\n",
            "batch: 14/979, loss: 0.0153, time: 0.0035\n",
            "batch: 15/979, loss: 0.0198, time: 0.0036\n",
            "batch: 16/979, loss: 0.0186, time: 0.0035\n",
            "batch: 17/979, loss: 0.0178, time: 0.0049\n",
            "batch: 18/979, loss: 0.0168, time: 0.0035\n",
            "batch: 19/979, loss: 0.0160, time: 0.0035\n",
            "batch: 20/979, loss: 0.0167, time: 0.0035\n",
            "batch: 21/979, loss: 0.0161, time: 0.0035\n",
            "batch: 22/979, loss: 0.0154, time: 0.0035\n",
            "batch: 23/979, loss: 0.0151, time: 0.0035\n",
            "batch: 24/979, loss: 0.0145, time: 0.0035\n",
            "batch: 25/979, loss: 0.0140, time: 0.0035\n",
            "batch: 26/979, loss: 0.0135, time: 0.0035\n",
            "batch: 27/979, loss: 0.0130, time: 0.0035\n",
            "batch: 28/979, loss: 0.0127, time: 0.0035\n",
            "batch: 29/979, loss: 0.0127, time: 0.0035\n",
            "batch: 30/979, loss: 0.0123, time: 0.0072\n",
            "batch: 31/979, loss: 0.0121, time: 0.0037\n",
            "batch: 32/979, loss: 0.0118, time: 0.0036\n",
            "batch: 33/979, loss: 0.0116, time: 0.0035\n",
            "batch: 34/979, loss: 0.0113, time: 0.0035\n",
            "batch: 35/979, loss: 0.0110, time: 0.0035\n",
            "batch: 36/979, loss: 0.0107, time: 0.0036\n",
            "batch: 37/979, loss: 0.0105, time: 0.0035\n",
            "batch: 38/979, loss: 0.0102, time: 0.0036\n",
            "batch: 39/979, loss: 0.0100, time: 0.0035\n",
            "batch: 40/979, loss: 0.0097, time: 0.0035\n",
            "batch: 41/979, loss: 0.0095, time: 0.0035\n",
            "batch: 42/979, loss: 0.0093, time: 0.0036\n",
            "batch: 43/979, loss: 0.0091, time: 0.0042\n",
            "batch: 44/979, loss: 0.0090, time: 0.0041\n",
            "batch: 45/979, loss: 0.0089, time: 0.0035\n",
            "batch: 46/979, loss: 0.0088, time: 0.0036\n",
            "batch: 47/979, loss: 0.0086, time: 0.0034\n",
            "batch: 48/979, loss: 0.0084, time: 0.0035\n",
            "batch: 49/979, loss: 0.0083, time: 0.0034\n",
            "batch: 50/979, loss: 0.0082, time: 0.0034\n",
            "batch: 51/979, loss: 0.0080, time: 0.0034\n",
            "batch: 52/979, loss: 0.0079, time: 0.0035\n",
            "batch: 53/979, loss: 0.0078, time: 0.0035\n",
            "batch: 54/979, loss: 0.0077, time: 0.0039\n",
            "batch: 55/979, loss: 0.0076, time: 0.0035\n",
            "batch: 56/979, loss: 0.0074, time: 0.0035\n",
            "batch: 57/979, loss: 0.0073, time: 0.0035\n",
            "batch: 58/979, loss: 0.0072, time: 0.0035\n",
            "batch: 59/979, loss: 0.0071, time: 0.0036\n",
            "batch: 60/979, loss: 0.0070, time: 0.0036\n",
            "batch: 61/979, loss: 0.0069, time: 0.0072\n",
            "batch: 62/979, loss: 0.0068, time: 0.0043\n",
            "batch: 63/979, loss: 0.0067, time: 0.0041\n",
            "batch: 64/979, loss: 0.0067, time: 0.0035\n",
            "batch: 65/979, loss: 0.0066, time: 0.0039\n",
            "batch: 66/979, loss: 0.0065, time: 0.0035\n",
            "batch: 67/979, loss: 0.0064, time: 0.0045\n",
            "batch: 68/979, loss: 0.0063, time: 0.0043\n",
            "batch: 69/979, loss: 0.0063, time: 0.0041\n",
            "batch: 70/979, loss: 0.0062, time: 0.0041\n",
            "batch: 71/979, loss: 0.0061, time: 0.0042\n",
            "batch: 72/979, loss: 0.0061, time: 0.0042\n",
            "batch: 73/979, loss: 0.0060, time: 0.0041\n",
            "batch: 74/979, loss: 0.0060, time: 0.0038\n",
            "batch: 75/979, loss: 0.0059, time: 0.0045\n",
            "batch: 76/979, loss: 0.0059, time: 0.0038\n",
            "batch: 77/979, loss: 0.0058, time: 0.0036\n",
            "batch: 78/979, loss: 0.0057, time: 0.0042\n",
            "batch: 79/979, loss: 0.0057, time: 0.0048\n",
            "batch: 80/979, loss: 0.0057, time: 0.0048\n",
            "batch: 81/979, loss: 0.0057, time: 0.0046\n",
            "batch: 82/979, loss: 0.0056, time: 0.0046\n",
            "batch: 83/979, loss: 0.0056, time: 0.0062\n",
            "batch: 84/979, loss: 0.0055, time: 0.0036\n",
            "batch: 85/979, loss: 0.0055, time: 0.0036\n",
            "batch: 86/979, loss: 0.0054, time: 0.0085\n",
            "batch: 87/979, loss: 0.0054, time: 0.0037\n",
            "batch: 88/979, loss: 0.0056, time: 0.0036\n",
            "batch: 89/979, loss: 0.0056, time: 0.0041\n",
            "batch: 90/979, loss: 0.0056, time: 0.0077\n",
            "batch: 91/979, loss: 0.0056, time: 0.0042\n",
            "batch: 92/979, loss: 0.0055, time: 0.0041\n",
            "batch: 93/979, loss: 0.0055, time: 0.0042\n",
            "batch: 94/979, loss: 0.0054, time: 0.0038\n",
            "batch: 95/979, loss: 0.0054, time: 0.0035\n",
            "batch: 96/979, loss: 0.0054, time: 0.0035\n",
            "batch: 97/979, loss: 0.0054, time: 0.0037\n",
            "batch: 98/979, loss: 0.0054, time: 0.0035\n",
            "batch: 99/979, loss: 0.0053, time: 0.0036\n",
            "batch: 100/979, loss: 0.0053, time: 0.0040\n",
            "batch: 101/979, loss: 0.0052, time: 0.0041\n",
            "batch: 102/979, loss: 0.0052, time: 0.0041\n",
            "batch: 103/979, loss: 0.0052, time: 0.0042\n",
            "batch: 104/979, loss: 0.0051, time: 0.0033\n",
            "batch: 105/979, loss: 0.0051, time: 0.0055\n",
            "batch: 106/979, loss: 0.0051, time: 0.0045\n",
            "batch: 107/979, loss: 0.0051, time: 0.0041\n",
            "batch: 108/979, loss: 0.0051, time: 0.0058\n",
            "batch: 109/979, loss: 0.0050, time: 0.0042\n",
            "batch: 110/979, loss: 0.0050, time: 0.0071\n",
            "batch: 111/979, loss: 0.0050, time: 0.0056\n",
            "batch: 112/979, loss: 0.0050, time: 0.0057\n",
            "batch: 113/979, loss: 0.0049, time: 0.0041\n",
            "batch: 114/979, loss: 0.0049, time: 0.0042\n",
            "batch: 115/979, loss: 0.0049, time: 0.0038\n",
            "batch: 116/979, loss: 0.0049, time: 0.0036\n",
            "batch: 117/979, loss: 0.0048, time: 0.0037\n",
            "batch: 118/979, loss: 0.0051, time: 0.0042\n",
            "batch: 119/979, loss: 0.0051, time: 0.0031\n",
            "batch: 120/979, loss: 0.0051, time: 0.0036\n",
            "batch: 121/979, loss: 0.0052, time: 0.0042\n",
            "batch: 122/979, loss: 0.0051, time: 0.0040\n",
            "batch: 123/979, loss: 0.0051, time: 0.0038\n",
            "batch: 124/979, loss: 0.0051, time: 0.0041\n",
            "batch: 125/979, loss: 0.0050, time: 0.0049\n",
            "batch: 126/979, loss: 0.0050, time: 0.0061\n",
            "batch: 127/979, loss: 0.0050, time: 0.0041\n",
            "batch: 128/979, loss: 0.0049, time: 0.0042\n",
            "batch: 129/979, loss: 0.0049, time: 0.0043\n",
            "batch: 130/979, loss: 0.0049, time: 0.0045\n",
            "batch: 131/979, loss: 0.0049, time: 0.0041\n",
            "batch: 132/979, loss: 0.0049, time: 0.0041\n",
            "batch: 133/979, loss: 0.0048, time: 0.0041\n",
            "batch: 134/979, loss: 0.0048, time: 0.0041\n",
            "batch: 135/979, loss: 0.0048, time: 0.0043\n",
            "batch: 136/979, loss: 0.0048, time: 0.0041\n",
            "batch: 137/979, loss: 0.0047, time: 0.0039\n",
            "batch: 138/979, loss: 0.0047, time: 0.0039\n",
            "batch: 139/979, loss: 0.0047, time: 0.0039\n",
            "batch: 140/979, loss: 0.0047, time: 0.0039\n",
            "batch: 141/979, loss: 0.0047, time: 0.0039\n",
            "batch: 142/979, loss: 0.0047, time: 0.0040\n",
            "batch: 143/979, loss: 0.0051, time: 0.0043\n",
            "batch: 144/979, loss: 0.0051, time: 0.0040\n",
            "batch: 145/979, loss: 0.0051, time: 0.0059\n",
            "batch: 146/979, loss: 0.0051, time: 0.0038\n",
            "batch: 147/979, loss: 0.0050, time: 0.0042\n",
            "batch: 148/979, loss: 0.0050, time: 0.0039\n",
            "batch: 149/979, loss: 0.0050, time: 0.0038\n",
            "batch: 150/979, loss: 0.0050, time: 0.0039\n",
            "batch: 151/979, loss: 0.0050, time: 0.0039\n",
            "batch: 152/979, loss: 0.0049, time: 0.0039\n",
            "batch: 153/979, loss: 0.0049, time: 0.0039\n",
            "batch: 154/979, loss: 0.0049, time: 0.0038\n",
            "batch: 155/979, loss: 0.0049, time: 0.0039\n",
            "batch: 156/979, loss: 0.0048, time: 0.0039\n",
            "batch: 157/979, loss: 0.0048, time: 0.0038\n",
            "batch: 158/979, loss: 0.0048, time: 0.0038\n",
            "batch: 159/979, loss: 0.0048, time: 0.0038\n",
            "batch: 160/979, loss: 0.0048, time: 0.0044\n",
            "batch: 161/979, loss: 0.0048, time: 0.0038\n",
            "batch: 162/979, loss: 0.0048, time: 0.0039\n",
            "batch: 163/979, loss: 0.0048, time: 0.0039\n",
            "batch: 164/979, loss: 0.0047, time: 0.0042\n",
            "batch: 165/979, loss: 0.0047, time: 0.0045\n",
            "batch: 166/979, loss: 0.0047, time: 0.0055\n",
            "batch: 167/979, loss: 0.0047, time: 0.0041\n",
            "batch: 168/979, loss: 0.0047, time: 0.0039\n",
            "batch: 169/979, loss: 0.0049, time: 0.0040\n",
            "batch: 170/979, loss: 0.0049, time: 0.0039\n",
            "batch: 171/979, loss: 0.0049, time: 0.0039\n",
            "batch: 172/979, loss: 0.0048, time: 0.0039\n",
            "batch: 173/979, loss: 0.0048, time: 0.0038\n",
            "batch: 174/979, loss: 0.0048, time: 0.0039\n",
            "batch: 175/979, loss: 0.0048, time: 0.0038\n",
            "batch: 176/979, loss: 0.0048, time: 0.0038\n",
            "batch: 177/979, loss: 0.0048, time: 0.0038\n",
            "batch: 178/979, loss: 0.0047, time: 0.0038\n",
            "batch: 179/979, loss: 0.0047, time: 0.0039\n",
            "batch: 180/979, loss: 0.0047, time: 0.0038\n",
            "batch: 181/979, loss: 0.0047, time: 0.0038\n",
            "batch: 182/979, loss: 0.0047, time: 0.0039\n",
            "batch: 183/979, loss: 0.0048, time: 0.0039\n",
            "batch: 184/979, loss: 0.0047, time: 0.0046\n",
            "batch: 185/979, loss: 0.0047, time: 0.0047\n",
            "batch: 186/979, loss: 0.0047, time: 0.0059\n",
            "batch: 187/979, loss: 0.0047, time: 0.0119\n",
            "batch: 188/979, loss: 0.0047, time: 0.0062\n",
            "batch: 189/979, loss: 0.0047, time: 0.0068\n",
            "batch: 190/979, loss: 0.0047, time: 0.0071\n",
            "batch: 191/979, loss: 0.0047, time: 0.0063\n",
            "batch: 192/979, loss: 0.0046, time: 0.0040\n",
            "batch: 193/979, loss: 0.0046, time: 0.0040\n",
            "batch: 194/979, loss: 0.0046, time: 0.0039\n",
            "batch: 195/979, loss: 0.0046, time: 0.0039\n",
            "batch: 196/979, loss: 0.0046, time: 0.0039\n",
            "batch: 197/979, loss: 0.0046, time: 0.0039\n",
            "batch: 198/979, loss: 0.0045, time: 0.0040\n",
            "batch: 199/979, loss: 0.0046, time: 0.0043\n",
            "batch: 200/979, loss: 0.0045, time: 0.0065\n",
            "batch: 201/979, loss: 0.0045, time: 0.0083\n",
            "batch: 202/979, loss: 0.0045, time: 0.0041\n",
            "batch: 203/979, loss: 0.0045, time: 0.0042\n",
            "batch: 204/979, loss: 0.0045, time: 0.0041\n",
            "batch: 205/979, loss: 0.0044, time: 0.0040\n",
            "batch: 206/979, loss: 0.0047, time: 0.0041\n",
            "batch: 207/979, loss: 0.0047, time: 0.0041\n",
            "batch: 208/979, loss: 0.0047, time: 0.0042\n",
            "batch: 209/979, loss: 0.0047, time: 0.0042\n",
            "batch: 210/979, loss: 0.0047, time: 0.0041\n",
            "batch: 211/979, loss: 0.0046, time: 0.0101\n",
            "batch: 212/979, loss: 0.0046, time: 0.0038\n",
            "batch: 213/979, loss: 0.0046, time: 0.0039\n",
            "batch: 214/979, loss: 0.0046, time: 0.0055\n",
            "batch: 215/979, loss: 0.0046, time: 0.0049\n",
            "batch: 216/979, loss: 0.0046, time: 0.0042\n",
            "batch: 217/979, loss: 0.0046, time: 0.0041\n",
            "batch: 218/979, loss: 0.0046, time: 0.0042\n",
            "batch: 219/979, loss: 0.0045, time: 0.0041\n",
            "batch: 220/979, loss: 0.0045, time: 0.0050\n",
            "batch: 221/979, loss: 0.0045, time: 0.0041\n",
            "batch: 222/979, loss: 0.0045, time: 0.0040\n",
            "batch: 223/979, loss: 0.0045, time: 0.0043\n",
            "batch: 224/979, loss: 0.0045, time: 0.0045\n",
            "batch: 225/979, loss: 0.0045, time: 0.0042\n",
            "batch: 226/979, loss: 0.0044, time: 0.0042\n",
            "batch: 227/979, loss: 0.0044, time: 0.0044\n",
            "batch: 228/979, loss: 0.0044, time: 0.0042\n",
            "batch: 229/979, loss: 0.0044, time: 0.0040\n",
            "batch: 230/979, loss: 0.0044, time: 0.0039\n",
            "batch: 231/979, loss: 0.0044, time: 0.0037\n",
            "batch: 232/979, loss: 0.0044, time: 0.0036\n",
            "batch: 233/979, loss: 0.0044, time: 0.0035\n",
            "batch: 234/979, loss: 0.0044, time: 0.0036\n",
            "batch: 235/979, loss: 0.0044, time: 0.0036\n",
            "batch: 236/979, loss: 0.0044, time: 0.0036\n",
            "batch: 237/979, loss: 0.0043, time: 0.0036\n",
            "batch: 238/979, loss: 0.0043, time: 0.0047\n",
            "batch: 239/979, loss: 0.0043, time: 0.0036\n",
            "batch: 240/979, loss: 0.0043, time: 0.0036\n",
            "batch: 241/979, loss: 0.0043, time: 0.0035\n",
            "batch: 242/979, loss: 0.0043, time: 0.0036\n",
            "batch: 243/979, loss: 0.0043, time: 0.0041\n",
            "batch: 244/979, loss: 0.0043, time: 0.0078\n",
            "batch: 245/979, loss: 0.0043, time: 0.0078\n",
            "batch: 246/979, loss: 0.0042, time: 0.0037\n",
            "batch: 247/979, loss: 0.0042, time: 0.0046\n",
            "batch: 248/979, loss: 0.0042, time: 0.0039\n",
            "batch: 249/979, loss: 0.0042, time: 0.0036\n",
            "batch: 250/979, loss: 0.0042, time: 0.0037\n",
            "batch: 251/979, loss: 0.0042, time: 0.0036\n",
            "batch: 252/979, loss: 0.0042, time: 0.0036\n",
            "batch: 253/979, loss: 0.0042, time: 0.0038\n",
            "batch: 254/979, loss: 0.0041, time: 0.0036\n",
            "batch: 255/979, loss: 0.0041, time: 0.0035\n",
            "batch: 256/979, loss: 0.0041, time: 0.0036\n",
            "batch: 257/979, loss: 0.0041, time: 0.0036\n",
            "batch: 258/979, loss: 0.0041, time: 0.0035\n",
            "batch: 259/979, loss: 0.0041, time: 0.0036\n",
            "batch: 260/979, loss: 0.0041, time: 0.0028\n",
            "batch: 261/979, loss: 0.0041, time: 0.0029\n",
            "batch: 262/979, loss: 0.0040, time: 0.0028\n",
            "batch: 263/979, loss: 0.0040, time: 0.0030\n",
            "batch: 264/979, loss: 0.0040, time: 0.0028\n",
            "batch: 265/979, loss: 0.0040, time: 0.0064\n",
            "batch: 266/979, loss: 0.0040, time: 0.0029\n",
            "batch: 267/979, loss: 0.0040, time: 0.0029\n",
            "batch: 268/979, loss: 0.0040, time: 0.0029\n",
            "batch: 269/979, loss: 0.0040, time: 0.0029\n",
            "batch: 270/979, loss: 0.0040, time: 0.0042\n",
            "batch: 271/979, loss: 0.0040, time: 0.0030\n",
            "batch: 272/979, loss: 0.0040, time: 0.0029\n",
            "batch: 273/979, loss: 0.0040, time: 0.0029\n",
            "batch: 274/979, loss: 0.0040, time: 0.0030\n",
            "batch: 275/979, loss: 0.0040, time: 0.0029\n",
            "batch: 276/979, loss: 0.0040, time: 0.0028\n",
            "batch: 277/979, loss: 0.0040, time: 0.0028\n",
            "batch: 278/979, loss: 0.0040, time: 0.0028\n",
            "batch: 279/979, loss: 0.0040, time: 0.0030\n",
            "batch: 280/979, loss: 0.0039, time: 0.0029\n",
            "batch: 281/979, loss: 0.0039, time: 0.0028\n",
            "batch: 282/979, loss: 0.0039, time: 0.0028\n",
            "batch: 283/979, loss: 0.0039, time: 0.0028\n",
            "batch: 284/979, loss: 0.0039, time: 0.0029\n",
            "batch: 285/979, loss: 0.0039, time: 0.0029\n",
            "batch: 286/979, loss: 0.0040, time: 0.0029\n",
            "batch: 287/979, loss: 0.0040, time: 0.0028\n",
            "batch: 288/979, loss: 0.0040, time: 0.0029\n",
            "batch: 289/979, loss: 0.0040, time: 0.0028\n",
            "batch: 290/979, loss: 0.0039, time: 0.0028\n",
            "batch: 291/979, loss: 0.0040, time: 0.0029\n",
            "batch: 292/979, loss: 0.0039, time: 0.0029\n",
            "batch: 293/979, loss: 0.0039, time: 0.0029\n",
            "batch: 294/979, loss: 0.0039, time: 0.0028\n",
            "batch: 295/979, loss: 0.0039, time: 0.0029\n",
            "batch: 296/979, loss: 0.0039, time: 0.0029\n",
            "batch: 297/979, loss: 0.0039, time: 0.0029\n",
            "batch: 298/979, loss: 0.0039, time: 0.0045\n",
            "batch: 299/979, loss: 0.0039, time: 0.0031\n",
            "batch: 300/979, loss: 0.0039, time: 0.0029\n",
            "batch: 301/979, loss: 0.0039, time: 0.0029\n",
            "batch: 302/979, loss: 0.0039, time: 0.0029\n",
            "batch: 303/979, loss: 0.0039, time: 0.0028\n",
            "batch: 304/979, loss: 0.0039, time: 0.0029\n",
            "batch: 305/979, loss: 0.0039, time: 0.0029\n",
            "batch: 306/979, loss: 0.0039, time: 0.0029\n",
            "batch: 307/979, loss: 0.0039, time: 0.0029\n",
            "batch: 308/979, loss: 0.0039, time: 0.0029\n",
            "batch: 309/979, loss: 0.0039, time: 0.0029\n",
            "batch: 310/979, loss: 0.0039, time: 0.0029\n",
            "batch: 311/979, loss: 0.0039, time: 0.0044\n",
            "batch: 312/979, loss: 0.0039, time: 0.0029\n",
            "batch: 313/979, loss: 0.0039, time: 0.0029\n",
            "batch: 314/979, loss: 0.0039, time: 0.0028\n",
            "batch: 315/979, loss: 0.0038, time: 0.0028\n",
            "batch: 316/979, loss: 0.0038, time: 0.0028\n",
            "batch: 317/979, loss: 0.0038, time: 0.0030\n",
            "batch: 318/979, loss: 0.0038, time: 0.0028\n",
            "batch: 319/979, loss: 0.0038, time: 0.0028\n",
            "batch: 320/979, loss: 0.0038, time: 0.0028\n",
            "batch: 321/979, loss: 0.0038, time: 0.0028\n",
            "batch: 322/979, loss: 0.0038, time: 0.0029\n",
            "batch: 323/979, loss: 0.0038, time: 0.0035\n",
            "batch: 324/979, loss: 0.0038, time: 0.0031\n",
            "batch: 325/979, loss: 0.0038, time: 0.0029\n",
            "batch: 326/979, loss: 0.0038, time: 0.0028\n",
            "batch: 327/979, loss: 0.0038, time: 0.0028\n",
            "batch: 328/979, loss: 0.0038, time: 0.0028\n",
            "batch: 329/979, loss: 0.0037, time: 0.0029\n",
            "batch: 330/979, loss: 0.0037, time: 0.0047\n",
            "batch: 331/979, loss: 0.0037, time: 0.0029\n",
            "batch: 332/979, loss: 0.0037, time: 0.0029\n",
            "batch: 333/979, loss: 0.0037, time: 0.0029\n",
            "batch: 334/979, loss: 0.0037, time: 0.0028\n",
            "batch: 335/979, loss: 0.0037, time: 0.0029\n",
            "batch: 336/979, loss: 0.0037, time: 0.0031\n",
            "batch: 337/979, loss: 0.0037, time: 0.0028\n",
            "batch: 338/979, loss: 0.0037, time: 0.0030\n",
            "batch: 339/979, loss: 0.0037, time: 0.0029\n",
            "batch: 340/979, loss: 0.0037, time: 0.0030\n",
            "batch: 341/979, loss: 0.0037, time: 0.0029\n",
            "batch: 342/979, loss: 0.0036, time: 0.0040\n",
            "batch: 343/979, loss: 0.0036, time: 0.0042\n",
            "batch: 344/979, loss: 0.0036, time: 0.0036\n",
            "batch: 345/979, loss: 0.0036, time: 0.0032\n",
            "batch: 346/979, loss: 0.0036, time: 0.0030\n",
            "batch: 347/979, loss: 0.0036, time: 0.0029\n",
            "batch: 348/979, loss: 0.0036, time: 0.0029\n",
            "batch: 349/979, loss: 0.0036, time: 0.0029\n",
            "batch: 350/979, loss: 0.0036, time: 0.0028\n",
            "batch: 351/979, loss: 0.0036, time: 0.0029\n",
            "batch: 352/979, loss: 0.0036, time: 0.0029\n",
            "batch: 353/979, loss: 0.0036, time: 0.0029\n",
            "batch: 354/979, loss: 0.0036, time: 0.0028\n",
            "batch: 355/979, loss: 0.0036, time: 0.0031\n",
            "batch: 356/979, loss: 0.0036, time: 0.0029\n",
            "batch: 357/979, loss: 0.0036, time: 0.0029\n",
            "batch: 358/979, loss: 0.0036, time: 0.0029\n",
            "batch: 359/979, loss: 0.0036, time: 0.0029\n",
            "batch: 360/979, loss: 0.0036, time: 0.0029\n",
            "batch: 361/979, loss: 0.0038, time: 0.0040\n",
            "batch: 362/979, loss: 0.0038, time: 0.0029\n",
            "batch: 363/979, loss: 0.0038, time: 0.0030\n",
            "batch: 364/979, loss: 0.0038, time: 0.0029\n",
            "batch: 365/979, loss: 0.0038, time: 0.0029\n",
            "batch: 366/979, loss: 0.0038, time: 0.0029\n",
            "batch: 367/979, loss: 0.0038, time: 0.0029\n",
            "batch: 368/979, loss: 0.0038, time: 0.0029\n",
            "batch: 369/979, loss: 0.0038, time: 0.0034\n",
            "batch: 370/979, loss: 0.0038, time: 0.0044\n",
            "batch: 371/979, loss: 0.0038, time: 0.0043\n",
            "batch: 372/979, loss: 0.0038, time: 0.0044\n",
            "batch: 373/979, loss: 0.0038, time: 0.0028\n",
            "batch: 374/979, loss: 0.0037, time: 0.0029\n",
            "batch: 375/979, loss: 0.0037, time: 0.0029\n",
            "batch: 376/979, loss: 0.0037, time: 0.0029\n",
            "batch: 377/979, loss: 0.0037, time: 0.0029\n",
            "batch: 378/979, loss: 0.0037, time: 0.0028\n",
            "batch: 379/979, loss: 0.0037, time: 0.0028\n",
            "batch: 380/979, loss: 0.0037, time: 0.0028\n",
            "batch: 381/979, loss: 0.0037, time: 0.0029\n",
            "batch: 382/979, loss: 0.0037, time: 0.0029\n",
            "batch: 383/979, loss: 0.0037, time: 0.0028\n",
            "batch: 384/979, loss: 0.0037, time: 0.0028\n",
            "batch: 385/979, loss: 0.0037, time: 0.0028\n",
            "batch: 386/979, loss: 0.0037, time: 0.0028\n",
            "batch: 387/979, loss: 0.0037, time: 0.0029\n",
            "batch: 388/979, loss: 0.0037, time: 0.0029\n",
            "batch: 389/979, loss: 0.0037, time: 0.0028\n",
            "batch: 390/979, loss: 0.0037, time: 0.0028\n",
            "batch: 391/979, loss: 0.0037, time: 0.0029\n",
            "batch: 392/979, loss: 0.0037, time: 0.0038\n",
            "batch: 393/979, loss: 0.0037, time: 0.0029\n",
            "batch: 394/979, loss: 0.0037, time: 0.0028\n",
            "batch: 395/979, loss: 0.0037, time: 0.0029\n",
            "batch: 396/979, loss: 0.0037, time: 0.0028\n",
            "batch: 397/979, loss: 0.0037, time: 0.0028\n",
            "batch: 398/979, loss: 0.0037, time: 0.0038\n",
            "batch: 399/979, loss: 0.0037, time: 0.0028\n",
            "batch: 400/979, loss: 0.0037, time: 0.0036\n",
            "batch: 401/979, loss: 0.0037, time: 0.0037\n",
            "batch: 402/979, loss: 0.0038, time: 0.0029\n",
            "batch: 403/979, loss: 0.0038, time: 0.0030\n",
            "batch: 404/979, loss: 0.0038, time: 0.0029\n",
            "batch: 405/979, loss: 0.0038, time: 0.0029\n",
            "batch: 406/979, loss: 0.0038, time: 0.0029\n",
            "batch: 407/979, loss: 0.0038, time: 0.0029\n",
            "batch: 408/979, loss: 0.0038, time: 0.0028\n",
            "batch: 409/979, loss: 0.0038, time: 0.0029\n",
            "batch: 410/979, loss: 0.0038, time: 0.0028\n",
            "batch: 411/979, loss: 0.0038, time: 0.0029\n",
            "batch: 412/979, loss: 0.0038, time: 0.0028\n",
            "batch: 413/979, loss: 0.0038, time: 0.0029\n",
            "batch: 414/979, loss: 0.0038, time: 0.0029\n",
            "batch: 415/979, loss: 0.0037, time: 0.0028\n",
            "batch: 416/979, loss: 0.0037, time: 0.0030\n",
            "batch: 417/979, loss: 0.0037, time: 0.0029\n",
            "batch: 418/979, loss: 0.0042, time: 0.0028\n",
            "batch: 419/979, loss: 0.0042, time: 0.0028\n",
            "batch: 420/979, loss: 0.0042, time: 0.0028\n",
            "batch: 421/979, loss: 0.0042, time: 0.0028\n",
            "batch: 422/979, loss: 0.0042, time: 0.0030\n",
            "batch: 423/979, loss: 0.0042, time: 0.0029\n",
            "batch: 424/979, loss: 0.0042, time: 0.0028\n",
            "batch: 425/979, loss: 0.0041, time: 0.0083\n",
            "batch: 426/979, loss: 0.0041, time: 0.0030\n",
            "batch: 427/979, loss: 0.0041, time: 0.0028\n",
            "batch: 428/979, loss: 0.0041, time: 0.0030\n",
            "batch: 429/979, loss: 0.0041, time: 0.0028\n",
            "batch: 430/979, loss: 0.0041, time: 0.0028\n",
            "batch: 431/979, loss: 0.0041, time: 0.0028\n",
            "batch: 432/979, loss: 0.0041, time: 0.0029\n",
            "batch: 433/979, loss: 0.0041, time: 0.0028\n",
            "batch: 434/979, loss: 0.0041, time: 0.0029\n",
            "batch: 435/979, loss: 0.0041, time: 0.0029\n",
            "batch: 436/979, loss: 0.0041, time: 0.0029\n",
            "batch: 437/979, loss: 0.0041, time: 0.0029\n",
            "batch: 438/979, loss: 0.0041, time: 0.0031\n",
            "batch: 439/979, loss: 0.0041, time: 0.0029\n",
            "batch: 440/979, loss: 0.0040, time: 0.0029\n",
            "batch: 441/979, loss: 0.0040, time: 0.0028\n",
            "batch: 442/979, loss: 0.0040, time: 0.0031\n",
            "batch: 443/979, loss: 0.0040, time: 0.0029\n",
            "batch: 444/979, loss: 0.0040, time: 0.0029\n",
            "batch: 445/979, loss: 0.0040, time: 0.0030\n",
            "batch: 446/979, loss: 0.0040, time: 0.0028\n",
            "batch: 447/979, loss: 0.0040, time: 0.0029\n",
            "batch: 448/979, loss: 0.0040, time: 0.0029\n",
            "batch: 449/979, loss: 0.0040, time: 0.0029\n",
            "batch: 450/979, loss: 0.0040, time: 0.0028\n",
            "batch: 451/979, loss: 0.0040, time: 0.0029\n",
            "batch: 452/979, loss: 0.0040, time: 0.0029\n",
            "batch: 453/979, loss: 0.0040, time: 0.0031\n",
            "batch: 454/979, loss: 0.0040, time: 0.0030\n",
            "batch: 455/979, loss: 0.0040, time: 0.0030\n",
            "batch: 456/979, loss: 0.0040, time: 0.0046\n",
            "batch: 457/979, loss: 0.0040, time: 0.0034\n",
            "batch: 458/979, loss: 0.0040, time: 0.0029\n",
            "batch: 459/979, loss: 0.0040, time: 0.0029\n",
            "batch: 460/979, loss: 0.0040, time: 0.0029\n",
            "batch: 461/979, loss: 0.0040, time: 0.0028\n",
            "batch: 462/979, loss: 0.0040, time: 0.0029\n",
            "batch: 463/979, loss: 0.0040, time: 0.0029\n",
            "batch: 464/979, loss: 0.0040, time: 0.0028\n",
            "batch: 465/979, loss: 0.0039, time: 0.0028\n",
            "batch: 466/979, loss: 0.0039, time: 0.0029\n",
            "batch: 467/979, loss: 0.0039, time: 0.0028\n",
            "batch: 468/979, loss: 0.0039, time: 0.0028\n",
            "batch: 469/979, loss: 0.0039, time: 0.0028\n",
            "batch: 470/979, loss: 0.0039, time: 0.0029\n",
            "batch: 471/979, loss: 0.0039, time: 0.0032\n",
            "batch: 472/979, loss: 0.0039, time: 0.0028\n",
            "batch: 473/979, loss: 0.0039, time: 0.0030\n",
            "batch: 474/979, loss: 0.0039, time: 0.0029\n",
            "batch: 475/979, loss: 0.0039, time: 0.0030\n",
            "batch: 476/979, loss: 0.0039, time: 0.0030\n",
            "batch: 477/979, loss: 0.0039, time: 0.0029\n",
            "batch: 478/979, loss: 0.0039, time: 0.0030\n",
            "batch: 479/979, loss: 0.0039, time: 0.0029\n",
            "batch: 480/979, loss: 0.0039, time: 0.0028\n",
            "batch: 481/979, loss: 0.0039, time: 0.0028\n",
            "batch: 482/979, loss: 0.0039, time: 0.0029\n",
            "batch: 483/979, loss: 0.0039, time: 0.0035\n",
            "batch: 484/979, loss: 0.0039, time: 0.0029\n",
            "batch: 485/979, loss: 0.0039, time: 0.0029\n",
            "batch: 486/979, loss: 0.0039, time: 0.0028\n",
            "batch: 487/979, loss: 0.0039, time: 0.0028\n",
            "batch: 488/979, loss: 0.0039, time: 0.0029\n",
            "batch: 489/979, loss: 0.0039, time: 0.0053\n",
            "batch: 490/979, loss: 0.0039, time: 0.0029\n",
            "batch: 491/979, loss: 0.0039, time: 0.0029\n",
            "batch: 492/979, loss: 0.0038, time: 0.0029\n",
            "batch: 493/979, loss: 0.0038, time: 0.0028\n",
            "batch: 494/979, loss: 0.0039, time: 0.0028\n",
            "batch: 495/979, loss: 0.0039, time: 0.0030\n",
            "batch: 496/979, loss: 0.0039, time: 0.0036\n",
            "batch: 497/979, loss: 0.0039, time: 0.0037\n",
            "batch: 498/979, loss: 0.0039, time: 0.0035\n",
            "batch: 499/979, loss: 0.0039, time: 0.0029\n",
            "batch: 500/979, loss: 0.0039, time: 0.0031\n",
            "batch: 501/979, loss: 0.0039, time: 0.0029\n",
            "batch: 502/979, loss: 0.0038, time: 0.0028\n",
            "batch: 503/979, loss: 0.0038, time: 0.0029\n",
            "batch: 504/979, loss: 0.0038, time: 0.0058\n",
            "batch: 505/979, loss: 0.0038, time: 0.0030\n",
            "batch: 506/979, loss: 0.0038, time: 0.0028\n",
            "batch: 507/979, loss: 0.0038, time: 0.0029\n",
            "batch: 508/979, loss: 0.0038, time: 0.0029\n",
            "batch: 509/979, loss: 0.0038, time: 0.0029\n",
            "batch: 510/979, loss: 0.0038, time: 0.0028\n",
            "batch: 511/979, loss: 0.0038, time: 0.0028\n",
            "batch: 512/979, loss: 0.0038, time: 0.0028\n",
            "batch: 513/979, loss: 0.0038, time: 0.0028\n",
            "batch: 514/979, loss: 0.0038, time: 0.0028\n",
            "batch: 515/979, loss: 0.0038, time: 0.0028\n",
            "batch: 516/979, loss: 0.0038, time: 0.0029\n",
            "batch: 517/979, loss: 0.0038, time: 0.0029\n",
            "batch: 518/979, loss: 0.0038, time: 0.0030\n",
            "batch: 519/979, loss: 0.0038, time: 0.0028\n",
            "batch: 520/979, loss: 0.0038, time: 0.0054\n",
            "batch: 521/979, loss: 0.0037, time: 0.0031\n",
            "batch: 522/979, loss: 0.0037, time: 0.0029\n",
            "batch: 523/979, loss: 0.0037, time: 0.0029\n",
            "batch: 524/979, loss: 0.0037, time: 0.0028\n",
            "batch: 525/979, loss: 0.0037, time: 0.0028\n",
            "batch: 526/979, loss: 0.0037, time: 0.0029\n",
            "batch: 527/979, loss: 0.0037, time: 0.0028\n",
            "batch: 528/979, loss: 0.0037, time: 0.0045\n",
            "batch: 529/979, loss: 0.0037, time: 0.0041\n",
            "batch: 530/979, loss: 0.0037, time: 0.0046\n",
            "batch: 531/979, loss: 0.0037, time: 0.0029\n",
            "batch: 532/979, loss: 0.0037, time: 0.0028\n",
            "batch: 533/979, loss: 0.0037, time: 0.0028\n",
            "batch: 534/979, loss: 0.0037, time: 0.0028\n",
            "batch: 535/979, loss: 0.0037, time: 0.0028\n",
            "batch: 536/979, loss: 0.0037, time: 0.0028\n",
            "batch: 537/979, loss: 0.0037, time: 0.0031\n",
            "batch: 538/979, loss: 0.0037, time: 0.0029\n",
            "batch: 539/979, loss: 0.0037, time: 0.0030\n",
            "batch: 540/979, loss: 0.0037, time: 0.0029\n",
            "batch: 541/979, loss: 0.0037, time: 0.0030\n",
            "batch: 542/979, loss: 0.0037, time: 0.0029\n",
            "batch: 543/979, loss: 0.0037, time: 0.0028\n",
            "batch: 544/979, loss: 0.0037, time: 0.0029\n",
            "batch: 545/979, loss: 0.0037, time: 0.0030\n",
            "batch: 546/979, loss: 0.0037, time: 0.0029\n",
            "batch: 547/979, loss: 0.0037, time: 0.0030\n",
            "batch: 548/979, loss: 0.0037, time: 0.0028\n",
            "batch: 549/979, loss: 0.0037, time: 0.0029\n",
            "batch: 550/979, loss: 0.0037, time: 0.0028\n",
            "batch: 551/979, loss: 0.0037, time: 0.0049\n",
            "batch: 552/979, loss: 0.0037, time: 0.0039\n",
            "batch: 553/979, loss: 0.0037, time: 0.0029\n",
            "batch: 554/979, loss: 0.0037, time: 0.0029\n",
            "batch: 555/979, loss: 0.0037, time: 0.0036\n",
            "batch: 556/979, loss: 0.0037, time: 0.0044\n",
            "batch: 557/979, loss: 0.0037, time: 0.0040\n",
            "batch: 558/979, loss: 0.0036, time: 0.0038\n",
            "batch: 559/979, loss: 0.0036, time: 0.0041\n",
            "batch: 560/979, loss: 0.0036, time: 0.0029\n",
            "batch: 561/979, loss: 0.0036, time: 0.0030\n",
            "batch: 562/979, loss: 0.0036, time: 0.0029\n",
            "batch: 563/979, loss: 0.0036, time: 0.0029\n",
            "batch: 564/979, loss: 0.0036, time: 0.0029\n",
            "batch: 565/979, loss: 0.0036, time: 0.0028\n",
            "batch: 566/979, loss: 0.0036, time: 0.0029\n",
            "batch: 567/979, loss: 0.0036, time: 0.0028\n",
            "batch: 568/979, loss: 0.0036, time: 0.0029\n",
            "batch: 569/979, loss: 0.0036, time: 0.0029\n",
            "batch: 570/979, loss: 0.0036, time: 0.0028\n",
            "batch: 571/979, loss: 0.0036, time: 0.0028\n",
            "batch: 572/979, loss: 0.0036, time: 0.0029\n",
            "batch: 573/979, loss: 0.0036, time: 0.0029\n",
            "batch: 574/979, loss: 0.0036, time: 0.0028\n",
            "batch: 575/979, loss: 0.0036, time: 0.0028\n",
            "batch: 576/979, loss: 0.0036, time: 0.0028\n",
            "batch: 577/979, loss: 0.0036, time: 0.0030\n",
            "batch: 578/979, loss: 0.0036, time: 0.0029\n",
            "batch: 579/979, loss: 0.0036, time: 0.0030\n",
            "batch: 580/979, loss: 0.0036, time: 0.0028\n",
            "batch: 581/979, loss: 0.0036, time: 0.0028\n",
            "batch: 582/979, loss: 0.0036, time: 0.0028\n",
            "batch: 583/979, loss: 0.0036, time: 0.0038\n",
            "batch: 584/979, loss: 0.0036, time: 0.0031\n",
            "batch: 585/979, loss: 0.0035, time: 0.0029\n",
            "batch: 586/979, loss: 0.0035, time: 0.0028\n",
            "batch: 587/979, loss: 0.0035, time: 0.0028\n",
            "batch: 588/979, loss: 0.0035, time: 0.0028\n",
            "batch: 589/979, loss: 0.0035, time: 0.0028\n",
            "batch: 590/979, loss: 0.0035, time: 0.0029\n",
            "batch: 591/979, loss: 0.0035, time: 0.0029\n",
            "batch: 592/979, loss: 0.0035, time: 0.0028\n",
            "batch: 593/979, loss: 0.0035, time: 0.0028\n",
            "batch: 594/979, loss: 0.0035, time: 0.0029\n",
            "batch: 595/979, loss: 0.0035, time: 0.0029\n",
            "batch: 596/979, loss: 0.0035, time: 0.0032\n",
            "batch: 597/979, loss: 0.0035, time: 0.0029\n",
            "batch: 598/979, loss: 0.0035, time: 0.0029\n",
            "batch: 599/979, loss: 0.0035, time: 0.0029\n",
            "batch: 600/979, loss: 0.0035, time: 0.0028\n",
            "batch: 601/979, loss: 0.0035, time: 0.0038\n",
            "batch: 602/979, loss: 0.0035, time: 0.0028\n",
            "batch: 603/979, loss: 0.0035, time: 0.0029\n",
            "batch: 604/979, loss: 0.0035, time: 0.0028\n",
            "batch: 605/979, loss: 0.0035, time: 0.0028\n",
            "batch: 606/979, loss: 0.0035, time: 0.0029\n",
            "batch: 607/979, loss: 0.0035, time: 0.0029\n",
            "batch: 608/979, loss: 0.0035, time: 0.0029\n",
            "batch: 609/979, loss: 0.0035, time: 0.0029\n",
            "batch: 610/979, loss: 0.0035, time: 0.0032\n",
            "batch: 611/979, loss: 0.0035, time: 0.0029\n",
            "batch: 612/979, loss: 0.0035, time: 0.0028\n",
            "batch: 613/979, loss: 0.0035, time: 0.0029\n",
            "batch: 614/979, loss: 0.0035, time: 0.0036\n",
            "batch: 615/979, loss: 0.0035, time: 0.0071\n",
            "batch: 616/979, loss: 0.0035, time: 0.0029\n",
            "batch: 617/979, loss: 0.0035, time: 0.0028\n",
            "batch: 618/979, loss: 0.0035, time: 0.0054\n",
            "batch: 619/979, loss: 0.0035, time: 0.0044\n",
            "batch: 620/979, loss: 0.0034, time: 0.0028\n",
            "batch: 621/979, loss: 0.0034, time: 0.0028\n",
            "batch: 622/979, loss: 0.0034, time: 0.0028\n",
            "batch: 623/979, loss: 0.0034, time: 0.0029\n",
            "batch: 624/979, loss: 0.0034, time: 0.0029\n",
            "batch: 625/979, loss: 0.0034, time: 0.0028\n",
            "batch: 626/979, loss: 0.0034, time: 0.0028\n",
            "batch: 627/979, loss: 0.0034, time: 0.0029\n",
            "batch: 628/979, loss: 0.0034, time: 0.0028\n",
            "batch: 629/979, loss: 0.0034, time: 0.0028\n",
            "batch: 630/979, loss: 0.0034, time: 0.0028\n",
            "batch: 631/979, loss: 0.0034, time: 0.0029\n",
            "batch: 632/979, loss: 0.0034, time: 0.0028\n",
            "batch: 633/979, loss: 0.0034, time: 0.0029\n",
            "batch: 634/979, loss: 0.0034, time: 0.0028\n",
            "batch: 635/979, loss: 0.0034, time: 0.0028\n",
            "batch: 636/979, loss: 0.0034, time: 0.0028\n",
            "batch: 637/979, loss: 0.0034, time: 0.0030\n",
            "batch: 638/979, loss: 0.0034, time: 0.0028\n",
            "batch: 639/979, loss: 0.0034, time: 0.0028\n",
            "batch: 640/979, loss: 0.0034, time: 0.0028\n",
            "batch: 641/979, loss: 0.0034, time: 0.0030\n",
            "batch: 642/979, loss: 0.0036, time: 0.0029\n",
            "batch: 643/979, loss: 0.0036, time: 0.0029\n",
            "batch: 644/979, loss: 0.0036, time: 0.0029\n",
            "batch: 645/979, loss: 0.0036, time: 0.0029\n",
            "batch: 646/979, loss: 0.0036, time: 0.0036\n",
            "batch: 647/979, loss: 0.0036, time: 0.0057\n",
            "batch: 648/979, loss: 0.0036, time: 0.0031\n",
            "batch: 649/979, loss: 0.0036, time: 0.0028\n",
            "batch: 650/979, loss: 0.0036, time: 0.0037\n",
            "batch: 651/979, loss: 0.0036, time: 0.0040\n",
            "batch: 652/979, loss: 0.0036, time: 0.0035\n",
            "batch: 653/979, loss: 0.0036, time: 0.0030\n",
            "batch: 654/979, loss: 0.0037, time: 0.0028\n",
            "batch: 655/979, loss: 0.0037, time: 0.0028\n",
            "batch: 656/979, loss: 0.0037, time: 0.0028\n",
            "batch: 657/979, loss: 0.0037, time: 0.0027\n",
            "batch: 658/979, loss: 0.0036, time: 0.0027\n",
            "batch: 659/979, loss: 0.0037, time: 0.0028\n",
            "batch: 660/979, loss: 0.0037, time: 0.0028\n",
            "batch: 661/979, loss: 0.0037, time: 0.0028\n",
            "batch: 662/979, loss: 0.0037, time: 0.0028\n",
            "batch: 663/979, loss: 0.0037, time: 0.0027\n",
            "batch: 664/979, loss: 0.0036, time: 0.0028\n",
            "batch: 665/979, loss: 0.0036, time: 0.0028\n",
            "batch: 666/979, loss: 0.0036, time: 0.0028\n",
            "batch: 667/979, loss: 0.0036, time: 0.0028\n",
            "batch: 668/979, loss: 0.0036, time: 0.0028\n",
            "batch: 669/979, loss: 0.0036, time: 0.0028\n",
            "batch: 670/979, loss: 0.0036, time: 0.0041\n",
            "batch: 671/979, loss: 0.0036, time: 0.0029\n",
            "batch: 672/979, loss: 0.0036, time: 0.0028\n",
            "batch: 673/979, loss: 0.0036, time: 0.0028\n",
            "batch: 674/979, loss: 0.0036, time: 0.0029\n",
            "batch: 675/979, loss: 0.0036, time: 0.0029\n",
            "batch: 676/979, loss: 0.0036, time: 0.0028\n",
            "batch: 677/979, loss: 0.0036, time: 0.0039\n",
            "batch: 678/979, loss: 0.0036, time: 0.0036\n",
            "batch: 679/979, loss: 0.0036, time: 0.0047\n",
            "batch: 680/979, loss: 0.0036, time: 0.0028\n",
            "batch: 681/979, loss: 0.0036, time: 0.0029\n",
            "batch: 682/979, loss: 0.0036, time: 0.0052\n",
            "batch: 683/979, loss: 0.0036, time: 0.0029\n",
            "batch: 684/979, loss: 0.0036, time: 0.0029\n",
            "batch: 685/979, loss: 0.0036, time: 0.0040\n",
            "batch: 686/979, loss: 0.0037, time: 0.0045\n",
            "batch: 687/979, loss: 0.0037, time: 0.0041\n",
            "batch: 688/979, loss: 0.0037, time: 0.0037\n",
            "batch: 689/979, loss: 0.0037, time: 0.0028\n",
            "batch: 690/979, loss: 0.0037, time: 0.0028\n",
            "batch: 691/979, loss: 0.0037, time: 0.0028\n",
            "batch: 692/979, loss: 0.0037, time: 0.0032\n",
            "batch: 693/979, loss: 0.0037, time: 0.0028\n",
            "batch: 694/979, loss: 0.0037, time: 0.0029\n",
            "batch: 695/979, loss: 0.0037, time: 0.0028\n",
            "batch: 696/979, loss: 0.0037, time: 0.0028\n",
            "batch: 697/979, loss: 0.0037, time: 0.0030\n",
            "batch: 698/979, loss: 0.0037, time: 0.0029\n",
            "batch: 699/979, loss: 0.0037, time: 0.0028\n",
            "batch: 700/979, loss: 0.0037, time: 0.0028\n",
            "batch: 701/979, loss: 0.0037, time: 0.0029\n",
            "batch: 702/979, loss: 0.0037, time: 0.0028\n",
            "batch: 703/979, loss: 0.0037, time: 0.0029\n",
            "batch: 704/979, loss: 0.0037, time: 0.0028\n",
            "batch: 705/979, loss: 0.0037, time: 0.0028\n",
            "batch: 706/979, loss: 0.0037, time: 0.0028\n",
            "batch: 707/979, loss: 0.0036, time: 0.0028\n",
            "batch: 708/979, loss: 0.0036, time: 0.0029\n",
            "batch: 709/979, loss: 0.0036, time: 0.0031\n",
            "batch: 710/979, loss: 0.0036, time: 0.0073\n",
            "batch: 711/979, loss: 0.0036, time: 0.0043\n",
            "batch: 712/979, loss: 0.0036, time: 0.0028\n",
            "batch: 713/979, loss: 0.0036, time: 0.0030\n",
            "batch: 714/979, loss: 0.0036, time: 0.0029\n",
            "batch: 715/979, loss: 0.0036, time: 0.0028\n",
            "batch: 716/979, loss: 0.0036, time: 0.0029\n",
            "batch: 717/979, loss: 0.0036, time: 0.0029\n",
            "batch: 718/979, loss: 0.0036, time: 0.0029\n",
            "batch: 719/979, loss: 0.0036, time: 0.0029\n",
            "batch: 720/979, loss: 0.0036, time: 0.0029\n",
            "batch: 721/979, loss: 0.0036, time: 0.0029\n",
            "batch: 722/979, loss: 0.0036, time: 0.0030\n",
            "batch: 723/979, loss: 0.0036, time: 0.0029\n",
            "batch: 724/979, loss: 0.0036, time: 0.0029\n",
            "batch: 725/979, loss: 0.0036, time: 0.0028\n",
            "batch: 726/979, loss: 0.0036, time: 0.0028\n",
            "batch: 727/979, loss: 0.0036, time: 0.0028\n",
            "batch: 728/979, loss: 0.0036, time: 0.0029\n",
            "batch: 729/979, loss: 0.0036, time: 0.0029\n",
            "batch: 730/979, loss: 0.0036, time: 0.0028\n",
            "batch: 731/979, loss: 0.0036, time: 0.0028\n",
            "batch: 732/979, loss: 0.0036, time: 0.0029\n",
            "batch: 733/979, loss: 0.0036, time: 0.0039\n",
            "batch: 734/979, loss: 0.0036, time: 0.0028\n",
            "batch: 735/979, loss: 0.0036, time: 0.0028\n",
            "batch: 736/979, loss: 0.0036, time: 0.0028\n",
            "batch: 737/979, loss: 0.0036, time: 0.0029\n",
            "batch: 738/979, loss: 0.0036, time: 0.0029\n",
            "batch: 739/979, loss: 0.0036, time: 0.0028\n",
            "batch: 740/979, loss: 0.0036, time: 0.0028\n",
            "batch: 741/979, loss: 0.0036, time: 0.0028\n",
            "batch: 742/979, loss: 0.0036, time: 0.0037\n",
            "batch: 743/979, loss: 0.0036, time: 0.0039\n",
            "batch: 744/979, loss: 0.0036, time: 0.0040\n",
            "batch: 745/979, loss: 0.0036, time: 0.0034\n",
            "batch: 746/979, loss: 0.0036, time: 0.0028\n",
            "batch: 747/979, loss: 0.0036, time: 0.0028\n",
            "batch: 748/979, loss: 0.0036, time: 0.0030\n",
            "batch: 749/979, loss: 0.0036, time: 0.0036\n",
            "batch: 750/979, loss: 0.0036, time: 0.0030\n",
            "batch: 751/979, loss: 0.0036, time: 0.0029\n",
            "batch: 752/979, loss: 0.0036, time: 0.0028\n",
            "batch: 753/979, loss: 0.0036, time: 0.0028\n",
            "batch: 754/979, loss: 0.0037, time: 0.0029\n",
            "batch: 755/979, loss: 0.0037, time: 0.0030\n",
            "batch: 756/979, loss: 0.0037, time: 0.0028\n",
            "batch: 757/979, loss: 0.0037, time: 0.0029\n",
            "batch: 758/979, loss: 0.0036, time: 0.0029\n",
            "batch: 759/979, loss: 0.0036, time: 0.0028\n",
            "batch: 760/979, loss: 0.0036, time: 0.0032\n",
            "batch: 761/979, loss: 0.0036, time: 0.0031\n",
            "batch: 762/979, loss: 0.0036, time: 0.0032\n",
            "batch: 763/979, loss: 0.0036, time: 0.0028\n",
            "batch: 764/979, loss: 0.0036, time: 0.0030\n",
            "batch: 765/979, loss: 0.0036, time: 0.0029\n",
            "batch: 766/979, loss: 0.0036, time: 0.0030\n",
            "batch: 767/979, loss: 0.0036, time: 0.0028\n",
            "batch: 768/979, loss: 0.0036, time: 0.0028\n",
            "batch: 769/979, loss: 0.0036, time: 0.0029\n",
            "batch: 770/979, loss: 0.0036, time: 0.0029\n",
            "batch: 771/979, loss: 0.0036, time: 0.0028\n",
            "batch: 772/979, loss: 0.0036, time: 0.0030\n",
            "batch: 773/979, loss: 0.0036, time: 0.0029\n",
            "batch: 774/979, loss: 0.0036, time: 0.0044\n",
            "batch: 775/979, loss: 0.0036, time: 0.0033\n",
            "batch: 776/979, loss: 0.0036, time: 0.0028\n",
            "batch: 777/979, loss: 0.0036, time: 0.0029\n",
            "batch: 778/979, loss: 0.0036, time: 0.0030\n",
            "batch: 779/979, loss: 0.0036, time: 0.0029\n",
            "batch: 780/979, loss: 0.0036, time: 0.0028\n",
            "batch: 781/979, loss: 0.0036, time: 0.0029\n",
            "batch: 782/979, loss: 0.0036, time: 0.0028\n",
            "batch: 783/979, loss: 0.0036, time: 0.0028\n",
            "batch: 784/979, loss: 0.0036, time: 0.0029\n",
            "batch: 785/979, loss: 0.0036, time: 0.0029\n",
            "batch: 786/979, loss: 0.0036, time: 0.0028\n",
            "batch: 787/979, loss: 0.0036, time: 0.0028\n",
            "batch: 788/979, loss: 0.0036, time: 0.0028\n",
            "batch: 789/979, loss: 0.0036, time: 0.0028\n",
            "batch: 790/979, loss: 0.0036, time: 0.0028\n",
            "batch: 791/979, loss: 0.0036, time: 0.0029\n",
            "batch: 792/979, loss: 0.0036, time: 0.0028\n",
            "batch: 793/979, loss: 0.0036, time: 0.0028\n",
            "batch: 794/979, loss: 0.0036, time: 0.0028\n",
            "batch: 795/979, loss: 0.0036, time: 0.0029\n",
            "batch: 796/979, loss: 0.0036, time: 0.0029\n",
            "batch: 797/979, loss: 0.0036, time: 0.0028\n",
            "batch: 798/979, loss: 0.0036, time: 0.0028\n",
            "batch: 799/979, loss: 0.0036, time: 0.0029\n",
            "batch: 800/979, loss: 0.0036, time: 0.0028\n",
            "batch: 801/979, loss: 0.0036, time: 0.0028\n",
            "batch: 802/979, loss: 0.0036, time: 0.0028\n",
            "batch: 803/979, loss: 0.0036, time: 0.0030\n",
            "batch: 804/979, loss: 0.0036, time: 0.0029\n",
            "batch: 805/979, loss: 0.0036, time: 0.0042\n",
            "batch: 806/979, loss: 0.0036, time: 0.0058\n",
            "batch: 807/979, loss: 0.0036, time: 0.0037\n",
            "batch: 808/979, loss: 0.0036, time: 0.0038\n",
            "batch: 809/979, loss: 0.0036, time: 0.0031\n",
            "batch: 810/979, loss: 0.0036, time: 0.0029\n",
            "batch: 811/979, loss: 0.0036, time: 0.0028\n",
            "batch: 812/979, loss: 0.0036, time: 0.0029\n",
            "batch: 813/979, loss: 0.0036, time: 0.0028\n",
            "batch: 814/979, loss: 0.0036, time: 0.0028\n",
            "batch: 815/979, loss: 0.0036, time: 0.0028\n",
            "batch: 816/979, loss: 0.0036, time: 0.0029\n",
            "batch: 817/979, loss: 0.0036, time: 0.0029\n",
            "batch: 818/979, loss: 0.0036, time: 0.0029\n",
            "batch: 819/979, loss: 0.0036, time: 0.0029\n",
            "batch: 820/979, loss: 0.0036, time: 0.0028\n",
            "batch: 821/979, loss: 0.0036, time: 0.0029\n",
            "batch: 822/979, loss: 0.0036, time: 0.0029\n",
            "batch: 823/979, loss: 0.0036, time: 0.0028\n",
            "batch: 824/979, loss: 0.0036, time: 0.0028\n",
            "batch: 825/979, loss: 0.0036, time: 0.0029\n",
            "batch: 826/979, loss: 0.0036, time: 0.0029\n",
            "batch: 827/979, loss: 0.0036, time: 0.0029\n",
            "batch: 828/979, loss: 0.0036, time: 0.0028\n",
            "batch: 829/979, loss: 0.0036, time: 0.0028\n",
            "batch: 830/979, loss: 0.0036, time: 0.0029\n",
            "batch: 831/979, loss: 0.0036, time: 0.0029\n",
            "batch: 832/979, loss: 0.0036, time: 0.0028\n",
            "batch: 833/979, loss: 0.0036, time: 0.0029\n",
            "batch: 834/979, loss: 0.0035, time: 0.0028\n",
            "batch: 835/979, loss: 0.0035, time: 0.0028\n",
            "batch: 836/979, loss: 0.0035, time: 0.0028\n",
            "batch: 837/979, loss: 0.0035, time: 0.0028\n",
            "batch: 838/979, loss: 0.0035, time: 0.0051\n",
            "batch: 839/979, loss: 0.0035, time: 0.0030\n",
            "batch: 840/979, loss: 0.0035, time: 0.0029\n",
            "batch: 841/979, loss: 0.0035, time: 0.0028\n",
            "batch: 842/979, loss: 0.0035, time: 0.0029\n",
            "batch: 843/979, loss: 0.0035, time: 0.0028\n",
            "batch: 844/979, loss: 0.0035, time: 0.0044\n",
            "batch: 845/979, loss: 0.0035, time: 0.0041\n",
            "batch: 846/979, loss: 0.0035, time: 0.0043\n",
            "batch: 847/979, loss: 0.0035, time: 0.0030\n",
            "batch: 848/979, loss: 0.0035, time: 0.0028\n",
            "batch: 849/979, loss: 0.0035, time: 0.0028\n",
            "batch: 850/979, loss: 0.0035, time: 0.0028\n",
            "batch: 851/979, loss: 0.0035, time: 0.0028\n",
            "batch: 852/979, loss: 0.0035, time: 0.0028\n",
            "batch: 853/979, loss: 0.0035, time: 0.0028\n",
            "batch: 854/979, loss: 0.0035, time: 0.0028\n",
            "batch: 855/979, loss: 0.0035, time: 0.0028\n",
            "batch: 856/979, loss: 0.0035, time: 0.0045\n",
            "batch: 857/979, loss: 0.0035, time: 0.0029\n",
            "batch: 858/979, loss: 0.0035, time: 0.0029\n",
            "batch: 859/979, loss: 0.0035, time: 0.0028\n",
            "batch: 860/979, loss: 0.0035, time: 0.0028\n",
            "batch: 861/979, loss: 0.0035, time: 0.0029\n",
            "batch: 862/979, loss: 0.0035, time: 0.0029\n",
            "batch: 863/979, loss: 0.0035, time: 0.0036\n",
            "batch: 864/979, loss: 0.0035, time: 0.0028\n",
            "batch: 865/979, loss: 0.0035, time: 0.0028\n",
            "batch: 866/979, loss: 0.0035, time: 0.0038\n",
            "batch: 867/979, loss: 0.0035, time: 0.0038\n",
            "batch: 868/979, loss: 0.0035, time: 0.0050\n",
            "batch: 869/979, loss: 0.0035, time: 0.0041\n",
            "batch: 870/979, loss: 0.0035, time: 0.0029\n",
            "batch: 871/979, loss: 0.0035, time: 0.0029\n",
            "batch: 872/979, loss: 0.0035, time: 0.0029\n",
            "batch: 873/979, loss: 0.0034, time: 0.0028\n",
            "batch: 874/979, loss: 0.0034, time: 0.0028\n",
            "batch: 875/979, loss: 0.0034, time: 0.0028\n",
            "batch: 876/979, loss: 0.0034, time: 0.0028\n",
            "batch: 877/979, loss: 0.0034, time: 0.0031\n",
            "batch: 878/979, loss: 0.0034, time: 0.0029\n",
            "batch: 879/979, loss: 0.0034, time: 0.0028\n",
            "batch: 880/979, loss: 0.0034, time: 0.0028\n",
            "batch: 881/979, loss: 0.0034, time: 0.0028\n",
            "batch: 882/979, loss: 0.0034, time: 0.0028\n",
            "batch: 883/979, loss: 0.0034, time: 0.0029\n",
            "batch: 884/979, loss: 0.0034, time: 0.0028\n",
            "batch: 885/979, loss: 0.0034, time: 0.0029\n",
            "batch: 886/979, loss: 0.0034, time: 0.0027\n",
            "batch: 887/979, loss: 0.0034, time: 0.0027\n",
            "batch: 888/979, loss: 0.0034, time: 0.0029\n",
            "batch: 889/979, loss: 0.0034, time: 0.0028\n",
            "batch: 890/979, loss: 0.0034, time: 0.0028\n",
            "batch: 891/979, loss: 0.0034, time: 0.0028\n",
            "batch: 892/979, loss: 0.0034, time: 0.0028\n",
            "batch: 893/979, loss: 0.0034, time: 0.0028\n",
            "batch: 894/979, loss: 0.0034, time: 0.0033\n",
            "batch: 895/979, loss: 0.0034, time: 0.0028\n",
            "batch: 896/979, loss: 0.0034, time: 0.0028\n",
            "batch: 897/979, loss: 0.0034, time: 0.0029\n",
            "batch: 898/979, loss: 0.0034, time: 0.0028\n",
            "batch: 899/979, loss: 0.0034, time: 0.0027\n",
            "batch: 900/979, loss: 0.0034, time: 0.0027\n",
            "batch: 901/979, loss: 0.0034, time: 0.0052\n",
            "batch: 902/979, loss: 0.0034, time: 0.0028\n",
            "batch: 903/979, loss: 0.0034, time: 0.0028\n",
            "batch: 904/979, loss: 0.0034, time: 0.0027\n",
            "batch: 905/979, loss: 0.0034, time: 0.0028\n",
            "batch: 906/979, loss: 0.0034, time: 0.0029\n",
            "batch: 907/979, loss: 0.0034, time: 0.0028\n",
            "batch: 908/979, loss: 0.0034, time: 0.0028\n",
            "batch: 909/979, loss: 0.0034, time: 0.0027\n",
            "batch: 910/979, loss: 0.0034, time: 0.0028\n",
            "batch: 911/979, loss: 0.0034, time: 0.0028\n",
            "batch: 912/979, loss: 0.0034, time: 0.0028\n",
            "batch: 913/979, loss: 0.0034, time: 0.0028\n",
            "batch: 914/979, loss: 0.0034, time: 0.0028\n",
            "batch: 915/979, loss: 0.0034, time: 0.0029\n",
            "batch: 916/979, loss: 0.0034, time: 0.0030\n",
            "batch: 917/979, loss: 0.0034, time: 0.0031\n",
            "batch: 918/979, loss: 0.0034, time: 0.0028\n",
            "batch: 919/979, loss: 0.0034, time: 0.0028\n",
            "batch: 920/979, loss: 0.0034, time: 0.0027\n",
            "batch: 921/979, loss: 0.0034, time: 0.0027\n",
            "batch: 922/979, loss: 0.0034, time: 0.0028\n",
            "batch: 923/979, loss: 0.0034, time: 0.0028\n",
            "batch: 924/979, loss: 0.0034, time: 0.0028\n",
            "batch: 925/979, loss: 0.0033, time: 0.0028\n",
            "batch: 926/979, loss: 0.0033, time: 0.0045\n",
            "batch: 927/979, loss: 0.0035, time: 0.0043\n",
            "batch: 928/979, loss: 0.0035, time: 0.0031\n",
            "batch: 929/979, loss: 0.0035, time: 0.0028\n",
            "batch: 930/979, loss: 0.0035, time: 0.0028\n",
            "batch: 931/979, loss: 0.0035, time: 0.0028\n",
            "batch: 932/979, loss: 0.0035, time: 0.0028\n",
            "batch: 933/979, loss: 0.0035, time: 0.0027\n",
            "batch: 934/979, loss: 0.0035, time: 0.0043\n",
            "batch: 935/979, loss: 0.0035, time: 0.0035\n",
            "batch: 936/979, loss: 0.0035, time: 0.0027\n",
            "batch: 937/979, loss: 0.0035, time: 0.0028\n",
            "batch: 938/979, loss: 0.0035, time: 0.0027\n",
            "batch: 939/979, loss: 0.0035, time: 0.0028\n",
            "batch: 940/979, loss: 0.0035, time: 0.0028\n",
            "batch: 941/979, loss: 0.0035, time: 0.0028\n",
            "batch: 942/979, loss: 0.0035, time: 0.0028\n",
            "batch: 943/979, loss: 0.0035, time: 0.0028\n",
            "batch: 944/979, loss: 0.0035, time: 0.0027\n",
            "batch: 945/979, loss: 0.0035, time: 0.0027\n",
            "batch: 946/979, loss: 0.0035, time: 0.0029\n",
            "batch: 947/979, loss: 0.0035, time: 0.0028\n",
            "batch: 948/979, loss: 0.0035, time: 0.0030\n",
            "batch: 949/979, loss: 0.0035, time: 0.0029\n",
            "batch: 950/979, loss: 0.0035, time: 0.0029\n",
            "batch: 951/979, loss: 0.0035, time: 0.0029\n",
            "batch: 952/979, loss: 0.0035, time: 0.0029\n",
            "batch: 953/979, loss: 0.0035, time: 0.0029\n",
            "batch: 954/979, loss: 0.0035, time: 0.0029\n",
            "batch: 955/979, loss: 0.0035, time: 0.0029\n",
            "batch: 956/979, loss: 0.0035, time: 0.0030\n",
            "batch: 957/979, loss: 0.0035, time: 0.0029\n",
            "batch: 958/979, loss: 0.0035, time: 0.0029\n",
            "batch: 959/979, loss: 0.0035, time: 0.0031\n",
            "batch: 960/979, loss: 0.0035, time: 0.0031\n",
            "batch: 961/979, loss: 0.0035, time: 0.0037\n",
            "batch: 962/979, loss: 0.0035, time: 0.0037\n",
            "batch: 963/979, loss: 0.0035, time: 0.0030\n",
            "batch: 964/979, loss: 0.0035, time: 0.0029\n",
            "batch: 965/979, loss: 0.0035, time: 0.0030\n",
            "batch: 966/979, loss: 0.0035, time: 0.0030\n",
            "batch: 967/979, loss: 0.0035, time: 0.0041\n",
            "batch: 968/979, loss: 0.0035, time: 0.0029\n",
            "batch: 969/979, loss: 0.0035, time: 0.0028\n",
            "batch: 970/979, loss: 0.0035, time: 0.0028\n",
            "batch: 971/979, loss: 0.0035, time: 0.0030\n",
            "batch: 972/979, loss: 0.0035, time: 0.0029\n",
            "batch: 973/979, loss: 0.0035, time: 0.0028\n",
            "batch: 974/979, loss: 0.0035, time: 0.0028\n",
            "batch: 975/979, loss: 0.0035, time: 0.0030\n",
            "batch: 976/979, loss: 0.0035, time: 0.0029\n",
            "batch: 977/979, loss: 0.0035, time: 0.0028\n",
            "batch: 978/979, loss: 0.0035, time: 0.0031\n",
            "batch: 979/979, loss: 0.0035, time: 0.0030\n",
            "batch: 1/979, loss: 0.0003, time: 0.0048\n",
            "batch: 2/979, loss: 0.0004, time: 0.0050\n",
            "batch: 3/979, loss: 0.0006, time: 0.0035\n",
            "batch: 4/979, loss: 0.0005, time: 0.0028\n",
            "batch: 5/979, loss: 0.0005, time: 0.0028\n",
            "batch: 6/979, loss: 0.0005, time: 0.0030\n",
            "batch: 7/979, loss: 0.0010, time: 0.0029\n",
            "batch: 8/979, loss: 0.0011, time: 0.0028\n",
            "batch: 9/979, loss: 0.0012, time: 0.0028\n",
            "batch: 10/979, loss: 0.0012, time: 0.0029\n",
            "batch: 11/979, loss: 0.0011, time: 0.0028\n",
            "batch: 12/979, loss: 0.0011, time: 0.0028\n",
            "batch: 13/979, loss: 0.0010, time: 0.0028\n",
            "batch: 14/979, loss: 0.0010, time: 0.0028\n",
            "batch: 15/979, loss: 0.0009, time: 0.0028\n",
            "batch: 16/979, loss: 0.0009, time: 0.0028\n",
            "batch: 17/979, loss: 0.0009, time: 0.0028\n",
            "batch: 18/979, loss: 0.0008, time: 0.0029\n",
            "batch: 19/979, loss: 0.0008, time: 0.0028\n",
            "batch: 20/979, loss: 0.0008, time: 0.0028\n",
            "batch: 21/979, loss: 0.0010, time: 0.0029\n",
            "batch: 22/979, loss: 0.0009, time: 0.0028\n",
            "batch: 23/979, loss: 0.0010, time: 0.0028\n",
            "batch: 24/979, loss: 0.0009, time: 0.0028\n",
            "batch: 25/979, loss: 0.0009, time: 0.0028\n",
            "batch: 26/979, loss: 0.0009, time: 0.0028\n",
            "batch: 27/979, loss: 0.0009, time: 0.0028\n",
            "batch: 28/979, loss: 0.0008, time: 0.0029\n",
            "batch: 29/979, loss: 0.0008, time: 0.0028\n",
            "batch: 30/979, loss: 0.0008, time: 0.0028\n",
            "batch: 31/979, loss: 0.0008, time: 0.0028\n",
            "batch: 32/979, loss: 0.0008, time: 0.0028\n",
            "batch: 33/979, loss: 0.0008, time: 0.0037\n",
            "batch: 34/979, loss: 0.0008, time: 0.0035\n",
            "batch: 35/979, loss: 0.0008, time: 0.0029\n",
            "batch: 36/979, loss: 0.0009, time: 0.0028\n",
            "batch: 37/979, loss: 0.0009, time: 0.0028\n",
            "batch: 38/979, loss: 0.0009, time: 0.0028\n",
            "batch: 39/979, loss: 0.0009, time: 0.0028\n",
            "batch: 40/979, loss: 0.0009, time: 0.0030\n",
            "batch: 41/979, loss: 0.0009, time: 0.0029\n",
            "batch: 42/979, loss: 0.0009, time: 0.0028\n",
            "batch: 43/979, loss: 0.0009, time: 0.0028\n",
            "batch: 44/979, loss: 0.0009, time: 0.0028\n",
            "batch: 45/979, loss: 0.0009, time: 0.0029\n",
            "batch: 46/979, loss: 0.0009, time: 0.0030\n",
            "batch: 47/979, loss: 0.0009, time: 0.0029\n",
            "batch: 48/979, loss: 0.0009, time: 0.0028\n",
            "batch: 49/979, loss: 0.0009, time: 0.0028\n",
            "batch: 50/979, loss: 0.0008, time: 0.0030\n",
            "batch: 51/979, loss: 0.0009, time: 0.0028\n",
            "batch: 52/979, loss: 0.0008, time: 0.0029\n",
            "batch: 53/979, loss: 0.0009, time: 0.0028\n",
            "batch: 54/979, loss: 0.0009, time: 0.0028\n",
            "batch: 55/979, loss: 0.0009, time: 0.0029\n",
            "batch: 56/979, loss: 0.0009, time: 0.0029\n",
            "batch: 57/979, loss: 0.0009, time: 0.0028\n",
            "batch: 58/979, loss: 0.0009, time: 0.0028\n",
            "batch: 59/979, loss: 0.0009, time: 0.0028\n",
            "batch: 60/979, loss: 0.0009, time: 0.0028\n",
            "batch: 61/979, loss: 0.0009, time: 0.0029\n",
            "batch: 62/979, loss: 0.0009, time: 0.0028\n",
            "batch: 63/979, loss: 0.0009, time: 0.0028\n",
            "batch: 64/979, loss: 0.0009, time: 0.0028\n",
            "batch: 65/979, loss: 0.0009, time: 0.0028\n",
            "batch: 66/979, loss: 0.0009, time: 0.0047\n",
            "batch: 67/979, loss: 0.0010, time: 0.0037\n",
            "batch: 68/979, loss: 0.0009, time: 0.0029\n",
            "batch: 69/979, loss: 0.0010, time: 0.0028\n",
            "batch: 70/979, loss: 0.0010, time: 0.0029\n",
            "batch: 71/979, loss: 0.0010, time: 0.0029\n",
            "batch: 72/979, loss: 0.0010, time: 0.0030\n",
            "batch: 73/979, loss: 0.0010, time: 0.0029\n",
            "batch: 74/979, loss: 0.0010, time: 0.0029\n",
            "batch: 75/979, loss: 0.0010, time: 0.0029\n",
            "batch: 76/979, loss: 0.0010, time: 0.0029\n",
            "batch: 77/979, loss: 0.0010, time: 0.0029\n",
            "batch: 78/979, loss: 0.0010, time: 0.0028\n",
            "batch: 79/979, loss: 0.0010, time: 0.0028\n",
            "batch: 80/979, loss: 0.0010, time: 0.0029\n",
            "batch: 81/979, loss: 0.0010, time: 0.0028\n",
            "batch: 82/979, loss: 0.0010, time: 0.0042\n",
            "batch: 83/979, loss: 0.0010, time: 0.0037\n",
            "batch: 84/979, loss: 0.0010, time: 0.0028\n",
            "batch: 85/979, loss: 0.0010, time: 0.0028\n",
            "batch: 86/979, loss: 0.0010, time: 0.0028\n",
            "batch: 87/979, loss: 0.0010, time: 0.0028\n",
            "batch: 88/979, loss: 0.0010, time: 0.0029\n",
            "batch: 89/979, loss: 0.0010, time: 0.0033\n",
            "batch: 90/979, loss: 0.0010, time: 0.0037\n",
            "batch: 91/979, loss: 0.0010, time: 0.0036\n",
            "batch: 92/979, loss: 0.0009, time: 0.0036\n",
            "batch: 93/979, loss: 0.0009, time: 0.0036\n",
            "batch: 94/979, loss: 0.0009, time: 0.0037\n",
            "batch: 95/979, loss: 0.0009, time: 0.0029\n",
            "batch: 96/979, loss: 0.0009, time: 0.0029\n",
            "batch: 97/979, loss: 0.0009, time: 0.0029\n",
            "batch: 98/979, loss: 0.0009, time: 0.0049\n",
            "batch: 99/979, loss: 0.0009, time: 0.0028\n",
            "batch: 100/979, loss: 0.0009, time: 0.0028\n",
            "batch: 101/979, loss: 0.0009, time: 0.0028\n",
            "batch: 102/979, loss: 0.0009, time: 0.0028\n",
            "batch: 103/979, loss: 0.0009, time: 0.0028\n",
            "batch: 104/979, loss: 0.0009, time: 0.0028\n",
            "batch: 105/979, loss: 0.0009, time: 0.0028\n",
            "batch: 106/979, loss: 0.0009, time: 0.0028\n",
            "batch: 107/979, loss: 0.0009, time: 0.0028\n",
            "batch: 108/979, loss: 0.0009, time: 0.0029\n",
            "batch: 109/979, loss: 0.0009, time: 0.0029\n",
            "batch: 110/979, loss: 0.0009, time: 0.0047\n",
            "batch: 111/979, loss: 0.0009, time: 0.0029\n",
            "batch: 112/979, loss: 0.0009, time: 0.0029\n",
            "batch: 113/979, loss: 0.0009, time: 0.0029\n",
            "batch: 114/979, loss: 0.0009, time: 0.0028\n",
            "batch: 115/979, loss: 0.0009, time: 0.0028\n",
            "batch: 116/979, loss: 0.0009, time: 0.0028\n",
            "batch: 117/979, loss: 0.0009, time: 0.0038\n",
            "batch: 118/979, loss: 0.0009, time: 0.0029\n",
            "batch: 119/979, loss: 0.0009, time: 0.0029\n",
            "batch: 120/979, loss: 0.0009, time: 0.0029\n",
            "batch: 121/979, loss: 0.0008, time: 0.0029\n",
            "batch: 122/979, loss: 0.0008, time: 0.0028\n",
            "batch: 123/979, loss: 0.0008, time: 0.0029\n",
            "batch: 124/979, loss: 0.0008, time: 0.0029\n",
            "batch: 125/979, loss: 0.0008, time: 0.0029\n",
            "batch: 126/979, loss: 0.0008, time: 0.0029\n",
            "batch: 127/979, loss: 0.0008, time: 0.0029\n",
            "batch: 128/979, loss: 0.0008, time: 0.0029\n",
            "batch: 129/979, loss: 0.0008, time: 0.0046\n",
            "batch: 130/979, loss: 0.0008, time: 0.0036\n",
            "batch: 131/979, loss: 0.0008, time: 0.0035\n",
            "batch: 132/979, loss: 0.0008, time: 0.0032\n",
            "batch: 133/979, loss: 0.0008, time: 0.0028\n",
            "batch: 134/979, loss: 0.0008, time: 0.0028\n",
            "batch: 135/979, loss: 0.0008, time: 0.0029\n",
            "batch: 136/979, loss: 0.0008, time: 0.0041\n",
            "batch: 137/979, loss: 0.0008, time: 0.0040\n",
            "batch: 138/979, loss: 0.0008, time: 0.0045\n",
            "batch: 139/979, loss: 0.0008, time: 0.0030\n",
            "batch: 140/979, loss: 0.0008, time: 0.0029\n",
            "batch: 141/979, loss: 0.0008, time: 0.0029\n",
            "batch: 142/979, loss: 0.0008, time: 0.0030\n",
            "batch: 143/979, loss: 0.0008, time: 0.0028\n",
            "batch: 144/979, loss: 0.0008, time: 0.0029\n",
            "batch: 145/979, loss: 0.0008, time: 0.0028\n",
            "batch: 146/979, loss: 0.0008, time: 0.0028\n",
            "batch: 147/979, loss: 0.0008, time: 0.0028\n",
            "batch: 148/979, loss: 0.0008, time: 0.0043\n",
            "batch: 149/979, loss: 0.0008, time: 0.0030\n",
            "batch: 150/979, loss: 0.0008, time: 0.0030\n",
            "batch: 151/979, loss: 0.0008, time: 0.0029\n",
            "batch: 152/979, loss: 0.0008, time: 0.0041\n",
            "batch: 153/979, loss: 0.0008, time: 0.0029\n",
            "batch: 154/979, loss: 0.0008, time: 0.0029\n",
            "batch: 155/979, loss: 0.0008, time: 0.0028\n",
            "batch: 156/979, loss: 0.0008, time: 0.0028\n",
            "batch: 157/979, loss: 0.0008, time: 0.0028\n",
            "batch: 158/979, loss: 0.0008, time: 0.0029\n",
            "batch: 159/979, loss: 0.0008, time: 0.0028\n",
            "batch: 160/979, loss: 0.0008, time: 0.0037\n",
            "batch: 161/979, loss: 0.0009, time: 0.0039\n",
            "batch: 162/979, loss: 0.0009, time: 0.0029\n",
            "batch: 163/979, loss: 0.0009, time: 0.0032\n",
            "batch: 164/979, loss: 0.0009, time: 0.0029\n",
            "batch: 165/979, loss: 0.0009, time: 0.0030\n",
            "batch: 166/979, loss: 0.0009, time: 0.0028\n",
            "batch: 167/979, loss: 0.0009, time: 0.0028\n",
            "batch: 168/979, loss: 0.0009, time: 0.0028\n",
            "batch: 169/979, loss: 0.0009, time: 0.0030\n",
            "batch: 170/979, loss: 0.0009, time: 0.0028\n",
            "batch: 171/979, loss: 0.0009, time: 0.0028\n",
            "batch: 172/979, loss: 0.0009, time: 0.0031\n",
            "batch: 173/979, loss: 0.0009, time: 0.0028\n",
            "batch: 174/979, loss: 0.0009, time: 0.0028\n",
            "batch: 175/979, loss: 0.0009, time: 0.0028\n",
            "batch: 176/979, loss: 0.0009, time: 0.0029\n",
            "batch: 177/979, loss: 0.0008, time: 0.0028\n",
            "batch: 178/979, loss: 0.0008, time: 0.0028\n",
            "batch: 179/979, loss: 0.0008, time: 0.0030\n",
            "batch: 180/979, loss: 0.0008, time: 0.0028\n",
            "batch: 181/979, loss: 0.0008, time: 0.0028\n",
            "batch: 182/979, loss: 0.0008, time: 0.0028\n",
            "batch: 183/979, loss: 0.0008, time: 0.0029\n",
            "batch: 184/979, loss: 0.0008, time: 0.0028\n",
            "batch: 185/979, loss: 0.0008, time: 0.0028\n",
            "batch: 186/979, loss: 0.0008, time: 0.0028\n",
            "batch: 187/979, loss: 0.0008, time: 0.0028\n",
            "batch: 188/979, loss: 0.0008, time: 0.0028\n",
            "batch: 189/979, loss: 0.0009, time: 0.0028\n",
            "batch: 190/979, loss: 0.0009, time: 0.0029\n",
            "batch: 191/979, loss: 0.0009, time: 0.0029\n",
            "batch: 192/979, loss: 0.0009, time: 0.0029\n",
            "batch: 193/979, loss: 0.0009, time: 0.0042\n",
            "batch: 194/979, loss: 0.0009, time: 0.0036\n",
            "batch: 195/979, loss: 0.0009, time: 0.0029\n",
            "batch: 196/979, loss: 0.0009, time: 0.0028\n",
            "batch: 197/979, loss: 0.0009, time: 0.0028\n",
            "batch: 198/979, loss: 0.0009, time: 0.0028\n",
            "batch: 199/979, loss: 0.0009, time: 0.0028\n",
            "batch: 200/979, loss: 0.0009, time: 0.0029\n",
            "batch: 201/979, loss: 0.0009, time: 0.0030\n",
            "batch: 202/979, loss: 0.0009, time: 0.0029\n",
            "batch: 203/979, loss: 0.0009, time: 0.0028\n",
            "batch: 204/979, loss: 0.0009, time: 0.0028\n",
            "batch: 205/979, loss: 0.0009, time: 0.0030\n",
            "batch: 206/979, loss: 0.0009, time: 0.0028\n",
            "batch: 207/979, loss: 0.0009, time: 0.0028\n",
            "batch: 208/979, loss: 0.0009, time: 0.0029\n",
            "batch: 209/979, loss: 0.0009, time: 0.0028\n",
            "batch: 210/979, loss: 0.0009, time: 0.0028\n",
            "batch: 211/979, loss: 0.0009, time: 0.0028\n",
            "batch: 212/979, loss: 0.0009, time: 0.0028\n",
            "batch: 213/979, loss: 0.0009, time: 0.0028\n",
            "batch: 214/979, loss: 0.0009, time: 0.0028\n",
            "batch: 215/979, loss: 0.0009, time: 0.0029\n",
            "batch: 216/979, loss: 0.0009, time: 0.0028\n",
            "batch: 217/979, loss: 0.0009, time: 0.0028\n",
            "batch: 218/979, loss: 0.0009, time: 0.0029\n",
            "batch: 219/979, loss: 0.0009, time: 0.0041\n",
            "batch: 220/979, loss: 0.0009, time: 0.0029\n",
            "batch: 221/979, loss: 0.0009, time: 0.0029\n",
            "batch: 222/979, loss: 0.0009, time: 0.0028\n",
            "batch: 223/979, loss: 0.0009, time: 0.0028\n",
            "batch: 224/979, loss: 0.0009, time: 0.0036\n",
            "batch: 225/979, loss: 0.0009, time: 0.0053\n",
            "batch: 226/979, loss: 0.0009, time: 0.0029\n",
            "batch: 227/979, loss: 0.0009, time: 0.0028\n",
            "batch: 228/979, loss: 0.0009, time: 0.0028\n",
            "batch: 229/979, loss: 0.0009, time: 0.0028\n",
            "batch: 230/979, loss: 0.0009, time: 0.0028\n",
            "batch: 231/979, loss: 0.0009, time: 0.0029\n",
            "batch: 232/979, loss: 0.0009, time: 0.0029\n",
            "batch: 233/979, loss: 0.0009, time: 0.0028\n",
            "batch: 234/979, loss: 0.0009, time: 0.0028\n",
            "batch: 235/979, loss: 0.0009, time: 0.0029\n",
            "batch: 236/979, loss: 0.0009, time: 0.0028\n",
            "batch: 237/979, loss: 0.0009, time: 0.0029\n",
            "batch: 238/979, loss: 0.0009, time: 0.0028\n",
            "batch: 239/979, loss: 0.0009, time: 0.0028\n",
            "batch: 240/979, loss: 0.0009, time: 0.0033\n",
            "batch: 241/979, loss: 0.0009, time: 0.0029\n",
            "batch: 242/979, loss: 0.0009, time: 0.0029\n",
            "batch: 243/979, loss: 0.0010, time: 0.0029\n",
            "batch: 244/979, loss: 0.0010, time: 0.0033\n",
            "batch: 245/979, loss: 0.0010, time: 0.0037\n",
            "batch: 246/979, loss: 0.0009, time: 0.0038\n",
            "batch: 247/979, loss: 0.0009, time: 0.0029\n",
            "batch: 248/979, loss: 0.0009, time: 0.0028\n",
            "batch: 249/979, loss: 0.0009, time: 0.0028\n",
            "batch: 250/979, loss: 0.0009, time: 0.0029\n",
            "batch: 251/979, loss: 0.0009, time: 0.0029\n",
            "batch: 252/979, loss: 0.0009, time: 0.0028\n",
            "batch: 253/979, loss: 0.0009, time: 0.0028\n",
            "batch: 254/979, loss: 0.0009, time: 0.0029\n",
            "batch: 255/979, loss: 0.0009, time: 0.0029\n",
            "batch: 256/979, loss: 0.0017, time: 0.0036\n",
            "batch: 257/979, loss: 0.0021, time: 0.0057\n",
            "batch: 258/979, loss: 0.0021, time: 0.0029\n",
            "batch: 259/979, loss: 0.0021, time: 0.0030\n",
            "batch: 260/979, loss: 0.0021, time: 0.0028\n",
            "batch: 261/979, loss: 0.0021, time: 0.0028\n",
            "batch: 262/979, loss: 0.0021, time: 0.0028\n",
            "batch: 263/979, loss: 0.0020, time: 0.0028\n",
            "batch: 264/979, loss: 0.0020, time: 0.0029\n",
            "batch: 265/979, loss: 0.0020, time: 0.0028\n",
            "batch: 266/979, loss: 0.0020, time: 0.0028\n",
            "batch: 267/979, loss: 0.0021, time: 0.0028\n",
            "batch: 268/979, loss: 0.0021, time: 0.0028\n",
            "batch: 269/979, loss: 0.0021, time: 0.0028\n",
            "batch: 270/979, loss: 0.0021, time: 0.0028\n",
            "batch: 271/979, loss: 0.0023, time: 0.0028\n",
            "batch: 272/979, loss: 0.0023, time: 0.0027\n",
            "batch: 273/979, loss: 0.0023, time: 0.0027\n",
            "batch: 274/979, loss: 0.0022, time: 0.0027\n",
            "batch: 275/979, loss: 0.0022, time: 0.0028\n",
            "batch: 276/979, loss: 0.0022, time: 0.0028\n",
            "batch: 277/979, loss: 0.0022, time: 0.0028\n",
            "batch: 278/979, loss: 0.0022, time: 0.0028\n",
            "batch: 279/979, loss: 0.0022, time: 0.0028\n",
            "batch: 280/979, loss: 0.0022, time: 0.0028\n",
            "batch: 281/979, loss: 0.0022, time: 0.0028\n",
            "batch: 282/979, loss: 0.0022, time: 0.0027\n",
            "batch: 283/979, loss: 0.0022, time: 0.0027\n",
            "batch: 284/979, loss: 0.0022, time: 0.0046\n",
            "batch: 285/979, loss: 0.0022, time: 0.0028\n",
            "batch: 286/979, loss: 0.0022, time: 0.0028\n",
            "batch: 287/979, loss: 0.0022, time: 0.0028\n",
            "batch: 288/979, loss: 0.0022, time: 0.0028\n",
            "batch: 289/979, loss: 0.0022, time: 0.0060\n",
            "batch: 290/979, loss: 0.0022, time: 0.0035\n",
            "batch: 291/979, loss: 0.0022, time: 0.0029\n",
            "batch: 292/979, loss: 0.0022, time: 0.0028\n",
            "batch: 293/979, loss: 0.0021, time: 0.0029\n",
            "batch: 294/979, loss: 0.0021, time: 0.0028\n",
            "batch: 295/979, loss: 0.0021, time: 0.0029\n",
            "batch: 296/979, loss: 0.0021, time: 0.0042\n",
            "batch: 297/979, loss: 0.0021, time: 0.0042\n",
            "batch: 298/979, loss: 0.0021, time: 0.0036\n",
            "batch: 299/979, loss: 0.0021, time: 0.0028\n",
            "batch: 300/979, loss: 0.0021, time: 0.0028\n",
            "batch: 301/979, loss: 0.0021, time: 0.0028\n",
            "batch: 302/979, loss: 0.0021, time: 0.0028\n",
            "batch: 303/979, loss: 0.0021, time: 0.0028\n",
            "batch: 304/979, loss: 0.0021, time: 0.0028\n",
            "batch: 305/979, loss: 0.0021, time: 0.0028\n",
            "batch: 306/979, loss: 0.0021, time: 0.0029\n",
            "batch: 307/979, loss: 0.0021, time: 0.0035\n",
            "batch: 308/979, loss: 0.0021, time: 0.0028\n",
            "batch: 309/979, loss: 0.0021, time: 0.0037\n",
            "batch: 310/979, loss: 0.0021, time: 0.0028\n",
            "batch: 311/979, loss: 0.0021, time: 0.0028\n",
            "batch: 312/979, loss: 0.0021, time: 0.0028\n",
            "batch: 313/979, loss: 0.0021, time: 0.0028\n",
            "batch: 314/979, loss: 0.0021, time: 0.0027\n",
            "batch: 315/979, loss: 0.0021, time: 0.0027\n",
            "batch: 316/979, loss: 0.0021, time: 0.0027\n",
            "batch: 317/979, loss: 0.0021, time: 0.0028\n",
            "batch: 318/979, loss: 0.0021, time: 0.0028\n",
            "batch: 319/979, loss: 0.0021, time: 0.0028\n",
            "batch: 320/979, loss: 0.0021, time: 0.0028\n",
            "batch: 321/979, loss: 0.0021, time: 0.0047\n",
            "batch: 322/979, loss: 0.0021, time: 0.0034\n",
            "batch: 323/979, loss: 0.0021, time: 0.0028\n",
            "batch: 324/979, loss: 0.0021, time: 0.0028\n",
            "batch: 325/979, loss: 0.0021, time: 0.0029\n",
            "batch: 326/979, loss: 0.0021, time: 0.0028\n",
            "batch: 327/979, loss: 0.0021, time: 0.0028\n",
            "batch: 328/979, loss: 0.0021, time: 0.0038\n",
            "batch: 329/979, loss: 0.0021, time: 0.0029\n",
            "batch: 330/979, loss: 0.0021, time: 0.0028\n",
            "batch: 331/979, loss: 0.0020, time: 0.0028\n",
            "batch: 332/979, loss: 0.0020, time: 0.0029\n",
            "batch: 333/979, loss: 0.0020, time: 0.0030\n",
            "batch: 334/979, loss: 0.0020, time: 0.0028\n",
            "batch: 335/979, loss: 0.0020, time: 0.0028\n",
            "batch: 336/979, loss: 0.0020, time: 0.0028\n",
            "batch: 337/979, loss: 0.0020, time: 0.0028\n",
            "batch: 338/979, loss: 0.0020, time: 0.0027\n",
            "batch: 339/979, loss: 0.0020, time: 0.0027\n",
            "batch: 340/979, loss: 0.0020, time: 0.0028\n",
            "batch: 341/979, loss: 0.0020, time: 0.0028\n",
            "batch: 342/979, loss: 0.0020, time: 0.0028\n",
            "batch: 343/979, loss: 0.0020, time: 0.0028\n",
            "batch: 344/979, loss: 0.0020, time: 0.0028\n",
            "batch: 345/979, loss: 0.0020, time: 0.0028\n",
            "batch: 346/979, loss: 0.0020, time: 0.0029\n",
            "batch: 347/979, loss: 0.0020, time: 0.0028\n",
            "batch: 348/979, loss: 0.0020, time: 0.0048\n",
            "batch: 349/979, loss: 0.0020, time: 0.0029\n",
            "batch: 350/979, loss: 0.0020, time: 0.0027\n",
            "batch: 351/979, loss: 0.0020, time: 0.0028\n",
            "batch: 352/979, loss: 0.0020, time: 0.0028\n",
            "batch: 353/979, loss: 0.0020, time: 0.0028\n",
            "batch: 354/979, loss: 0.0020, time: 0.0039\n",
            "batch: 355/979, loss: 0.0020, time: 0.0031\n",
            "batch: 356/979, loss: 0.0020, time: 0.0029\n",
            "batch: 357/979, loss: 0.0020, time: 0.0030\n",
            "batch: 358/979, loss: 0.0020, time: 0.0028\n",
            "batch: 359/979, loss: 0.0020, time: 0.0028\n",
            "batch: 360/979, loss: 0.0020, time: 0.0028\n",
            "batch: 361/979, loss: 0.0020, time: 0.0027\n",
            "batch: 362/979, loss: 0.0020, time: 0.0028\n",
            "batch: 363/979, loss: 0.0020, time: 0.0028\n",
            "batch: 364/979, loss: 0.0020, time: 0.0028\n",
            "batch: 365/979, loss: 0.0020, time: 0.0028\n",
            "batch: 366/979, loss: 0.0020, time: 0.0027\n",
            "batch: 367/979, loss: 0.0020, time: 0.0027\n",
            "batch: 368/979, loss: 0.0019, time: 0.0027\n",
            "batch: 369/979, loss: 0.0019, time: 0.0028\n",
            "batch: 370/979, loss: 0.0019, time: 0.0028\n",
            "batch: 371/979, loss: 0.0019, time: 0.0029\n",
            "batch: 372/979, loss: 0.0019, time: 0.0028\n",
            "batch: 373/979, loss: 0.0019, time: 0.0028\n",
            "batch: 374/979, loss: 0.0019, time: 0.0027\n",
            "batch: 375/979, loss: 0.0019, time: 0.0027\n",
            "batch: 376/979, loss: 0.0019, time: 0.0028\n",
            "batch: 377/979, loss: 0.0019, time: 0.0028\n",
            "batch: 378/979, loss: 0.0019, time: 0.0028\n",
            "batch: 379/979, loss: 0.0019, time: 0.0028\n",
            "batch: 380/979, loss: 0.0019, time: 0.0028\n",
            "batch: 381/979, loss: 0.0019, time: 0.0027\n",
            "batch: 382/979, loss: 0.0019, time: 0.0027\n",
            "batch: 383/979, loss: 0.0019, time: 0.0028\n",
            "batch: 384/979, loss: 0.0019, time: 0.0028\n",
            "batch: 385/979, loss: 0.0019, time: 0.0028\n",
            "batch: 386/979, loss: 0.0019, time: 0.0048\n",
            "batch: 387/979, loss: 0.0019, time: 0.0035\n",
            "batch: 388/979, loss: 0.0019, time: 0.0028\n",
            "batch: 389/979, loss: 0.0019, time: 0.0027\n",
            "batch: 390/979, loss: 0.0019, time: 0.0028\n",
            "batch: 391/979, loss: 0.0019, time: 0.0027\n",
            "batch: 392/979, loss: 0.0019, time: 0.0030\n",
            "batch: 393/979, loss: 0.0019, time: 0.0028\n",
            "batch: 394/979, loss: 0.0019, time: 0.0028\n",
            "batch: 395/979, loss: 0.0019, time: 0.0028\n",
            "batch: 396/979, loss: 0.0019, time: 0.0028\n",
            "batch: 397/979, loss: 0.0019, time: 0.0027\n",
            "batch: 398/979, loss: 0.0018, time: 0.0027\n",
            "batch: 399/979, loss: 0.0018, time: 0.0027\n",
            "batch: 400/979, loss: 0.0018, time: 0.0028\n",
            "batch: 401/979, loss: 0.0018, time: 0.0032\n",
            "batch: 402/979, loss: 0.0018, time: 0.0036\n",
            "batch: 403/979, loss: 0.0019, time: 0.0036\n",
            "batch: 404/979, loss: 0.0019, time: 0.0028\n",
            "batch: 405/979, loss: 0.0019, time: 0.0028\n",
            "batch: 406/979, loss: 0.0019, time: 0.0028\n",
            "batch: 407/979, loss: 0.0019, time: 0.0028\n",
            "batch: 408/979, loss: 0.0019, time: 0.0028\n",
            "batch: 409/979, loss: 0.0019, time: 0.0027\n",
            "batch: 410/979, loss: 0.0019, time: 0.0027\n",
            "batch: 411/979, loss: 0.0019, time: 0.0028\n",
            "batch: 412/979, loss: 0.0019, time: 0.0028\n",
            "batch: 413/979, loss: 0.0019, time: 0.0028\n",
            "batch: 414/979, loss: 0.0019, time: 0.0028\n",
            "batch: 415/979, loss: 0.0019, time: 0.0039\n",
            "batch: 416/979, loss: 0.0019, time: 0.0028\n",
            "batch: 417/979, loss: 0.0019, time: 0.0028\n",
            "batch: 418/979, loss: 0.0019, time: 0.0028\n",
            "batch: 419/979, loss: 0.0019, time: 0.0040\n",
            "batch: 420/979, loss: 0.0019, time: 0.0030\n",
            "batch: 421/979, loss: 0.0019, time: 0.0029\n",
            "batch: 422/979, loss: 0.0019, time: 0.0029\n",
            "batch: 423/979, loss: 0.0019, time: 0.0029\n",
            "batch: 424/979, loss: 0.0019, time: 0.0028\n",
            "batch: 425/979, loss: 0.0019, time: 0.0029\n",
            "batch: 426/979, loss: 0.0019, time: 0.0029\n",
            "batch: 427/979, loss: 0.0019, time: 0.0029\n",
            "batch: 428/979, loss: 0.0019, time: 0.0028\n",
            "batch: 429/979, loss: 0.0019, time: 0.0028\n",
            "batch: 430/979, loss: 0.0018, time: 0.0028\n",
            "batch: 431/979, loss: 0.0018, time: 0.0029\n",
            "batch: 432/979, loss: 0.0018, time: 0.0028\n",
            "batch: 433/979, loss: 0.0018, time: 0.0028\n",
            "batch: 434/979, loss: 0.0018, time: 0.0028\n",
            "batch: 435/979, loss: 0.0018, time: 0.0028\n",
            "batch: 436/979, loss: 0.0018, time: 0.0029\n",
            "batch: 437/979, loss: 0.0018, time: 0.0028\n",
            "batch: 438/979, loss: 0.0018, time: 0.0028\n",
            "batch: 439/979, loss: 0.0018, time: 0.0028\n",
            "batch: 440/979, loss: 0.0018, time: 0.0044\n",
            "batch: 441/979, loss: 0.0018, time: 0.0028\n",
            "batch: 442/979, loss: 0.0018, time: 0.0028\n",
            "batch: 443/979, loss: 0.0018, time: 0.0028\n",
            "batch: 444/979, loss: 0.0018, time: 0.0028\n",
            "batch: 445/979, loss: 0.0018, time: 0.0028\n",
            "batch: 446/979, loss: 0.0018, time: 0.0029\n",
            "batch: 447/979, loss: 0.0018, time: 0.0033\n",
            "batch: 448/979, loss: 0.0018, time: 0.0028\n",
            "batch: 449/979, loss: 0.0018, time: 0.0028\n",
            "batch: 450/979, loss: 0.0018, time: 0.0036\n",
            "batch: 451/979, loss: 0.0018, time: 0.0049\n",
            "batch: 452/979, loss: 0.0018, time: 0.0029\n",
            "batch: 453/979, loss: 0.0018, time: 0.0028\n",
            "batch: 454/979, loss: 0.0018, time: 0.0029\n",
            "batch: 455/979, loss: 0.0018, time: 0.0028\n",
            "batch: 456/979, loss: 0.0018, time: 0.0034\n",
            "batch: 457/979, loss: 0.0018, time: 0.0041\n",
            "batch: 458/979, loss: 0.0018, time: 0.0041\n",
            "batch: 459/979, loss: 0.0018, time: 0.0043\n",
            "batch: 460/979, loss: 0.0018, time: 0.0029\n",
            "batch: 461/979, loss: 0.0018, time: 0.0029\n",
            "batch: 462/979, loss: 0.0018, time: 0.0030\n",
            "batch: 463/979, loss: 0.0018, time: 0.0028\n",
            "batch: 464/979, loss: 0.0018, time: 0.0028\n",
            "batch: 465/979, loss: 0.0018, time: 0.0029\n",
            "batch: 466/979, loss: 0.0018, time: 0.0029\n",
            "batch: 467/979, loss: 0.0018, time: 0.0034\n",
            "batch: 468/979, loss: 0.0017, time: 0.0029\n",
            "batch: 469/979, loss: 0.0017, time: 0.0028\n",
            "batch: 470/979, loss: 0.0017, time: 0.0028\n",
            "batch: 471/979, loss: 0.0017, time: 0.0028\n",
            "batch: 472/979, loss: 0.0017, time: 0.0030\n",
            "batch: 473/979, loss: 0.0017, time: 0.0028\n",
            "batch: 474/979, loss: 0.0017, time: 0.0029\n",
            "batch: 475/979, loss: 0.0017, time: 0.0028\n",
            "batch: 476/979, loss: 0.0017, time: 0.0028\n",
            "batch: 477/979, loss: 0.0017, time: 0.0031\n",
            "batch: 478/979, loss: 0.0017, time: 0.0028\n",
            "batch: 479/979, loss: 0.0017, time: 0.0048\n",
            "batch: 480/979, loss: 0.0017, time: 0.0029\n",
            "batch: 481/979, loss: 0.0017, time: 0.0029\n",
            "batch: 482/979, loss: 0.0017, time: 0.0036\n",
            "batch: 483/979, loss: 0.0017, time: 0.0041\n",
            "batch: 484/979, loss: 0.0017, time: 0.0029\n",
            "batch: 485/979, loss: 0.0017, time: 0.0032\n",
            "batch: 486/979, loss: 0.0017, time: 0.0028\n",
            "batch: 487/979, loss: 0.0017, time: 0.0029\n",
            "batch: 488/979, loss: 0.0017, time: 0.0029\n",
            "batch: 489/979, loss: 0.0017, time: 0.0028\n",
            "batch: 490/979, loss: 0.0017, time: 0.0030\n",
            "batch: 491/979, loss: 0.0017, time: 0.0029\n",
            "batch: 492/979, loss: 0.0017, time: 0.0029\n",
            "batch: 493/979, loss: 0.0017, time: 0.0029\n",
            "batch: 494/979, loss: 0.0017, time: 0.0029\n",
            "batch: 495/979, loss: 0.0017, time: 0.0028\n",
            "batch: 496/979, loss: 0.0017, time: 0.0028\n",
            "batch: 497/979, loss: 0.0018, time: 0.0028\n",
            "batch: 498/979, loss: 0.0018, time: 0.0028\n",
            "batch: 499/979, loss: 0.0018, time: 0.0028\n",
            "batch: 500/979, loss: 0.0018, time: 0.0028\n",
            "batch: 501/979, loss: 0.0018, time: 0.0029\n",
            "batch: 502/979, loss: 0.0018, time: 0.0028\n",
            "batch: 503/979, loss: 0.0018, time: 0.0029\n",
            "batch: 504/979, loss: 0.0018, time: 0.0028\n",
            "batch: 505/979, loss: 0.0018, time: 0.0030\n",
            "batch: 506/979, loss: 0.0018, time: 0.0028\n",
            "batch: 507/979, loss: 0.0018, time: 0.0029\n",
            "batch: 508/979, loss: 0.0018, time: 0.0030\n",
            "batch: 509/979, loss: 0.0018, time: 0.0030\n",
            "batch: 510/979, loss: 0.0017, time: 0.0028\n",
            "batch: 511/979, loss: 0.0017, time: 0.0029\n",
            "batch: 512/979, loss: 0.0017, time: 0.0029\n",
            "batch: 513/979, loss: 0.0017, time: 0.0029\n",
            "batch: 514/979, loss: 0.0017, time: 0.0032\n",
            "batch: 515/979, loss: 0.0017, time: 0.0052\n",
            "batch: 516/979, loss: 0.0017, time: 0.0032\n",
            "batch: 517/979, loss: 0.0017, time: 0.0028\n",
            "batch: 518/979, loss: 0.0017, time: 0.0031\n",
            "batch: 519/979, loss: 0.0017, time: 0.0029\n",
            "batch: 520/979, loss: 0.0017, time: 0.0030\n",
            "batch: 521/979, loss: 0.0017, time: 0.0029\n",
            "batch: 522/979, loss: 0.0017, time: 0.0028\n",
            "batch: 523/979, loss: 0.0017, time: 0.0028\n",
            "batch: 524/979, loss: 0.0017, time: 0.0029\n",
            "batch: 525/979, loss: 0.0017, time: 0.0028\n",
            "batch: 526/979, loss: 0.0017, time: 0.0028\n",
            "batch: 527/979, loss: 0.0017, time: 0.0028\n",
            "batch: 528/979, loss: 0.0017, time: 0.0028\n",
            "batch: 529/979, loss: 0.0017, time: 0.0030\n",
            "batch: 530/979, loss: 0.0017, time: 0.0029\n",
            "batch: 531/979, loss: 0.0017, time: 0.0028\n",
            "batch: 532/979, loss: 0.0017, time: 0.0029\n",
            "batch: 533/979, loss: 0.0017, time: 0.0030\n",
            "batch: 534/979, loss: 0.0017, time: 0.0028\n",
            "batch: 535/979, loss: 0.0017, time: 0.0028\n",
            "batch: 536/979, loss: 0.0017, time: 0.0030\n",
            "batch: 537/979, loss: 0.0017, time: 0.0029\n",
            "batch: 538/979, loss: 0.0017, time: 0.0028\n",
            "batch: 539/979, loss: 0.0017, time: 0.0028\n",
            "batch: 540/979, loss: 0.0017, time: 0.0041\n",
            "batch: 541/979, loss: 0.0017, time: 0.0033\n",
            "batch: 542/979, loss: 0.0017, time: 0.0028\n",
            "batch: 543/979, loss: 0.0017, time: 0.0028\n",
            "batch: 544/979, loss: 0.0017, time: 0.0029\n",
            "batch: 545/979, loss: 0.0017, time: 0.0029\n",
            "batch: 546/979, loss: 0.0017, time: 0.0036\n",
            "batch: 547/979, loss: 0.0017, time: 0.0055\n",
            "batch: 548/979, loss: 0.0017, time: 0.0029\n",
            "batch: 549/979, loss: 0.0017, time: 0.0028\n",
            "batch: 550/979, loss: 0.0017, time: 0.0030\n",
            "batch: 551/979, loss: 0.0017, time: 0.0030\n",
            "batch: 552/979, loss: 0.0017, time: 0.0029\n",
            "batch: 553/979, loss: 0.0017, time: 0.0029\n",
            "batch: 554/979, loss: 0.0017, time: 0.0029\n",
            "batch: 555/979, loss: 0.0017, time: 0.0029\n",
            "batch: 556/979, loss: 0.0017, time: 0.0030\n",
            "batch: 557/979, loss: 0.0017, time: 0.0038\n",
            "batch: 558/979, loss: 0.0017, time: 0.0037\n",
            "batch: 559/979, loss: 0.0017, time: 0.0035\n",
            "batch: 560/979, loss: 0.0017, time: 0.0029\n",
            "batch: 561/979, loss: 0.0017, time: 0.0031\n",
            "batch: 562/979, loss: 0.0017, time: 0.0029\n",
            "batch: 563/979, loss: 0.0017, time: 0.0029\n",
            "batch: 564/979, loss: 0.0017, time: 0.0028\n",
            "batch: 565/979, loss: 0.0017, time: 0.0028\n",
            "batch: 566/979, loss: 0.0017, time: 0.0029\n",
            "batch: 567/979, loss: 0.0017, time: 0.0029\n",
            "batch: 568/979, loss: 0.0017, time: 0.0032\n",
            "batch: 569/979, loss: 0.0017, time: 0.0031\n",
            "batch: 570/979, loss: 0.0017, time: 0.0029\n",
            "batch: 571/979, loss: 0.0017, time: 0.0029\n",
            "batch: 572/979, loss: 0.0017, time: 0.0029\n",
            "batch: 573/979, loss: 0.0017, time: 0.0029\n",
            "batch: 574/979, loss: 0.0017, time: 0.0028\n",
            "batch: 575/979, loss: 0.0017, time: 0.0028\n",
            "batch: 576/979, loss: 0.0017, time: 0.0029\n",
            "batch: 577/979, loss: 0.0017, time: 0.0028\n",
            "batch: 578/979, loss: 0.0016, time: 0.0042\n",
            "batch: 579/979, loss: 0.0016, time: 0.0040\n",
            "batch: 580/979, loss: 0.0016, time: 0.0029\n",
            "batch: 581/979, loss: 0.0016, time: 0.0029\n",
            "batch: 582/979, loss: 0.0017, time: 0.0030\n",
            "batch: 583/979, loss: 0.0017, time: 0.0029\n",
            "batch: 584/979, loss: 0.0017, time: 0.0029\n",
            "batch: 585/979, loss: 0.0017, time: 0.0030\n",
            "batch: 586/979, loss: 0.0017, time: 0.0034\n",
            "batch: 587/979, loss: 0.0017, time: 0.0029\n",
            "batch: 588/979, loss: 0.0017, time: 0.0030\n",
            "batch: 589/979, loss: 0.0017, time: 0.0029\n",
            "batch: 590/979, loss: 0.0017, time: 0.0030\n",
            "batch: 591/979, loss: 0.0017, time: 0.0029\n",
            "batch: 592/979, loss: 0.0017, time: 0.0028\n",
            "batch: 593/979, loss: 0.0017, time: 0.0029\n",
            "batch: 594/979, loss: 0.0017, time: 0.0028\n",
            "batch: 595/979, loss: 0.0017, time: 0.0028\n",
            "batch: 596/979, loss: 0.0017, time: 0.0028\n",
            "batch: 597/979, loss: 0.0018, time: 0.0028\n",
            "batch: 598/979, loss: 0.0018, time: 0.0028\n",
            "batch: 599/979, loss: 0.0018, time: 0.0029\n",
            "batch: 600/979, loss: 0.0018, time: 0.0029\n",
            "batch: 601/979, loss: 0.0018, time: 0.0028\n",
            "batch: 602/979, loss: 0.0018, time: 0.0028\n",
            "batch: 603/979, loss: 0.0018, time: 0.0030\n",
            "batch: 604/979, loss: 0.0018, time: 0.0030\n",
            "batch: 605/979, loss: 0.0018, time: 0.0029\n",
            "batch: 606/979, loss: 0.0018, time: 0.0028\n",
            "batch: 607/979, loss: 0.0018, time: 0.0029\n",
            "batch: 608/979, loss: 0.0018, time: 0.0028\n",
            "batch: 609/979, loss: 0.0018, time: 0.0028\n",
            "batch: 610/979, loss: 0.0017, time: 0.0028\n",
            "batch: 611/979, loss: 0.0017, time: 0.0060\n",
            "batch: 612/979, loss: 0.0017, time: 0.0030\n",
            "batch: 613/979, loss: 0.0017, time: 0.0029\n",
            "batch: 614/979, loss: 0.0017, time: 0.0028\n",
            "batch: 615/979, loss: 0.0017, time: 0.0031\n",
            "batch: 616/979, loss: 0.0017, time: 0.0041\n",
            "batch: 617/979, loss: 0.0017, time: 0.0042\n",
            "batch: 618/979, loss: 0.0017, time: 0.0043\n",
            "batch: 619/979, loss: 0.0017, time: 0.0028\n",
            "batch: 620/979, loss: 0.0017, time: 0.0037\n",
            "batch: 621/979, loss: 0.0017, time: 0.0029\n",
            "batch: 622/979, loss: 0.0017, time: 0.0035\n",
            "batch: 623/979, loss: 0.0017, time: 0.0029\n",
            "batch: 624/979, loss: 0.0017, time: 0.0028\n",
            "batch: 625/979, loss: 0.0017, time: 0.0028\n",
            "batch: 626/979, loss: 0.0017, time: 0.0031\n",
            "batch: 627/979, loss: 0.0017, time: 0.0028\n",
            "batch: 628/979, loss: 0.0017, time: 0.0028\n",
            "batch: 629/979, loss: 0.0017, time: 0.0029\n",
            "batch: 630/979, loss: 0.0017, time: 0.0028\n",
            "batch: 631/979, loss: 0.0017, time: 0.0029\n",
            "batch: 632/979, loss: 0.0017, time: 0.0029\n",
            "batch: 633/979, loss: 0.0017, time: 0.0029\n",
            "batch: 634/979, loss: 0.0017, time: 0.0028\n",
            "batch: 635/979, loss: 0.0017, time: 0.0028\n",
            "batch: 636/979, loss: 0.0017, time: 0.0028\n",
            "batch: 637/979, loss: 0.0017, time: 0.0028\n",
            "batch: 638/979, loss: 0.0017, time: 0.0029\n",
            "batch: 639/979, loss: 0.0017, time: 0.0028\n",
            "batch: 640/979, loss: 0.0017, time: 0.0029\n",
            "batch: 641/979, loss: 0.0017, time: 0.0028\n",
            "batch: 642/979, loss: 0.0017, time: 0.0051\n",
            "batch: 643/979, loss: 0.0017, time: 0.0029\n",
            "batch: 644/979, loss: 0.0017, time: 0.0030\n",
            "batch: 645/979, loss: 0.0017, time: 0.0028\n",
            "batch: 646/979, loss: 0.0017, time: 0.0028\n",
            "batch: 647/979, loss: 0.0017, time: 0.0028\n",
            "batch: 648/979, loss: 0.0017, time: 0.0028\n",
            "batch: 649/979, loss: 0.0017, time: 0.0028\n",
            "batch: 650/979, loss: 0.0017, time: 0.0028\n",
            "batch: 651/979, loss: 0.0017, time: 0.0028\n",
            "batch: 652/979, loss: 0.0017, time: 0.0028\n",
            "batch: 653/979, loss: 0.0017, time: 0.0029\n",
            "batch: 654/979, loss: 0.0017, time: 0.0028\n",
            "batch: 655/979, loss: 0.0017, time: 0.0028\n",
            "batch: 656/979, loss: 0.0017, time: 0.0029\n",
            "batch: 657/979, loss: 0.0017, time: 0.0028\n",
            "batch: 658/979, loss: 0.0017, time: 0.0029\n",
            "batch: 659/979, loss: 0.0017, time: 0.0030\n",
            "batch: 660/979, loss: 0.0017, time: 0.0028\n",
            "batch: 661/979, loss: 0.0017, time: 0.0028\n",
            "batch: 662/979, loss: 0.0017, time: 0.0028\n",
            "batch: 663/979, loss: 0.0017, time: 0.0028\n",
            "batch: 664/979, loss: 0.0017, time: 0.0028\n",
            "batch: 665/979, loss: 0.0017, time: 0.0028\n",
            "batch: 666/979, loss: 0.0017, time: 0.0028\n",
            "batch: 667/979, loss: 0.0017, time: 0.0028\n",
            "batch: 668/979, loss: 0.0017, time: 0.0028\n",
            "batch: 669/979, loss: 0.0017, time: 0.0031\n",
            "batch: 670/979, loss: 0.0017, time: 0.0030\n",
            "batch: 671/979, loss: 0.0017, time: 0.0060\n",
            "batch: 672/979, loss: 0.0017, time: 0.0030\n",
            "batch: 673/979, loss: 0.0017, time: 0.0030\n",
            "batch: 674/979, loss: 0.0017, time: 0.0029\n",
            "batch: 675/979, loss: 0.0017, time: 0.0044\n",
            "batch: 676/979, loss: 0.0017, time: 0.0030\n",
            "batch: 677/979, loss: 0.0017, time: 0.0029\n",
            "batch: 678/979, loss: 0.0017, time: 0.0029\n",
            "batch: 679/979, loss: 0.0017, time: 0.0028\n",
            "batch: 680/979, loss: 0.0017, time: 0.0030\n",
            "batch: 681/979, loss: 0.0017, time: 0.0029\n",
            "batch: 682/979, loss: 0.0017, time: 0.0029\n",
            "batch: 683/979, loss: 0.0017, time: 0.0028\n",
            "batch: 684/979, loss: 0.0016, time: 0.0037\n",
            "batch: 685/979, loss: 0.0016, time: 0.0029\n",
            "batch: 686/979, loss: 0.0016, time: 0.0028\n",
            "batch: 687/979, loss: 0.0016, time: 0.0029\n",
            "batch: 688/979, loss: 0.0018, time: 0.0028\n",
            "batch: 689/979, loss: 0.0018, time: 0.0029\n",
            "batch: 690/979, loss: 0.0018, time: 0.0028\n",
            "batch: 691/979, loss: 0.0018, time: 0.0029\n",
            "batch: 692/979, loss: 0.0018, time: 0.0029\n",
            "batch: 693/979, loss: 0.0018, time: 0.0028\n",
            "batch: 694/979, loss: 0.0018, time: 0.0029\n",
            "batch: 695/979, loss: 0.0018, time: 0.0028\n",
            "batch: 696/979, loss: 0.0018, time: 0.0028\n",
            "batch: 697/979, loss: 0.0018, time: 0.0029\n",
            "batch: 698/979, loss: 0.0018, time: 0.0028\n",
            "batch: 699/979, loss: 0.0018, time: 0.0031\n",
            "batch: 700/979, loss: 0.0018, time: 0.0029\n",
            "batch: 701/979, loss: 0.0018, time: 0.0029\n",
            "batch: 702/979, loss: 0.0018, time: 0.0031\n",
            "batch: 703/979, loss: 0.0018, time: 0.0029\n",
            "batch: 704/979, loss: 0.0018, time: 0.0028\n",
            "batch: 705/979, loss: 0.0020, time: 0.0029\n",
            "batch: 706/979, loss: 0.0020, time: 0.0036\n",
            "batch: 707/979, loss: 0.0020, time: 0.0056\n",
            "batch: 708/979, loss: 0.0020, time: 0.0030\n",
            "batch: 709/979, loss: 0.0020, time: 0.0028\n",
            "batch: 710/979, loss: 0.0019, time: 0.0028\n",
            "batch: 711/979, loss: 0.0019, time: 0.0029\n",
            "batch: 712/979, loss: 0.0019, time: 0.0036\n",
            "batch: 713/979, loss: 0.0019, time: 0.0038\n",
            "batch: 714/979, loss: 0.0019, time: 0.0036\n",
            "batch: 715/979, loss: 0.0019, time: 0.0029\n",
            "batch: 716/979, loss: 0.0019, time: 0.0030\n",
            "batch: 717/979, loss: 0.0019, time: 0.0028\n",
            "batch: 718/979, loss: 0.0019, time: 0.0029\n",
            "batch: 719/979, loss: 0.0019, time: 0.0028\n",
            "batch: 720/979, loss: 0.0019, time: 0.0029\n",
            "batch: 721/979, loss: 0.0019, time: 0.0029\n",
            "batch: 722/979, loss: 0.0019, time: 0.0028\n",
            "batch: 723/979, loss: 0.0019, time: 0.0028\n",
            "batch: 724/979, loss: 0.0019, time: 0.0029\n",
            "batch: 725/979, loss: 0.0019, time: 0.0028\n",
            "batch: 726/979, loss: 0.0019, time: 0.0030\n",
            "batch: 727/979, loss: 0.0019, time: 0.0030\n",
            "batch: 728/979, loss: 0.0019, time: 0.0029\n",
            "batch: 729/979, loss: 0.0019, time: 0.0028\n",
            "batch: 730/979, loss: 0.0019, time: 0.0029\n",
            "batch: 731/979, loss: 0.0019, time: 0.0029\n",
            "batch: 732/979, loss: 0.0019, time: 0.0028\n",
            "batch: 733/979, loss: 0.0019, time: 0.0028\n",
            "batch: 734/979, loss: 0.0019, time: 0.0028\n",
            "batch: 735/979, loss: 0.0019, time: 0.0028\n",
            "batch: 736/979, loss: 0.0019, time: 0.0029\n",
            "batch: 737/979, loss: 0.0019, time: 0.0047\n",
            "batch: 738/979, loss: 0.0019, time: 0.0053\n",
            "batch: 739/979, loss: 0.0019, time: 0.0047\n",
            "batch: 740/979, loss: 0.0019, time: 0.0034\n",
            "batch: 741/979, loss: 0.0019, time: 0.0030\n",
            "batch: 742/979, loss: 0.0019, time: 0.0028\n",
            "batch: 743/979, loss: 0.0019, time: 0.0029\n",
            "batch: 744/979, loss: 0.0019, time: 0.0028\n",
            "batch: 745/979, loss: 0.0019, time: 0.0028\n",
            "batch: 746/979, loss: 0.0019, time: 0.0028\n",
            "batch: 747/979, loss: 0.0019, time: 0.0028\n",
            "batch: 748/979, loss: 0.0019, time: 0.0029\n",
            "batch: 749/979, loss: 0.0019, time: 0.0029\n",
            "batch: 750/979, loss: 0.0019, time: 0.0028\n",
            "batch: 751/979, loss: 0.0019, time: 0.0029\n",
            "batch: 752/979, loss: 0.0019, time: 0.0032\n",
            "batch: 753/979, loss: 0.0019, time: 0.0028\n",
            "batch: 754/979, loss: 0.0019, time: 0.0029\n",
            "batch: 755/979, loss: 0.0019, time: 0.0029\n",
            "batch: 756/979, loss: 0.0019, time: 0.0032\n",
            "batch: 757/979, loss: 0.0019, time: 0.0028\n",
            "batch: 758/979, loss: 0.0019, time: 0.0028\n",
            "batch: 759/979, loss: 0.0019, time: 0.0028\n",
            "batch: 760/979, loss: 0.0019, time: 0.0028\n",
            "batch: 761/979, loss: 0.0019, time: 0.0028\n",
            "batch: 762/979, loss: 0.0019, time: 0.0029\n",
            "batch: 763/979, loss: 0.0019, time: 0.0028\n",
            "batch: 764/979, loss: 0.0019, time: 0.0028\n",
            "batch: 765/979, loss: 0.0019, time: 0.0028\n",
            "batch: 766/979, loss: 0.0019, time: 0.0028\n",
            "batch: 767/979, loss: 0.0019, time: 0.0030\n",
            "batch: 768/979, loss: 0.0019, time: 0.0028\n",
            "batch: 769/979, loss: 0.0020, time: 0.0028\n",
            "batch: 770/979, loss: 0.0020, time: 0.0029\n",
            "batch: 771/979, loss: 0.0020, time: 0.0046\n",
            "batch: 772/979, loss: 0.0020, time: 0.0030\n",
            "batch: 773/979, loss: 0.0020, time: 0.0028\n",
            "batch: 774/979, loss: 0.0020, time: 0.0035\n",
            "batch: 775/979, loss: 0.0020, time: 0.0041\n",
            "batch: 776/979, loss: 0.0020, time: 0.0041\n",
            "batch: 777/979, loss: 0.0020, time: 0.0043\n",
            "batch: 778/979, loss: 0.0020, time: 0.0033\n",
            "batch: 779/979, loss: 0.0020, time: 0.0028\n",
            "batch: 780/979, loss: 0.0020, time: 0.0029\n",
            "batch: 781/979, loss: 0.0020, time: 0.0029\n",
            "batch: 782/979, loss: 0.0020, time: 0.0028\n",
            "batch: 783/979, loss: 0.0020, time: 0.0029\n",
            "batch: 784/979, loss: 0.0020, time: 0.0040\n",
            "batch: 785/979, loss: 0.0020, time: 0.0046\n",
            "batch: 786/979, loss: 0.0020, time: 0.0041\n",
            "batch: 787/979, loss: 0.0020, time: 0.0042\n",
            "batch: 788/979, loss: 0.0020, time: 0.0041\n",
            "batch: 789/979, loss: 0.0020, time: 0.0041\n",
            "batch: 790/979, loss: 0.0020, time: 0.0041\n",
            "batch: 791/979, loss: 0.0020, time: 0.0041\n",
            "batch: 792/979, loss: 0.0020, time: 0.0036\n",
            "batch: 793/979, loss: 0.0020, time: 0.0061\n",
            "batch: 794/979, loss: 0.0020, time: 0.0041\n",
            "batch: 795/979, loss: 0.0020, time: 0.0042\n",
            "batch: 796/979, loss: 0.0020, time: 0.0041\n",
            "batch: 797/979, loss: 0.0020, time: 0.0042\n",
            "batch: 798/979, loss: 0.0020, time: 0.0042\n",
            "batch: 799/979, loss: 0.0020, time: 0.0055\n",
            "batch: 800/979, loss: 0.0020, time: 0.0042\n",
            "batch: 801/979, loss: 0.0020, time: 0.0036\n",
            "batch: 802/979, loss: 0.0020, time: 0.0036\n",
            "batch: 803/979, loss: 0.0020, time: 0.0036\n",
            "batch: 804/979, loss: 0.0020, time: 0.0039\n",
            "batch: 805/979, loss: 0.0020, time: 0.0041\n",
            "batch: 806/979, loss: 0.0020, time: 0.0030\n",
            "batch: 807/979, loss: 0.0020, time: 0.0037\n",
            "batch: 808/979, loss: 0.0020, time: 0.0036\n",
            "batch: 809/979, loss: 0.0020, time: 0.0034\n",
            "batch: 810/979, loss: 0.0020, time: 0.0035\n",
            "batch: 811/979, loss: 0.0020, time: 0.0034\n",
            "batch: 812/979, loss: 0.0020, time: 0.0041\n",
            "batch: 813/979, loss: 0.0020, time: 0.0042\n",
            "batch: 814/979, loss: 0.0020, time: 0.0045\n",
            "batch: 815/979, loss: 0.0020, time: 0.0041\n",
            "batch: 816/979, loss: 0.0020, time: 0.0043\n",
            "batch: 817/979, loss: 0.0020, time: 0.0037\n",
            "batch: 818/979, loss: 0.0020, time: 0.0037\n",
            "batch: 819/979, loss: 0.0020, time: 0.0036\n",
            "batch: 820/979, loss: 0.0020, time: 0.0036\n",
            "batch: 821/979, loss: 0.0020, time: 0.0036\n",
            "batch: 822/979, loss: 0.0020, time: 0.0036\n",
            "batch: 823/979, loss: 0.0020, time: 0.0036\n",
            "batch: 824/979, loss: 0.0020, time: 0.0037\n",
            "batch: 825/979, loss: 0.0020, time: 0.0045\n",
            "batch: 826/979, loss: 0.0020, time: 0.0053\n",
            "batch: 827/979, loss: 0.0020, time: 0.0040\n",
            "batch: 828/979, loss: 0.0020, time: 0.0043\n",
            "batch: 829/979, loss: 0.0020, time: 0.0038\n",
            "batch: 830/979, loss: 0.0020, time: 0.0035\n",
            "batch: 831/979, loss: 0.0020, time: 0.0035\n",
            "batch: 832/979, loss: 0.0020, time: 0.0035\n",
            "batch: 833/979, loss: 0.0020, time: 0.0035\n",
            "batch: 834/979, loss: 0.0020, time: 0.0035\n",
            "batch: 835/979, loss: 0.0020, time: 0.0034\n",
            "batch: 836/979, loss: 0.0020, time: 0.0034\n",
            "batch: 837/979, loss: 0.0020, time: 0.0041\n",
            "batch: 838/979, loss: 0.0020, time: 0.0041\n",
            "batch: 839/979, loss: 0.0020, time: 0.0040\n",
            "batch: 840/979, loss: 0.0019, time: 0.0036\n",
            "batch: 841/979, loss: 0.0020, time: 0.0036\n",
            "batch: 842/979, loss: 0.0020, time: 0.0035\n",
            "batch: 843/979, loss: 0.0020, time: 0.0044\n",
            "batch: 844/979, loss: 0.0020, time: 0.0036\n",
            "batch: 845/979, loss: 0.0020, time: 0.0036\n",
            "batch: 846/979, loss: 0.0020, time: 0.0036\n",
            "batch: 847/979, loss: 0.0020, time: 0.0036\n",
            "batch: 848/979, loss: 0.0020, time: 0.0036\n",
            "batch: 849/979, loss: 0.0020, time: 0.0037\n",
            "batch: 850/979, loss: 0.0020, time: 0.0036\n",
            "batch: 851/979, loss: 0.0019, time: 0.0036\n",
            "batch: 852/979, loss: 0.0019, time: 0.0042\n",
            "batch: 853/979, loss: 0.0019, time: 0.0045\n",
            "batch: 854/979, loss: 0.0019, time: 0.0045\n",
            "batch: 855/979, loss: 0.0019, time: 0.0041\n",
            "batch: 856/979, loss: 0.0019, time: 0.0040\n",
            "batch: 857/979, loss: 0.0019, time: 0.0037\n",
            "batch: 858/979, loss: 0.0019, time: 0.0036\n",
            "batch: 859/979, loss: 0.0019, time: 0.0036\n",
            "batch: 860/979, loss: 0.0019, time: 0.0036\n",
            "batch: 861/979, loss: 0.0019, time: 0.0036\n",
            "batch: 862/979, loss: 0.0019, time: 0.0036\n",
            "batch: 863/979, loss: 0.0019, time: 0.0036\n",
            "batch: 864/979, loss: 0.0019, time: 0.0036\n",
            "batch: 865/979, loss: 0.0019, time: 0.0036\n",
            "batch: 866/979, loss: 0.0019, time: 0.0036\n",
            "batch: 867/979, loss: 0.0019, time: 0.0036\n",
            "batch: 868/979, loss: 0.0019, time: 0.0037\n",
            "batch: 869/979, loss: 0.0019, time: 0.0036\n",
            "batch: 870/979, loss: 0.0019, time: 0.0036\n",
            "batch: 871/979, loss: 0.0019, time: 0.0035\n",
            "batch: 872/979, loss: 0.0019, time: 0.0037\n",
            "batch: 873/979, loss: 0.0019, time: 0.0039\n",
            "batch: 874/979, loss: 0.0019, time: 0.0036\n",
            "batch: 875/979, loss: 0.0019, time: 0.0038\n",
            "batch: 876/979, loss: 0.0019, time: 0.0037\n",
            "batch: 877/979, loss: 0.0019, time: 0.0037\n",
            "batch: 878/979, loss: 0.0019, time: 0.0035\n",
            "batch: 879/979, loss: 0.0019, time: 0.0036\n",
            "batch: 880/979, loss: 0.0019, time: 0.0036\n",
            "batch: 881/979, loss: 0.0019, time: 0.0035\n",
            "batch: 882/979, loss: 0.0019, time: 0.0057\n",
            "batch: 883/979, loss: 0.0019, time: 0.0038\n",
            "batch: 884/979, loss: 0.0019, time: 0.0037\n",
            "batch: 885/979, loss: 0.0020, time: 0.0036\n",
            "batch: 886/979, loss: 0.0020, time: 0.0036\n",
            "batch: 887/979, loss: 0.0019, time: 0.0036\n",
            "batch: 888/979, loss: 0.0019, time: 0.0036\n",
            "batch: 889/979, loss: 0.0019, time: 0.0036\n",
            "batch: 890/979, loss: 0.0019, time: 0.0061\n",
            "batch: 891/979, loss: 0.0019, time: 0.0037\n",
            "batch: 892/979, loss: 0.0019, time: 0.0044\n",
            "batch: 893/979, loss: 0.0019, time: 0.0036\n",
            "batch: 894/979, loss: 0.0019, time: 0.0035\n",
            "batch: 895/979, loss: 0.0019, time: 0.0041\n",
            "batch: 896/979, loss: 0.0019, time: 0.0035\n",
            "batch: 897/979, loss: 0.0019, time: 0.0035\n",
            "batch: 898/979, loss: 0.0020, time: 0.0035\n",
            "batch: 899/979, loss: 0.0019, time: 0.0035\n",
            "batch: 900/979, loss: 0.0019, time: 0.0036\n",
            "batch: 901/979, loss: 0.0019, time: 0.0035\n",
            "batch: 902/979, loss: 0.0019, time: 0.0035\n",
            "batch: 903/979, loss: 0.0019, time: 0.0036\n",
            "batch: 904/979, loss: 0.0019, time: 0.0034\n",
            "batch: 905/979, loss: 0.0019, time: 0.0034\n",
            "batch: 906/979, loss: 0.0019, time: 0.0034\n",
            "batch: 907/979, loss: 0.0019, time: 0.0034\n",
            "batch: 908/979, loss: 0.0019, time: 0.0046\n",
            "batch: 909/979, loss: 0.0019, time: 0.0075\n",
            "batch: 910/979, loss: 0.0019, time: 0.0039\n",
            "batch: 911/979, loss: 0.0019, time: 0.0035\n",
            "batch: 912/979, loss: 0.0020, time: 0.0037\n",
            "batch: 913/979, loss: 0.0020, time: 0.0040\n",
            "batch: 914/979, loss: 0.0020, time: 0.0051\n",
            "batch: 915/979, loss: 0.0020, time: 0.0044\n",
            "batch: 916/979, loss: 0.0020, time: 0.0044\n",
            "batch: 917/979, loss: 0.0020, time: 0.0036\n",
            "batch: 918/979, loss: 0.0020, time: 0.0044\n",
            "batch: 919/979, loss: 0.0020, time: 0.0036\n",
            "batch: 920/979, loss: 0.0020, time: 0.0034\n",
            "batch: 921/979, loss: 0.0020, time: 0.0035\n",
            "batch: 922/979, loss: 0.0020, time: 0.0039\n",
            "batch: 923/979, loss: 0.0020, time: 0.0038\n",
            "batch: 924/979, loss: 0.0020, time: 0.0038\n",
            "batch: 925/979, loss: 0.0020, time: 0.0035\n",
            "batch: 926/979, loss: 0.0020, time: 0.0035\n",
            "batch: 927/979, loss: 0.0020, time: 0.0036\n",
            "batch: 928/979, loss: 0.0020, time: 0.0060\n",
            "batch: 929/979, loss: 0.0020, time: 0.0035\n",
            "batch: 930/979, loss: 0.0020, time: 0.0036\n",
            "batch: 931/979, loss: 0.0020, time: 0.0035\n",
            "batch: 932/979, loss: 0.0020, time: 0.0034\n",
            "batch: 933/979, loss: 0.0020, time: 0.0039\n",
            "batch: 934/979, loss: 0.0020, time: 0.0035\n",
            "batch: 935/979, loss: 0.0020, time: 0.0035\n",
            "batch: 936/979, loss: 0.0020, time: 0.0036\n",
            "batch: 937/979, loss: 0.0020, time: 0.0043\n",
            "batch: 938/979, loss: 0.0020, time: 0.0035\n",
            "batch: 939/979, loss: 0.0020, time: 0.0035\n",
            "batch: 940/979, loss: 0.0020, time: 0.0034\n",
            "batch: 941/979, loss: 0.0020, time: 0.0034\n",
            "batch: 942/979, loss: 0.0020, time: 0.0034\n",
            "batch: 943/979, loss: 0.0020, time: 0.0036\n",
            "batch: 944/979, loss: 0.0020, time: 0.0035\n",
            "batch: 945/979, loss: 0.0020, time: 0.0035\n",
            "batch: 946/979, loss: 0.0020, time: 0.0035\n",
            "batch: 947/979, loss: 0.0020, time: 0.0035\n",
            "batch: 948/979, loss: 0.0020, time: 0.0035\n",
            "batch: 949/979, loss: 0.0020, time: 0.0036\n",
            "batch: 950/979, loss: 0.0020, time: 0.0041\n",
            "batch: 951/979, loss: 0.0020, time: 0.0041\n",
            "batch: 952/979, loss: 0.0020, time: 0.0039\n",
            "batch: 953/979, loss: 0.0020, time: 0.0045\n",
            "batch: 954/979, loss: 0.0020, time: 0.0032\n",
            "batch: 955/979, loss: 0.0020, time: 0.0037\n",
            "batch: 956/979, loss: 0.0021, time: 0.0038\n",
            "batch: 957/979, loss: 0.0021, time: 0.0042\n",
            "batch: 958/979, loss: 0.0021, time: 0.0043\n",
            "batch: 959/979, loss: 0.0024, time: 0.0045\n",
            "batch: 960/979, loss: 0.0024, time: 0.0041\n",
            "batch: 961/979, loss: 0.0024, time: 0.0041\n",
            "batch: 962/979, loss: 0.0024, time: 0.0042\n",
            "batch: 963/979, loss: 0.0024, time: 0.0039\n",
            "batch: 964/979, loss: 0.0024, time: 0.0071\n",
            "batch: 965/979, loss: 0.0024, time: 0.0040\n",
            "batch: 966/979, loss: 0.0024, time: 0.0038\n",
            "batch: 967/979, loss: 0.0024, time: 0.0039\n",
            "batch: 968/979, loss: 0.0024, time: 0.0037\n",
            "batch: 969/979, loss: 0.0024, time: 0.0042\n",
            "batch: 970/979, loss: 0.0024, time: 0.0061\n",
            "batch: 971/979, loss: 0.0024, time: 0.0042\n",
            "batch: 972/979, loss: 0.0024, time: 0.0047\n",
            "batch: 973/979, loss: 0.0024, time: 0.0040\n",
            "batch: 974/979, loss: 0.0024, time: 0.0038\n",
            "batch: 975/979, loss: 0.0024, time: 0.0036\n",
            "batch: 976/979, loss: 0.0024, time: 0.0036\n",
            "batch: 977/979, loss: 0.0024, time: 0.0036\n",
            "batch: 978/979, loss: 0.0024, time: 0.0037\n",
            "batch: 979/979, loss: 0.0024, time: 0.0045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl_test_accuracy_arr"
      ],
      "metadata": {
        "id": "1SQ4xsOVd0bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba559db2-917b-47af-a5fe-3a7c42d8abf9"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.93169248, 0.92823562, 0.93003319, 0.92353429, 0.92284292,\n",
              "       0.92146018, 0.92063053, 0.91482301, 0.92090708, 0.91302544])"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hq_train_losses, \\\n",
        "    hq_test_losses, \\\n",
        "    hq_train_accuracy_arr, \\\n",
        "    hq_test_accuracy_arr = train_for_hqnn(hybrid_quantum_model, criterion=criterion,\n",
        "                              optimizer=hq_optimizer,\n",
        "                              train_dataloader=train_loader,\n",
        "                              test_dataloader=test_loader,\n",
        "                              num_epochs=1)"
      ],
      "metadata": {
        "id": "2CqcSaKTTMhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "368080a685bb4c7f9f398dbc1239b41d",
            "4ddb33c56a4a4101875550005d29d434",
            "4d8409aacd1b42218c94faecc0677687",
            "0da4eb9ce2f64127a63e664964d780ec",
            "b6580ba4076b4904b79d227de6358a17",
            "300ca0c9ed674de69a3f68b0e2869dfd",
            "9a237ae662484d1cbaf4767cb2f11982",
            "69a51b53c53d459883f8fbf70e9ca759",
            "9ba6bf48378c4551b10597113bd70103",
            "b7f9c2a472eb47bb8da6bf62b0646196",
            "4b5a6c5f99464b9ab74d6be8dcba9a41"
          ]
        },
        "outputId": "c071fea1-627c-4180-d3d1-c3202e97d774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "368080a685bb4c7f9f398dbc1239b41d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it: 1/979, loss: 1.6102, time: 139.4079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hq_test_accuracy_arr"
      ],
      "metadata": {
        "id": "Jgjxi_GhTVZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hq_train_accuracy_arr"
      ],
      "metadata": {
        "id": "Tsr4BWl-PGF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "in_channels = 1\n",
        "out_channels = 3\n",
        "length = 31\n",
        "num_classes = 5\n",
        "kernel_size = 2\n",
        "embedding_dim = 200\n",
        "hidden_dim = 100\n",
        "padding = 1\n",
        "stride = 1\n",
        "num_qubits = 4\n",
        "\n",
        "# Create dummy data & labels\n",
        "train_dummy_data = torch.randint(0, 1000, (batch_size, length)).to(device)\n",
        "train_dummy_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "\n",
        "test_dummy_data = torch.randn(batch_size, length)\n",
        "test_dummy_labels = torch.randint(0, num_classes, (batch_size,))\n",
        "\n",
        "# Create a simple Dataset and DataLoader\n",
        "train_dummy_dataset = TensorDataset(train_dummy_data, train_dummy_labels)\n",
        "train_dummy_loader = DataLoader(train_dummy_dataset, batch_size)\n",
        "\n",
        "test_dummy_dataset = TensorDataset(test_dummy_data, test_dummy_labels)\n",
        "test_dummy_loader = DataLoader(test_dummy_dataset, batch_size)\n"
      ],
      "metadata": {
        "id": "pF1KSYMGTdHO"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 8e-4 #Learning rate\n",
        "\n",
        "toy_hq_model = ToyHQModel(VOCAB_SIZE, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_qubits, num_classes, lstm_layers = 1)\n",
        "\n",
        "toy_hq_model = toy_hq_model.to(device)\n",
        "# Set up the criterion (loss function)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.AdamW(toy_hq_model.parameters(), lr=LR, weight_decay = 5e-6)\n",
        "\n",
        "print(toy_hq_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZjYDvFJLG0z",
        "outputId": "108bfc04-5bdf-43dc-8322-3659a2112648"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToyHQModel(\n",
            "  (embedding): Embedding(37569, 200)\n",
            "  (attention): Attention(\n",
            "    (attn): Linear(in_features=200, out_features=100, bias=True)\n",
            "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
            "  )\n",
            "  (lstm): HQLSTM(\n",
            "    (lstm_cells): ModuleList(\n",
            "      (0): HQLSTMCell()\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toy_hq_model(train_dummy_data)"
      ],
      "metadata": {
        "id": "AH2z4g_A5FjA",
        "outputId": "ca9d8fa0-d80c-4a16-b9b9-f4bb0df18bb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n",
            "torch.Size([2, 16])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.6381, -1.6118, -1.5608, -1.6813, -1.5606],\n",
              "        [-1.6472, -1.6141, -1.5558, -1.6795, -1.5565]], device='cuda:0',\n",
              "       grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "rActVEdigY8M",
        "outputId": "05ad61af-0c0d-4c9a-aeb7-d989abf48ac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28985, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dummy_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "# test_dummy_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ],
      "metadata": {
        "id": "Cps3yt0-eIKQ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE = 2"
      ],
      "metadata": {
        "id": "18tjNlfVeIKR"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dummy_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "test_dummy_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)"
      ],
      "metadata": {
        "id": "qQRprRrveIKR"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, \\\n",
        "    test_losses, \\\n",
        "    train_accuracy_arr, \\\n",
        "    test_accuracy_arr = train_for_hqnn(toy_hq_model, criterion=criterion,\n",
        "                              optimizer=optimizer,\n",
        "                              train_dataloader=train_dummy_loader,\n",
        "                              test_dataloader=test_dummy_loader,\n",
        "                              num_epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "06a8e085c5684549b8564f24bd762e4f",
            "3a33592a8c0f4c3e9e42f2d092dd0889",
            "885eeace639140e2a01b16692b64accc",
            "ca36aa40d1e64ad496b3b838462acaa0",
            "62374ed5a97a431c942ba275585d7d57",
            "6281ff12628a44da968a820165064dd1",
            "a32f46d1b39247d69617fd8d08255dde",
            "e437002cee7c48468d608ac3b6419490",
            "d4d7dc520db647d9bd626ee52f8a35bd",
            "27d5e2469ff444539de5d7d4b4ab6278",
            "875eff2116cf45b9b386071a7d8cc1c7"
          ]
        },
        "id": "RBB6oiTGG02t",
        "outputId": "48e1fb56-cbc4-4c18-9836-1a3a5342eb11"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06a8e085c5684549b8564f24bd762e4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-fd0fa7cb832d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_accuracy_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     test_accuracy_arr = train_for_hqnn(toy_hq_model, criterion=criterion,\n\u001b[0m\u001b[1;32m      5\u001b[0m                               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dummy_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-0036f823edc8>\u001b[0m in \u001b[0;36mtrain_for_hqnn\u001b[0;34m(model, criterion, optimizer, train_dataloader, test_dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# t.grad = torch.tensor([0., 0., 0.])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# t = t - lr * t.grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses"
      ],
      "metadata": {
        "id": "lPCx3xe9zXNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FysnlhgaW4TF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bbe2733aa8a944c1bd1a26e4dfa962db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73ea96ba473d410aa4ada890ba0d9f21",
              "IPY_MODEL_5006cfd87b7249d5a4831b38980a26eb",
              "IPY_MODEL_76b318d61b674a7aad44fd4259561ddf"
            ],
            "layout": "IPY_MODEL_89fee5e8b5e24462a52ac6a56f44b4bc"
          }
        },
        "73ea96ba473d410aa4ada890ba0d9f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e736b0c8fb9445fe892042b4afecd39d",
            "placeholder": "​",
            "style": "IPY_MODEL_4498d76a83a94e2d846903a0142d900a",
            "value": "100%"
          }
        },
        "5006cfd87b7249d5a4831b38980a26eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_151693e862e245d78cc828c4b5286141",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ff2611cef243cfb95716cd49353050",
            "value": 10
          }
        },
        "76b318d61b674a7aad44fd4259561ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f17bb282bb90488f9f71675c2e9b9073",
            "placeholder": "​",
            "style": "IPY_MODEL_6b328e39265244abb9300804ff9f6069",
            "value": " 10/10 [01:09&lt;00:00,  6.92s/it]"
          }
        },
        "89fee5e8b5e24462a52ac6a56f44b4bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e736b0c8fb9445fe892042b4afecd39d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4498d76a83a94e2d846903a0142d900a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "151693e862e245d78cc828c4b5286141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ff2611cef243cfb95716cd49353050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f17bb282bb90488f9f71675c2e9b9073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b328e39265244abb9300804ff9f6069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "368080a685bb4c7f9f398dbc1239b41d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ddb33c56a4a4101875550005d29d434",
              "IPY_MODEL_4d8409aacd1b42218c94faecc0677687",
              "IPY_MODEL_0da4eb9ce2f64127a63e664964d780ec"
            ],
            "layout": "IPY_MODEL_b6580ba4076b4904b79d227de6358a17"
          }
        },
        "4ddb33c56a4a4101875550005d29d434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_300ca0c9ed674de69a3f68b0e2869dfd",
            "placeholder": "​",
            "style": "IPY_MODEL_9a237ae662484d1cbaf4767cb2f11982",
            "value": "  0%"
          }
        },
        "4d8409aacd1b42218c94faecc0677687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69a51b53c53d459883f8fbf70e9ca759",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ba6bf48378c4551b10597113bd70103",
            "value": 0
          }
        },
        "0da4eb9ce2f64127a63e664964d780ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7f9c2a472eb47bb8da6bf62b0646196",
            "placeholder": "​",
            "style": "IPY_MODEL_4b5a6c5f99464b9ab74d6be8dcba9a41",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "b6580ba4076b4904b79d227de6358a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "300ca0c9ed674de69a3f68b0e2869dfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a237ae662484d1cbaf4767cb2f11982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69a51b53c53d459883f8fbf70e9ca759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba6bf48378c4551b10597113bd70103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7f9c2a472eb47bb8da6bf62b0646196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5a6c5f99464b9ab74d6be8dcba9a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06a8e085c5684549b8564f24bd762e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a33592a8c0f4c3e9e42f2d092dd0889",
              "IPY_MODEL_885eeace639140e2a01b16692b64accc",
              "IPY_MODEL_ca36aa40d1e64ad496b3b838462acaa0"
            ],
            "layout": "IPY_MODEL_62374ed5a97a431c942ba275585d7d57"
          }
        },
        "3a33592a8c0f4c3e9e42f2d092dd0889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6281ff12628a44da968a820165064dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_a32f46d1b39247d69617fd8d08255dde",
            "value": "  0%"
          }
        },
        "885eeace639140e2a01b16692b64accc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e437002cee7c48468d608ac3b6419490",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4d7dc520db647d9bd626ee52f8a35bd",
            "value": 0
          }
        },
        "ca36aa40d1e64ad496b3b838462acaa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d5e2469ff444539de5d7d4b4ab6278",
            "placeholder": "​",
            "style": "IPY_MODEL_875eff2116cf45b9b386071a7d8cc1c7",
            "value": " 0/2 [30:03&lt;?, ?it/s]"
          }
        },
        "62374ed5a97a431c942ba275585d7d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6281ff12628a44da968a820165064dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32f46d1b39247d69617fd8d08255dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e437002cee7c48468d608ac3b6419490": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d7dc520db647d9bd626ee52f8a35bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27d5e2469ff444539de5d7d4b4ab6278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875eff2116cf45b9b386071a7d8cc1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}