{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cyberbullying_tweets.csv')\n",
    "df\n",
    "df = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})\n",
    "df.head()\n",
    "# Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(\"\", text)\n",
    "\n",
    "# Remove punctuations, stopwords, links, mentions and new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = re.sub(r'\\r|\\n', ' ', text.lower())  # Replace newline and carriage return with space, and convert to lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)  # Remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)  # Remove non-ASCII characters\n",
    "    banned_list = string.punctuation\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    # Remove hashtags at the end of the sentence\n",
    "    new_tweet = re.sub(r'(\\s+#[\\w-]+)+\\s*$', '', tweet).strip()\n",
    "\n",
    "    # Remove the # symbol from hashtags in the middle of the sentence\n",
    "    new_tweet = re.sub(r'#([\\w-]+)', r'\\1', new_tweet).strip()\n",
    "\n",
    "    return new_tweet\n",
    "\n",
    "# Filter special characters such as & and $ present in some words\n",
    "def filter_chars(text):\n",
    "    return ' '.join('' if ('$' in word) or ('&' in word) else word for word in text.split())\n",
    "\n",
    "# Remove multiple spaces\n",
    "def remove_mult_spaces(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "# Function to check if the text is in English, and return an empty string if it's not\n",
    "def filter_non_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except LangDetectException:\n",
    "        lang = \"unknown\"\n",
    "    return text if lang == \"en\" else \"\"\n",
    "\n",
    "# Expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Lemmatize words\n",
    "def lemmatize(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Remove short words\n",
    "def remove_short_words(text, min_len=2):\n",
    "    words = text.split()\n",
    "    long_words = [word for word in words if len(word) >= min_len]\n",
    "    return ' '.join(long_words)\n",
    "\n",
    "# Replace elongated words with their base form\n",
    "def replace_elongated_words(text):\n",
    "    regex_pattern = r'\\b(\\w+)((\\w)\\3{2,})(\\w*)\\b'\n",
    "    return re.sub(regex_pattern, r'\\1\\3\\4', text)\n",
    "\n",
    "# Remove repeated punctuation\n",
    "def remove_repeated_punctuation(text):\n",
    "    return re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "\n",
    "# Remove extra whitespace\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_url_shorteners(text):\n",
    "    return re.sub(r'(?:http[s]?://)?(?:www\\.)?(?:bit\\.ly|goo\\.gl|t\\.co|tinyurl\\.com|tr\\.im|is\\.gd|cli\\.gs|u\\.nu|url\\.ie|tiny\\.cc|alturl\\.com|ow\\.ly|bit\\.do|adoro\\.to)\\S+', '', text)\n",
    "\n",
    "# Remove spaces at the beginning and end of the tweet\n",
    "def remove_spaces_tweets(tweet):\n",
    "    return tweet.strip()\n",
    "\n",
    "# Remove short tweets\n",
    "def remove_short_tweets(tweet, min_words=3):\n",
    "    words = tweet.split()\n",
    "    return tweet if len(words) >= min_words else \"\"\n",
    "\n",
    "# Function to call all the cleaning functions in the correct order\n",
    "def clean_tweet(tweet):\n",
    "    tweet = strip_emoji(tweet)\n",
    "    tweet = expand_contractions(tweet)\n",
    "    tweet = filter_non_english(tweet)\n",
    "    tweet = strip_all_entities(tweet)\n",
    "    tweet = clean_hashtags(tweet)\n",
    "    tweet = filter_chars(tweet)\n",
    "    tweet = remove_mult_spaces(tweet)\n",
    "    tweet = remove_numbers(tweet)\n",
    "    tweet = lemmatize(tweet)\n",
    "    tweet = remove_short_words(tweet)\n",
    "    tweet = replace_elongated_words(tweet)\n",
    "    tweet = remove_repeated_punctuation(tweet)\n",
    "    tweet = remove_extra_whitespace(tweet)\n",
    "    tweet = remove_url_shorteners(tweet)\n",
    "    tweet = remove_spaces_tweets(tweet)\n",
    "    tweet = remove_short_tweets(tweet)\n",
    "    tweet = ' '.join(tweet.split())  # Remove multiple spaces between words\n",
    "    return tweet\n",
    "df['text_clean'] = [clean_tweet(tweet) for tweet in df['text']]\n",
    "df.head()\n",
    "print(f'There are around {int(df[\"text_clean\"].duplicated().sum())} duplicated tweets, we will remove them.')\n",
    "df.drop_duplicates(\"text_clean\", inplace=True)\n",
    "df.sentiment.value_counts()\n",
    "df = df[df[\"sentiment\"]!=\"other_cyberbullying\"]\n",
    "df['text_len'] = [len(text.split()) for text in df.text_clean]\n",
    "df.sentiment.value_counts()\n",
    "df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})\n",
    "df_label_0 = df[df['sentiment'] == 0].sort_values(by='text_len', ascending=False)\n",
    "df_label_1 = df[df['sentiment'] == 1].sort_values(by='text_len', ascending=False)\n",
    "df_label_2 = df[df['sentiment'] == 2].sort_values(by='text_len', ascending=False)\n",
    "df_label_3 = df[df['sentiment'] == 3].sort_values(by='text_len', ascending=False)\n",
    "df_label_4 = df[df['sentiment'] == 4].sort_values(by='text_len', ascending=False)\n",
    "\n",
    "# Determine how many rows to drop from each (half)\n",
    "n_to_drop_0 = 7164\n",
    "n_to_drop_1 = 7068\n",
    "n_to_drop_2 = 6679\n",
    "n_to_drop_3 = 6643\n",
    "n_to_drop_4 = 3077\n",
    "\n",
    "# Drop rows from class 0\n",
    "df_label_0_dropped = df_label_0.iloc[n_to_drop_0:].drop(columns=['text_len'])\n",
    "\n",
    "# Drop rows from class 1\n",
    "df_label_1_dropped = df_label_1.iloc[n_to_drop_1:].drop(columns=['text_len'])\n",
    "\n",
    "df_label_2_dropped = df_label_0.iloc[n_to_drop_2:].drop(columns=['text_len'])\n",
    "\n",
    "df_label_3_dropped = df_label_0.iloc[n_to_drop_3:].drop(columns=['text_len'])\n",
    "\n",
    "df_label_4_dropped = df_label_0.iloc[n_to_drop_4:].drop(columns=['text_len'])\n",
    "\n",
    "# Concatenate the two DataFrames back into a single DataFrame\n",
    "df = pd.concat([df_label_0_dropped, df_label_1_dropped, df_label_2_dropped, df_label_3_dropped, df_label_4_dropped]).reset_index(drop=True)\n",
    "df['sentiment'] = df['sentiment'].replace({'religion':1,'age':0,'ethnicity':0,'gender':0,'not_cyberbullying':1})\n",
    "df = df[df['text_len'] < df['text_len'].quantile(0.995)]\n",
    "max_len = np.max(df['text_len'])\n",
    "max_len"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
