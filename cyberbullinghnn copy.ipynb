{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOw7Xm_A-Msu"
   },
   "source": [
    "# Text classification with Quanvolutional layer + Attention + HQLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXYmXj3x_Z3k"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fklFBxOt_n0r"
   },
   "source": [
    "Imports for computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KMjztfO_iLH",
    "outputId": "20143246-0e7b-4af0-d715-17167e02c356"
   },
   "outputs": [],
   "source": [
    "# !pip install custatevec_cu12\n",
    "# !pip install pennylane pennylane-lightning\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNda1wZNADCt"
   },
   "source": [
    "For plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0HzKYqfvkO5s",
    "outputId": "31d2ebeb-b99e-4e46-a490-a4a8aa952bf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "# plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEByIz_FAyoB"
   },
   "source": [
    "For data preprocessing and text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5pFgXpHkJ1fd",
    "outputId": "fc0be830-fe62-411f-c97b-12d3933962fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\den1s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\den1s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\den1s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\den1s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# !pip install langdetect\n",
    "# !pip install contractions\n",
    "# !pip install emoji==1.4.1\n",
    "# !pip install nltk\n",
    "\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from langdetect import detect, LangDetectException\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-O5RGApBB0S"
   },
   "source": [
    "Set seed for reproductivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0Ud4hzR0kO5v"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmjJoJyZkO5v",
    "outputId": "f1c8b41c-e728-4c51-9d12-3fbe200d9c2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightning.qubit device (wires=4) at 0x274a3373590>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 4\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "# dev = qml.device('qulacs.simulator', wires = range(num_qubits))\n",
    "device = \"cpu\"\n",
    "dev = qml.device(\"lightning.qubit\", wires=range(num_qubits))\n",
    "# dev = qml.device(\"default.tensor\", method=\"tn\", wires = range(num_qubits), **kwargs_tn)\n",
    "dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh5ts37oBzek"
   },
   "source": [
    "# Import data, data preprocessing and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZHah9RUDkV1"
   },
   "source": [
    "We use \"Cyberbullying Classification\" dataset from Kaggle. You can acquire more information about the data by the following link: https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "PN22QbhmkO5v"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('mental_health.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "rCNEZ-XakO5v",
    "outputId": "f60da740-fc14-4217-ba61-407d1de67a15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear american teens question dutch person hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing look forward lifei dont many reasons k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>music recommendations im looking expand playli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im done trying feel betterthe reason im still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worried  year old girl subject domestic physic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27972</th>\n",
       "      <td>posting everyday people stop caring  religion ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27973</th>\n",
       "      <td>okay definetly need hear guys opinion ive pret...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27974</th>\n",
       "      <td>cant get dog think ill kill myselfthe last thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27975</th>\n",
       "      <td>whats point princess bridei really think like ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27976</th>\n",
       "      <td>got nudes person might might know snapchat do ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27977 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      dear american teens question dutch person hear...      0\n",
       "1      nothing look forward lifei dont many reasons k...      1\n",
       "2      music recommendations im looking expand playli...      0\n",
       "3      im done trying feel betterthe reason im still ...      1\n",
       "4      worried  year old girl subject domestic physic...      1\n",
       "...                                                  ...    ...\n",
       "27972  posting everyday people stop caring  religion ...      0\n",
       "27973  okay definetly need hear guys opinion ive pret...      0\n",
       "27974  cant get dog think ill kill myselfthe last thi...      1\n",
       "27975  whats point princess bridei really think like ...      1\n",
       "27976  got nudes person might might know snapchat do ...      0\n",
       "\n",
       "[27977 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "92KibKseK0C7"
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NoSlde7eLDdp",
    "outputId": "cd8c0c41-c9d6-4215-c320-90bc4c1906a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear american teens question dutch person hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing look forward lifei dont many reasons k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>music recommendations im looking expand playli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im done trying feel betterthe reason im still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worried  year old girl subject domestic physic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  dear american teens question dutch person hear...      0\n",
       "1  nothing look forward lifei dont many reasons k...      1\n",
       "2  music recommendations im looking expand playli...      0\n",
       "3  im done trying feel betterthe reason im still ...      1\n",
       "4  worried  year old girl subject domestic physic...      1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsXc6JwwCTMQ"
   },
   "source": [
    "Define cleaning functions. Source: https://www.kaggle.com/code/ludovicocuoghi/detecting-bullying-tweets-pytorch-lstm-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RYNGL2_Uc4vP",
    "outputId": "9b8507c9-cac5-43b8-8b90-e762b4efc881"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear american teens question dutch person hear...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nothing look forward lifei dont many reasons k...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>music recommendations im looking expand playli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im done trying feel betterthe reason im still ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worried  year old girl subject domestic physic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  dear american teens question dutch person hear...      0\n",
       "1  nothing look forward lifei dont many reasons k...      1\n",
       "2  music recommendations im looking expand playli...      0\n",
       "3  im done trying feel betterthe reason im still ...      1\n",
       "4  worried  year old girl subject domestic physic...      1"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh56r73aewhE",
    "outputId": "e0bcd169-4a09-4934-f874-4f8b66f10651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are around 5 duplicated tweets, we will remove them.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are around {int(df[\"text\"].duplicated().sum())} duplicated tweets, we will remove them.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "LNlO3hdhez4-"
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"text\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "LGEU8XC9e3Ue",
    "outputId": "30a08ebb-5b77-40c0-b84e-796ce19fdbcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    14134\n",
       "1    13838\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qZ7PoaKC808"
   },
   "source": [
    "As we can see, after cleaning classes are unbalanced so we will drop \"other_cyberbullying\" class as there is not enough data. Later we will oversample \"not_cyberbullying\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q712dE4kftOe",
    "outputId": "e653bc8f-996d-4c87-a15e-e5265df95cd1"
   },
   "outputs": [],
   "source": [
    "df['text_len'] = [len(text.split()) for text in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.66136724960255"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] == 1]['text_len'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.40045280882977"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] == 0]['text_len'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_0 = df[df['label'] == 0].sort_values(by='text_len', ascending=False)\n",
    "df_label_1 = df[df['label'] == 1].sort_values(by='text_len', ascending=False)\n",
    "\n",
    "# Determine how many rows to drop from each (half)\n",
    "n_to_drop_0 = 3*len(df_label_0) // 4\n",
    "n_to_drop_1 = 3*len(df_label_1) // 4\n",
    "\n",
    "# Drop rows from class 0\n",
    "df_label_0_dropped = df_label_0.iloc[n_to_drop_0:].drop(columns=['text_len'])\n",
    "\n",
    "# Drop rows from class 1\n",
    "df_label_1_dropped = df_label_1.iloc[n_to_drop_1:].drop(columns=['text_len'])\n",
    "\n",
    "# Concatenate the two DataFrames back into a single DataFrame\n",
    "df = pd.concat([df_label_0_dropped, df_label_1_dropped]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gathering bunch fat people zombie apocalypse c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flair change time boys youngest sub anymore im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>see year bitches fill next year im going leave...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>last post wanted see reaction posted random di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>got called broski girl im talking fuck got gia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>lifel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6990</th>\n",
       "      <td>name</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>sorry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>medicationmarijuana</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6994 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     gathering bunch fat people zombie apocalypse c...      0\n",
       "1     flair change time boys youngest sub anymore im...      0\n",
       "2     see year bitches fill next year im going leave...      0\n",
       "3     last post wanted see reaction posted random di...      0\n",
       "4     got called broski girl im talking fuck got gia...      0\n",
       "...                                                 ...    ...\n",
       "6989                                              lifel      1\n",
       "6990                                               name      1\n",
       "6991                                             sorry       1\n",
       "6992                                medicationmarijuana      1\n",
       "6993                                                         1\n",
       "\n",
       "[6994 rows x 2 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_len'] = [len(text.split()) for text in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set target count\n",
    "# target_count = 4901\n",
    "\n",
    "# # Dropping random items to equalize the counts\n",
    "# for column in df.columns:\n",
    "#     current_count = df[column].value_counts().iloc[0]  # Get current count of the first category\n",
    "#     if current_count > target_count:\n",
    "#         # Calculate the number of items to drop\n",
    "#         n_to_drop = current_count - target_count\n",
    "        \n",
    "#         # Select random indices to drop\n",
    "#         indices_to_drop = df[df[column] == df[column].value_counts().index[0]].sample(n=n_to_drop, random_state=1).index\n",
    "\n",
    "#         # Drop the selected rows\n",
    "#         df = df.drop(indices_to_drop)\n",
    "\n",
    "# # Resulting DataFrame value counts\n",
    "# print(\"Value Counts after Dropping Random Items:\")\n",
    "# print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "WQjmvAMHfyG6",
    "outputId": "2a4d2a28-3e89-4178-e9c5-217e036ea37c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\den1s\\AppData\\Local\\Temp\\ipykernel_17288\\631618343.py:2: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n",
    "plt.title('Count of tweets with less than 10 words', fontsize=20)\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "Ir99Mjatf7Sz",
    "outputId": "a2fa6798-54ff-419e-8fcf-1fb2b40c8614"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\den1s\\AppData\\Local\\Temp\\ipykernel_17288\\2060477503.py:2: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.countplot(x='text_len', data=df[(df['text_len']<=1000) & (df['text_len']>10)], palette='Blues_r')\n",
    "plt.title('Count of tweets with high number of words', fontsize=25)\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yx4uFQtBgNf_"
   },
   "outputs": [],
   "source": [
    "# df = df[df['text_len'] < df['text_len'].quantile(0.995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sv5RTF8ggS7a",
    "outputId": "58a4dd8d-c18e-4023-9134-b49ddfe2549d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = np.max(df['text_len'])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    3534\n",
       "1    3460\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSjqH8-0gs9X",
    "outputId": "7ef2ad60-a121-497d-cb14-a4d739109ed8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\den1s\\AppData\\Local\\Temp\\ipykernel_12612\\4141559953.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})\n"
     ]
    }
   ],
   "source": [
    "# df['sentiment'] = df['sentiment'].replace({'religion':0,'age':1,'ethnicity':2,'gender':3,'not_cyberbullying':4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSjlhVvdEqEQ"
   },
   "source": [
    "Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "nPIz-dF6kFEH"
   },
   "outputs": [],
   "source": [
    "def Tokenize(column, seq_len):\n",
    "    ##Create vocabulary of words from column\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    count_words = Counter(corpus)\n",
    "    sorted_words = count_words.most_common()\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "    ##Tokenize the columns text using the vocabulary\n",
    "    text_int = []\n",
    "    for text in column:\n",
    "        r = [vocab_to_int[word] for word in text.split()]\n",
    "        text_int.append(r)\n",
    "    ##Add padding to tokens\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(text_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return sorted_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "KqUTb1b8kd9P"
   },
   "outputs": [],
   "source": [
    "vocabulary, tokenized_column = Tokenize(df[\"text\"], max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "d02_zwPL-9EZ"
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "2azevoNY-y6g"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idKd2ISaEy9d"
   },
   "source": [
    "Text embedding with pre-trained Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "_TdbROTk7nYE"
   },
   "outputs": [],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "EefOe-5Y7rSm"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "fSmBTU0Y7vMO"
   },
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "IZyohOul7yPO"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx5AIcgi68QL",
    "outputId": "94715133-007a-4b77-faa8-b7bb1fad6019"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (12023, 200)\n"
     ]
    }
   ],
   "source": [
    "# Define an empty embedding matrix of shape (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "# Fill the embedding matrix with pre-trained values from word2vec\n",
    "for word, token in vocabulary:\n",
    "    # Check if the word is present in the word2vec model's vocabulary\n",
    "    if word in word2vec_model.wv.key_to_index:\n",
    "        # If the word is present, retrieve its embedding vector and add it to the embedding matrix\n",
    "        embedding_vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[token] = embedding_vector\n",
    "\n",
    "# Print the shape of the embedding matrix\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "tfO4cnjz7_Ot"
   },
   "outputs": [],
   "source": [
    "X = tokenized_column\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "EOAc1Vm38Njv"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly-3aQBnFRcj"
   },
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "achdE0dQ8WEO"
   },
   "outputs": [],
   "source": [
    "# ros = RandomOverSampler()\n",
    "# X_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sac738LjIWXP",
    "outputId": "864194da-252b-44c6-8f3f-a21764cad6f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5595, 29)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGYWBOQ0FW2M"
   },
   "source": [
    "Finally, we define our preprocessed dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "jyEGOm9Z8ffa"
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "WjpbNrLb8jN2"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "mr-o8hPq8lXW"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2V1uSsNFp3h"
   },
   "source": [
    "# Implementation of the classical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "660eM2fEF30E"
   },
   "source": [
    "In this paragraph we will implement model with classical convolution and LSTM in order to compare it with its hybrid-quantum analogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "-VTp5AVlJA3l"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        # The attention linear layer which transforms the input data to the hidden space\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim )\n",
    "        # The linear layer that calculates the attention scores\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Concatenate the last two hidden states in case of a bidirectional LSTM\n",
    "        hidden = hidden[-1]\n",
    "        # Repeat the hidden state across the sequence length\n",
    "        hidden_repeated = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Calculate attention weights\n",
    "        attn_weights = torch.tanh(self.attn(torch.cat((hidden_repeated, encoder_outputs), dim=2)))\n",
    "        # Compute attention scores\n",
    "        attn_weights = self.v(attn_weights).squeeze(2)\n",
    "        # Apply softmax to get valid probabilities\n",
    "        return nn.functional.softmax(attn_weights, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "raK4JVSHExXS"
   },
   "outputs": [],
   "source": [
    "class ClassicalModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_classes, lstm_layers):\n",
    "        super(ClassicalModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = lstm_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, lstm_layers, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Transform words to embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = self.conv(embedded.unsqueeze(1))\n",
    "        # # Pass embeddings to LSTM\n",
    "        # out, hidden = self.lstm(embedded.reshape(-1, embedded.size(1)*embedded.size(2), embedded.size(-1)), hidden)\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(hidden[0], out)\n",
    "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
    "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
    "        # Classify the context vector\n",
    "        out = self.softmax(self.fc(context))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFolBcU3GjzL"
   },
   "source": [
    "# Implementation of the hybrid-quantum model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "Zx9MSDbdkO5y"
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch')\n",
    "def quanvcirc(patch, weights, wires=range(num_qubits)):\n",
    "    # Angle embedding of the patch (reshape to match expected size)\n",
    "    qml.AngleEmbedding(patch, wires=wires, rotation='Y')\n",
    "\n",
    "    # Apply RX rotations based on the weights\n",
    "    qml.RX(weights[0][0], wires=0)\n",
    "    qml.RX(weights[0][1], wires=1)\n",
    "    qml.RY(weights[1][0], wires=2)\n",
    "    qml.RY(weights[1][1], wires=3)\n",
    "\n",
    "    # Apply CNOT gates\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.CNOT(wires=[1, 2])\n",
    "    qml.CNOT(wires=[2, 3])\n",
    "    qml.CNOT(wires=[3, 0])\n",
    "    # Apply RY rotations based on the weights\n",
    "\n",
    "\n",
    "    # Return the expectation values of Pauli-Z measurements on all qubits\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G1L7z_enlfZ",
    "outputId": "2bec97be-b42e-49eb-e113-1f50e6649e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ─╭AngleEmbedding(M0)──RX(1.00)─╭●───────╭X─┤  <Z>\n",
      "1: ─├AngleEmbedding(M0)──RX(1.00)─╰X─╭●────│──┤     \n",
      "2: ─├AngleEmbedding(M0)──RY(1.00)────╰X─╭●─│──┤     \n",
      "3: ─╰AngleEmbedding(M0)──RY(1.00)───────╰X─╰●─┤     \n",
      "\n",
      "M0 = \n",
      "[[1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(quanvcirc)(np.ones((1, 4)), np.ones((2, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "ikYNnTA1pr6A"
   },
   "outputs": [],
   "source": [
    "class QuanConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(QuanConv2D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Define weights and biases as trainable parameters\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size), requires_grad = True\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Extract dimensions\n",
    "        batch_size, in_channels, input_height, input_width = input.shape\n",
    "\n",
    "        # Calculate output dimensions\n",
    "        output_height = (input_height - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        output_width = (input_width - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = torch.zeros(batch_size, self.out_channels, output_height, output_width)\n",
    "\n",
    "        # If padding is required, add it to the input\n",
    "        if self.padding > 0:\n",
    "            input = F.pad(input, (self.padding, self.padding, self.padding, self.padding))\n",
    "\n",
    "        # Perform convolution\n",
    "        for b in range(batch_size):\n",
    "            for c_out in range(self.out_channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        # Slice the input for the current window\n",
    "                        input_slice = input[b, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                        # Perform element-wise multiplication and sum with bias\n",
    "                        output[b, c_out, h, w] = quanvcirc(input_slice.reshape(1, self.kernel_size * self.kernel_size), self.weights[c_out].squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "ZXzmC0KuyEDb"
   },
   "outputs": [],
   "source": [
    "qdi_reps1 = 2\n",
    "qdi_reps2 = 2\n",
    "qdi_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "QixHflZxxt-Y"
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch', diff_method=\"adjoint\")\n",
    "def qdi_circuit(weights, input_array, wires=range(num_qubits)):\n",
    "    for r in range(qdi_reps1):\n",
    "        for i in range(len(wires)):\n",
    "            qml.RX(weights[r][i], wires=wires[i])\n",
    "        for j in range(len(wires)-1):\n",
    "            qml.CNOT(wires=[wires[j], wires[j+1]])\n",
    "        qml.CNOT(wires=[wires[len(wires)-1], wires[0]])\n",
    "        # qml.Barrier()\n",
    "    for d in range(qdi_depth):\n",
    "        qml.AngleEmbedding(input_array, wires=range(num_qubits), rotation='Z')\n",
    "        for r in range(qdi_reps2):\n",
    "            for i in range(len(wires)):\n",
    "                qml.RX(weights[qdi_reps1+d*r][i], wires=wires[i])\n",
    "            for j in range(len(wires)-1):\n",
    "                qml.CNOT(wires=[wires[j], wires[j+1]])\n",
    "            qml.CNOT(wires=[wires[len(wires)-1], wires[0]])\n",
    "            # qml.Barrier()\n",
    "        # qml.Barrier()\n",
    "    return [qml.expval(qml.PauliY(w)) for w in wires]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRFx50H6RkcR",
    "outputId": "6c3c5367-a2d3-4332-d6b1-e9e445cc563f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──RX(1.00)─╭●───────╭X──RX(1.00)─╭●───────╭X─╭AngleEmbedding(M0)──RX(1.00)─╭●───────╭X──RX(1.00)\n",
      "1: ──RX(1.00)─╰X─╭●────│───RX(1.00)─╰X─╭●────│──├AngleEmbedding(M0)──RX(1.00)─╰X─╭●────│───RX(1.00)\n",
      "2: ──RX(1.00)────╰X─╭●─│───RX(1.00)────╰X─╭●─│──├AngleEmbedding(M0)──RX(1.00)────╰X─╭●─│───RX(1.00)\n",
      "3: ──RX(1.00)───────╰X─╰●──RX(1.00)───────╰X─╰●─╰AngleEmbedding(M0)──RX(1.00)───────╰X─╰●──RX(1.00)\n",
      "\n",
      "──╭●───────╭X─┤  <Y>\n",
      "──╰X─╭●────│──┤  <Y>\n",
      "─────╰X─╭●─│──┤  <Y>\n",
      "────────╰X─╰●─┤  <Y>\n",
      "\n",
      "M0 = \n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(qdi_circuit)(np.ones((qdi_reps1 + qdi_reps2, num_qubits)), np.ones((4, 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true,
    "id": "v_mvdRnDqcze"
   },
   "outputs": [],
   "source": [
    "class HQLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_qubits):\n",
    "        super(HQLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Combined weights for efficiency\n",
    "        self.W_input = nn.Parameter(torch.rand(4 * num_qubits, input_size), requires_grad = True).float()\n",
    "        self.W_hid = nn.Parameter(torch.rand(4 * num_qubits, hidden_size), requires_grad = True).float()\n",
    "        self.W_quan = nn.Parameter(torch.zeros(4, qdi_reps1 + qdi_reps2, num_qubits), requires_grad = True).float()\n",
    "        self.W = nn.Parameter(torch.rand(4, hidden_size, num_qubits), requires_grad = True).float()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "\n",
    "        yield_input = F.linear(x, self.W_input)\n",
    "        yield_hidden = F.linear(h_prev, self.W_hid)\n",
    "        # Concatenate input and previous hidden state\n",
    "        combined = yield_input + yield_hidden\n",
    "        # combined = torch.cat((yield_input, yield_hidden), dim=1)\n",
    "\n",
    "        # Apply linear transformation\n",
    "\n",
    "        # Split into gates\n",
    "        # combined = torch.cat([torch.stack(qdi_circuit(self.W_quan[i], combined[:, i:i*num_qubits].reshape(-1, num_qubits))) for i in range(num_qubits)], dim = 0)\n",
    "        i_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[0], combined[:, :num_qubits].reshape(-1, num_qubits))).T.float(), self.W[0])\n",
    "        f_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[1], combined[:, num_qubits:2*num_qubits].reshape(-1, num_qubits))).T.float(), self.W[1])\n",
    "        g_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[2], combined[:, 2*num_qubits:3*num_qubits].reshape(-1, num_qubits))).T.float(), self.W[2])\n",
    "        o_gate = F.linear(torch.stack(qdi_circuit(self.W_quan[3], combined[:, 3*num_qubits:4*num_qubits].reshape(-1, num_qubits))).T.float(), self.W[3])\n",
    "\n",
    "\n",
    "\n",
    "        # combined = torch.cat((i_gate, f_gate, g_gate, o_gate), dim=0).float()\n",
    "        # gates = F.linear(combined, self.W)\n",
    "\n",
    "        # # Split into gates\n",
    "        # i_gate, f_gate, g_gate, o_gate = gates.chunk(4, 0)\n",
    "\n",
    "        # Apply non-linearities\n",
    "        i_gate = torch.sigmoid(i_gate)\n",
    "        f_gate = torch.sigmoid(f_gate)\n",
    "        g_gate = torch.tanh(g_gate)\n",
    "        o_gate = torch.sigmoid(o_gate)\n",
    "\n",
    "        # Update cell state\n",
    "        c_next = (f_gate * c_prev) + (i_gate * g_gate)\n",
    "\n",
    "        # Update hidden state\n",
    "        h_next = o_gate * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "\n",
    "class HQLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_qubits, num_layers=1):\n",
    "        super(HQLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([HQLSTMCell(input_size if l==0 else hidden_size, hidden_size, num_qubits) for l in range(num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        if hidden is None:\n",
    "             h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "             c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        else:\n",
    "            h0, c0 = hidden\n",
    "\n",
    "        output_seq = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            input_t = x[:, t, :] # input at current timestep\n",
    "            # print(f'step:{t}/{seq_len}, hidden:')\n",
    "\n",
    "            new_h = []\n",
    "            new_c = []\n",
    "\n",
    "            for layer in range(self.num_layers):\n",
    "\n",
    "                # Get hidden state for current layer\n",
    "                h_t, c_t = self.lstm_cells[layer](input_t, (h0[layer], c0[layer]))\n",
    "\n",
    "                # Update hidden states for next timestep, for current layer\n",
    "                new_h.append(h_t)\n",
    "                new_c.append(c_t)\n",
    "                input_t = h_t # The output of current layer is input for the next\n",
    "\n",
    "            h0 = torch.stack(new_h)\n",
    "            c0 = torch.stack(new_c)\n",
    "            # Append hidden state at the topmost layer\n",
    "            output_seq.append(h_t.unsqueeze(1))\n",
    "\n",
    "        # Concatenate the outputs over the sequence length\n",
    "        output_seq = torch.cat(output_seq, dim=1) # output_seq is of shape (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return output_seq, (h0, c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py3iiomANCjY"
   },
   "source": [
    "Toy hybrid-quantum model for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "RRz02NuCLNvM"
   },
   "outputs": [],
   "source": [
    "class ToyHQModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_qubits, num_classes, lstm_layers):\n",
    "        super(ToyHQModel, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.num_layers = lstm_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        # self.conv = QuanConv2D(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.lstm = HQLSTM(embedding_dim, hidden_dim, num_qubits, lstm_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transform words to embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        # x = self.conv(x.unsqueeze(1))\n",
    "        # Pass embeddings to LSTM\n",
    "        out, hidden = self.lstm(embedded)\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(hidden[0], out)\n",
    "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
    "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
    "        # Classify the context vector\n",
    "        out = self.softmax(self.fc(context))\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "0V4YMT0JN6Sy"
   },
   "outputs": [],
   "source": [
    "class HybridQuantumModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_qubits, num_classes, lstm_layers):\n",
    "        super(HybridQuantumModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = HQLSTM(embedding_dim, hidden_dim, num_qubits, lstm_layers)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transform words to embeddings\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded = self.conv(embedded.unsqueeze(1)).to(device)\n",
    "        # Pass embeddings to LSTM\n",
    "        # out, hidden = self.lstm(embedded.reshape(-1, embedded.size(1)*embedded.size(2), embedded.size(-1)))\n",
    "        out, hidden = self.lstm(embedded)\n",
    "        # Calculate attention weights\n",
    "        attn_weights = self.attention(hidden[0], out)\n",
    "        # Calculate context vector by taking the weighted sum of LSTM outputs\n",
    "        context = attn_weights.unsqueeze(1).bmm(out).squeeze(1)\n",
    "        # Classify the context vector\n",
    "        out = self.softmax(self.fc(context))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvla_k_6IWAH"
   },
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "bbH3jVkhSWCr"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2 #We are dealing with a multiclass classification of 5 classes\n",
    "HIDDEN_DIM = 16 #number of neurons of the internal state (internal neural network in the LSTM)\n",
    "LSTM_LAYERS = 1 #Number of stacked LSTM layers\n",
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 5\n",
    "KERNEL_SIZE = 2\n",
    "STRIDE = 1\n",
    "PADDING = 1\n",
    "\n",
    "\n",
    "LR = 0.001 #Learning rate\n",
    "EPOCHS = 10 #Number of training epoch\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJnLZ5GCMMNF",
    "outputId": "6bbdfd11-9d94-4d5f-dd61-3d08452a43fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassicalModel(\n",
      "  (embedding): Embedding(12023, 200)\n",
      "  (conv): Conv2d(1, 5, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (lstm): LSTM(200, 16, batch_first=True)\n",
      "  (attention): Attention(\n",
      "    (attn): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (v): Linear(in_features=16, out_features=1, bias=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classical_model = ClassicalModel(VOCAB_SIZE, EMBEDDING_DIM, IN_CHANNELS, OUT_CHANNELS, KERNEL_SIZE, STRIDE, PADDING, HIDDEN_DIM, NUM_CLASSES, LSTM_LAYERS)\n",
    "classical_model = classical_model\n",
    "# classical_model = torch.compile(classical_model)\n",
    "\n",
    "cl_optimizer = torch.optim.AdamW(classical_model.parameters(), lr=LR, weight_decay = 5e-6)\n",
    "\n",
    "print(classical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "vWLWMuTwu-ZR"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'embedding' in name:\n",
    "            nn.init.uniform_(param, -0.1, 0.1)\n",
    "        elif 'lstm' in name:\n",
    "            if 'weight' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "        elif 'fc' in name:\n",
    "            nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yVpeUhBSeXi",
    "outputId": "82533239-fb6e-47c4-bd0f-4462701a9b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridQuantumModel(\n",
      "  (embedding): Embedding(12023, 200)\n",
      "  (lstm): HQLSTM(\n",
      "    (lstm_cells): ModuleList(\n",
      "      (0): HQLSTMCell()\n",
      "    )\n",
      "  )\n",
      "  (attention): Attention(\n",
      "    (attn): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (v): Linear(in_features=16, out_features=1, bias=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hybrid_quantum_model = HybridQuantumModel(VOCAB_SIZE, EMBEDDING_DIM, IN_CHANNELS, OUT_CHANNELS, KERNEL_SIZE, STRIDE, PADDING, HIDDEN_DIM, num_qubits, NUM_CLASSES, LSTM_LAYERS)\n",
    "# initialize_weights(hybrid_quantum_model)\n",
    "\n",
    "\n",
    "# hybrid_quantum_model = torch.compile(hybrid_quantum_model)\n",
    "\n",
    "hq_optimizer = torch.optim.AdamW(hybrid_quantum_model.parameters(), lr=LR, weight_decay = 5e-6)\n",
    "\n",
    "print(hybrid_quantum_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "ObgTsuW6kO5z"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, criterion, optimizer,\n",
    "          train_dataloader, test_dataloader, num_epochs):\n",
    "\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    test_losses = np.zeros(num_epochs)\n",
    "\n",
    "    train_accuracy_arr = np.zeros(num_epochs)\n",
    "    test_accuracy_arr = np.zeros(num_epochs)\n",
    "\n",
    "    for i_epoch in tqdm(range(num_epochs)):\n",
    "        it = 0\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        # train step\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            start_time = time.time()\n",
    "            X = batch[0]\n",
    "            y = batch[1]\n",
    "\n",
    "            h = model.init_hidden(y.size(0))\n",
    "            # model forward-pass\n",
    "            preds, h = model(X, h)\n",
    "\n",
    "            # model backward-pass\n",
    "            optimizer.zero_grad() # t.grad = torch.tensor([0., 0., 0.])\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step() # t = t - lr * t.grad\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "\n",
    "            # save loss and accuracy\n",
    "            train_loss += loss.detach().cpu().numpy()\n",
    "            # print(f'batch: {it+1}/{len(train_dataloader)}, loss: {train_loss/(it+1):.4f}, time: {execution_time:.4f}')\n",
    "            it += 1\n",
    "            train_accuracy += (preds.argmax(-1).detach() == y).cpu().numpy().mean()\n",
    "\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_accuracy /= len(train_dataloader)\n",
    "        train_losses[i_epoch] = train_loss\n",
    "        train_accuracy_arr[i_epoch] = train_accuracy\n",
    "\n",
    "        # test step\n",
    "        model.eval()\n",
    "        for batch in test_dataloader:\n",
    "            X = batch[0]\n",
    "            y = batch[1]\n",
    "\n",
    "            h = model.init_hidden(y.size(0))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # model forward-pass\n",
    "                preds, h = model(X, h)\n",
    "                loss = criterion(preds, y)\n",
    "\n",
    "                # save loss and accuracy\n",
    "                test_loss += loss.detach().cpu().numpy()\n",
    "                test_accuracy += (preds.argmax(-1) == y).cpu().numpy().mean()\n",
    "\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_accuracy /= len(test_dataloader)\n",
    "\n",
    "        test_losses[i_epoch] = test_loss\n",
    "        test_accuracy_arr[i_epoch] = test_accuracy\n",
    "\n",
    "    return train_losses, test_losses, train_accuracy_arr, test_accuracy_arr\n",
    "\n",
    "def train_for_hqnn(model, criterion, optimizer,\n",
    "          train_dataloader, test_dataloader, num_epochs):\n",
    "\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    test_losses = np.zeros(num_epochs)\n",
    "\n",
    "    train_accuracy_arr = np.zeros(num_epochs)\n",
    "    test_accuracy_arr = np.zeros(num_epochs)\n",
    "\n",
    "    for i_epoch in tqdm(range(num_epochs)):\n",
    "        it = 0\n",
    "\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        train_accuracy = 0\n",
    "        test_accuracy = 0\n",
    "\n",
    "        # train step\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            start_time = time.time()\n",
    "            X = batch[0]\n",
    "            y = batch[1]\n",
    "\n",
    "            # model forward-pass\n",
    "            preds = model(X)\n",
    "\n",
    "            # model backward-pass\n",
    "            optimizer.zero_grad() # t.grad = torch.tensor([0., 0., 0.])\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step() # t = t - lr * t.grad\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "\n",
    "            # save loss and accuracy\n",
    "            train_loss += loss.detach().cpu().numpy()\n",
    "            print(f'it: {it+1}/{len(train_dataloader)}, loss: {train_loss/(it+1):.4f}, time: {execution_time:.4f}')\n",
    "            it += 1\n",
    "            train_accuracy += (preds.argmax(-1).detach() == y).cpu().numpy().mean()\n",
    "\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_accuracy /= len(train_dataloader)\n",
    "        train_losses[i_epoch] = train_loss\n",
    "\n",
    "        train_accuracy_arr[i_epoch] = train_accuracy\n",
    "\n",
    "        # test step\n",
    "        model.eval()\n",
    "        for batch in test_dataloader:\n",
    "            X = batch[0]\n",
    "            y = batch[1]\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # model forward-pass\n",
    "                preds = model(X)\n",
    "                loss = criterion(preds, y)\n",
    "\n",
    "                # save loss and accuracy\n",
    "                test_loss += loss.detach().cpu().numpy()\n",
    "                test_accuracy += (preds.argmax(-1) == y).cpu().numpy().mean()\n",
    "\n",
    "\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_accuracy /= len(test_dataloader)\n",
    "\n",
    "        test_losses[i_epoch] = test_loss\n",
    "        test_accuracy_arr[i_epoch] = test_accuracy\n",
    "\n",
    "    return train_losses, test_losses, train_accuracy_arr, test_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "661cd90b76c9451f98e0ca29c4b22d44",
      "d702bf265eca4bbbbcb7a358df381c65",
      "fc30a6a4b6064be0b3c4a786a4b64952",
      "df78c6ab42474534a547a0a7ba44b754",
      "d49849d9283546c9b34517f327232757",
      "8562c9e4bc8443db82350ff5f496958b",
      "e17bf6667ccd4167af9110a09ba012ac",
      "60ed458c82de4401a7eb400c23f1338d",
      "673f3f8df9244b538ad88f1fc82442b6",
      "029421ef2fec489b8b379351e00e188a",
      "98cad26d7f274b71a04d37fca89196bf"
     ]
    },
    "collapsed": true,
    "id": "1swJ9EmxZKAT",
    "outputId": "17589abb-15b9-4765-8297-2967fc10a0b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b8a4d9607d4a518b8e1184a680aed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cl_train_losses, \\\n",
    "    cl_test_losses, \\\n",
    "    cl_train_accuracy_arr, \\\n",
    "    cl_test_accuracy_arr = train(classical_model, criterion=criterion,\n",
    "                              optimizer=cl_optimizer,\n",
    "                              train_dataloader=train_loader,\n",
    "                              test_dataloader=test_loader,\n",
    "                              num_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "1SQ4xsOVd0bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.86555233, 0.87936047, 0.88081395, 0.88008721, 0.8684593 ,\n",
       "        0.87063953, 0.87718023, 0.87645349, 0.87936047, 0.87863372], requires_grad=True)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_test_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.77765805, 0.8908046 , 0.93372845, 0.96138649, 0.97755029,\n",
       "        0.99030172, 0.99425287, 0.99748563, 0.99874282, 0.99928161], requires_grad=True)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_train_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "7f2f4512c03d4d6d86d51f2f8a3a5fed",
      "22607ed241a74d4f9dc9d471f18e1690",
      "d5590ae5cbe246c8a3b97a1794582dec",
      "889ece199b2a469388992df645c9977e",
      "97def7d09b1a4359bda9237e1a46d28c",
      "b9b9d71402eb46b595cfc84dd522acfb",
      "8f43314093db461f8f90e91e48dbd0da",
      "d623bf979daa40e8af2598e247099b4e",
      "c6fd02082a4342fbb9b9da6449325cfb",
      "d788e171ee934b0d9275fcb6c88d69c8",
      "8ee7f59198f44ac6af6a747b4d7c505a"
     ]
    },
    "id": "2CqcSaKTTMhJ",
    "outputId": "30bdea22-b24c-40d6-f4e4-f220e5e04264"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb639ff7d4f64a81b44fe71aa31806c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x00000274F190AF20>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\den1s\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py\", line 1870, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\den1s\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py\", line 1892, in close\n",
      "    self._fpclose(fp)\n",
      "  File \"c:\\Users\\den1s\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\zipfile.py\", line 1989, in _fpclose\n",
      "    assert self._fileRefCnt > 0\n",
      "AssertionError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 1/174, loss: 0.7030, time: 21.4352\n",
      "it: 2/174, loss: 0.6941, time: 29.9690\n",
      "it: 3/174, loss: 0.7117, time: 36.0505\n",
      "it: 4/174, loss: 0.7072, time: 24.1602\n",
      "it: 5/174, loss: 0.7097, time: 20.0513\n",
      "it: 6/174, loss: 0.6997, time: 20.1978\n",
      "it: 7/174, loss: 0.7013, time: 19.9642\n",
      "it: 8/174, loss: 0.6970, time: 20.0021\n",
      "it: 9/174, loss: 0.6937, time: 20.1423\n",
      "it: 10/174, loss: 0.6953, time: 19.8578\n",
      "it: 11/174, loss: 0.6974, time: 19.8952\n",
      "it: 12/174, loss: 0.6913, time: 19.8230\n",
      "it: 13/174, loss: 0.6913, time: 19.8837\n",
      "it: 14/174, loss: 0.6890, time: 20.0597\n",
      "it: 15/174, loss: 0.6908, time: 20.0779\n",
      "it: 16/174, loss: 0.6903, time: 20.0972\n",
      "it: 17/174, loss: 0.6908, time: 19.9998\n",
      "it: 18/174, loss: 0.6917, time: 19.9707\n",
      "it: 19/174, loss: 0.6916, time: 20.3217\n",
      "it: 20/174, loss: 0.6911, time: 20.5164\n",
      "it: 21/174, loss: 0.6907, time: 20.0972\n",
      "it: 22/174, loss: 0.6924, time: 20.2130\n",
      "it: 23/174, loss: 0.6933, time: 19.9313\n",
      "it: 24/174, loss: 0.6924, time: 20.1199\n",
      "it: 25/174, loss: 0.6937, time: 20.1362\n",
      "it: 26/174, loss: 0.6950, time: 20.1066\n",
      "it: 27/174, loss: 0.6951, time: 20.0079\n",
      "it: 28/174, loss: 0.6943, time: 19.9000\n",
      "it: 29/174, loss: 0.6934, time: 19.9490\n",
      "it: 30/174, loss: 0.6946, time: 20.0270\n",
      "it: 31/174, loss: 0.6941, time: 20.0574\n",
      "it: 32/174, loss: 0.6932, time: 20.1051\n",
      "it: 33/174, loss: 0.6941, time: 20.0167\n",
      "it: 34/174, loss: 0.6948, time: 20.0671\n",
      "it: 35/174, loss: 0.6947, time: 20.5827\n",
      "it: 36/174, loss: 0.6935, time: 19.9993\n",
      "it: 37/174, loss: 0.6937, time: 20.2966\n",
      "it: 38/174, loss: 0.6929, time: 20.1332\n",
      "it: 39/174, loss: 0.6931, time: 20.0912\n",
      "it: 40/174, loss: 0.6940, time: 20.5294\n",
      "it: 41/174, loss: 0.6943, time: 20.2590\n",
      "it: 42/174, loss: 0.6939, time: 20.1970\n",
      "it: 43/174, loss: 0.6931, time: 20.1205\n",
      "it: 44/174, loss: 0.6932, time: 20.1553\n",
      "it: 45/174, loss: 0.6927, time: 20.1927\n",
      "it: 46/174, loss: 0.6928, time: 19.9307\n",
      "it: 47/174, loss: 0.6930, time: 19.9664\n",
      "it: 48/174, loss: 0.6926, time: 20.0304\n",
      "it: 49/174, loss: 0.6924, time: 20.6009\n",
      "it: 50/174, loss: 0.6918, time: 20.2208\n",
      "it: 51/174, loss: 0.6917, time: 20.6251\n",
      "it: 52/174, loss: 0.6914, time: 19.9192\n",
      "it: 53/174, loss: 0.6912, time: 19.8961\n",
      "it: 54/174, loss: 0.6906, time: 19.9430\n",
      "it: 55/174, loss: 0.6912, time: 20.0833\n",
      "it: 56/174, loss: 0.6908, time: 20.0555\n",
      "it: 57/174, loss: 0.6904, time: 20.2265\n",
      "it: 58/174, loss: 0.6905, time: 19.9745\n",
      "it: 59/174, loss: 0.6903, time: 20.2296\n",
      "it: 60/174, loss: 0.6899, time: 20.0386\n",
      "it: 61/174, loss: 0.6895, time: 20.0176\n",
      "it: 62/174, loss: 0.6890, time: 19.7534\n",
      "it: 63/174, loss: 0.6888, time: 20.0218\n",
      "it: 64/174, loss: 0.6882, time: 19.9051\n",
      "it: 65/174, loss: 0.6878, time: 20.3390\n",
      "it: 66/174, loss: 0.6877, time: 20.0863\n",
      "it: 67/174, loss: 0.6876, time: 19.9834\n",
      "it: 68/174, loss: 0.6872, time: 20.0264\n",
      "it: 69/174, loss: 0.6869, time: 20.0447\n",
      "it: 70/174, loss: 0.6868, time: 19.9233\n",
      "it: 71/174, loss: 0.6868, time: 20.1308\n",
      "it: 72/174, loss: 0.6866, time: 20.0959\n",
      "it: 73/174, loss: 0.6864, time: 20.1459\n",
      "it: 74/174, loss: 0.6864, time: 20.1512\n",
      "it: 75/174, loss: 0.6859, time: 20.0940\n",
      "it: 76/174, loss: 0.6856, time: 20.0194\n",
      "it: 77/174, loss: 0.6852, time: 19.9993\n",
      "it: 78/174, loss: 0.6848, time: 19.9285\n",
      "it: 79/174, loss: 0.6854, time: 19.8450\n",
      "it: 80/174, loss: 0.6857, time: 19.8201\n",
      "it: 81/174, loss: 0.6863, time: 20.0051\n",
      "it: 82/174, loss: 0.6874, time: 20.1983\n",
      "it: 83/174, loss: 0.6874, time: 20.0694\n",
      "it: 84/174, loss: 0.6880, time: 20.1302\n",
      "it: 85/174, loss: 0.6883, time: 20.0898\n",
      "it: 86/174, loss: 0.6883, time: 20.1181\n",
      "it: 87/174, loss: 0.6884, time: 19.9253\n",
      "it: 88/174, loss: 0.6884, time: 19.9531\n",
      "it: 89/174, loss: 0.6887, time: 20.0625\n",
      "it: 90/174, loss: 0.6888, time: 20.0559\n",
      "it: 91/174, loss: 0.6885, time: 20.0129\n",
      "it: 92/174, loss: 0.6882, time: 20.0988\n",
      "it: 93/174, loss: 0.6879, time: 20.1386\n",
      "it: 94/174, loss: 0.6875, time: 20.2114\n",
      "it: 95/174, loss: 0.6872, time: 20.1284\n",
      "it: 96/174, loss: 0.6872, time: 20.3714\n",
      "it: 97/174, loss: 0.6869, time: 19.7552\n",
      "it: 98/174, loss: 0.6867, time: 20.0224\n",
      "it: 99/174, loss: 0.6865, time: 19.9465\n",
      "it: 100/174, loss: 0.6863, time: 20.0378\n",
      "it: 101/174, loss: 0.6860, time: 20.1828\n",
      "it: 102/174, loss: 0.6858, time: 20.2407\n",
      "it: 103/174, loss: 0.6855, time: 20.1859\n",
      "it: 104/174, loss: 0.6850, time: 20.2895\n",
      "it: 105/174, loss: 0.6845, time: 19.8615\n",
      "it: 106/174, loss: 0.6843, time: 20.1056\n",
      "it: 107/174, loss: 0.6839, time: 20.2624\n",
      "it: 108/174, loss: 0.6837, time: 20.1122\n",
      "it: 109/174, loss: 0.6835, time: 20.1571\n",
      "it: 110/174, loss: 0.6830, time: 20.2747\n",
      "it: 111/174, loss: 0.6827, time: 20.3139\n",
      "it: 112/174, loss: 0.6824, time: 20.2172\n",
      "it: 113/174, loss: 0.6821, time: 20.0650\n",
      "it: 114/174, loss: 0.6816, time: 19.8722\n",
      "it: 115/174, loss: 0.6813, time: 19.9885\n",
      "it: 116/174, loss: 0.6810, time: 20.0385\n",
      "it: 117/174, loss: 0.6809, time: 20.1270\n",
      "it: 118/174, loss: 0.6808, time: 19.9872\n",
      "it: 119/174, loss: 0.6805, time: 19.9737\n",
      "it: 120/174, loss: 0.6800, time: 20.1066\n",
      "it: 121/174, loss: 0.6798, time: 20.1573\n",
      "it: 122/174, loss: 0.6794, time: 19.9665\n",
      "it: 123/174, loss: 0.6791, time: 20.0118\n",
      "it: 124/174, loss: 0.6789, time: 20.1760\n",
      "it: 125/174, loss: 0.6787, time: 20.3898\n",
      "it: 126/174, loss: 0.6785, time: 20.2096\n",
      "it: 127/174, loss: 0.6783, time: 19.9709\n",
      "it: 128/174, loss: 0.6781, time: 20.0625\n",
      "it: 129/174, loss: 0.6778, time: 19.9701\n",
      "it: 130/174, loss: 0.6774, time: 20.0278\n",
      "it: 131/174, loss: 0.6771, time: 20.0915\n",
      "it: 132/174, loss: 0.6768, time: 19.7686\n",
      "it: 133/174, loss: 0.6764, time: 19.8683\n",
      "it: 134/174, loss: 0.6762, time: 20.1396\n",
      "it: 135/174, loss: 0.6757, time: 20.0015\n",
      "it: 136/174, loss: 0.6753, time: 19.9801\n",
      "it: 137/174, loss: 0.6749, time: 20.0736\n",
      "it: 138/174, loss: 0.6743, time: 20.0484\n",
      "it: 139/174, loss: 0.6740, time: 19.9565\n",
      "it: 140/174, loss: 0.6737, time: 20.0100\n",
      "it: 141/174, loss: 0.6732, time: 19.9858\n",
      "it: 142/174, loss: 0.6727, time: 20.0582\n",
      "it: 143/174, loss: 0.6721, time: 20.0782\n",
      "it: 144/174, loss: 0.6719, time: 20.2559\n",
      "it: 145/174, loss: 0.6716, time: 20.1691\n",
      "it: 146/174, loss: 0.6709, time: 20.2275\n",
      "it: 147/174, loss: 0.6705, time: 20.2672\n",
      "it: 148/174, loss: 0.6702, time: 19.9955\n",
      "it: 149/174, loss: 0.6697, time: 19.9207\n",
      "it: 150/174, loss: 0.6692, time: 19.8209\n",
      "it: 151/174, loss: 0.6688, time: 19.8198\n",
      "it: 152/174, loss: 0.6685, time: 20.0849\n",
      "it: 153/174, loss: 0.6679, time: 20.0433\n",
      "it: 154/174, loss: 0.6673, time: 20.0116\n",
      "it: 155/174, loss: 0.6668, time: 20.1092\n",
      "it: 156/174, loss: 0.6663, time: 20.1590\n",
      "it: 157/174, loss: 0.6656, time: 20.2126\n",
      "it: 158/174, loss: 0.6649, time: 19.7710\n",
      "it: 159/174, loss: 0.6645, time: 20.0595\n",
      "it: 160/174, loss: 0.6643, time: 20.0275\n",
      "it: 161/174, loss: 0.6639, time: 20.4259\n",
      "it: 162/174, loss: 0.6634, time: 20.1624\n",
      "it: 163/174, loss: 0.6627, time: 20.2217\n",
      "it: 164/174, loss: 0.6622, time: 20.0894\n",
      "it: 165/174, loss: 0.6617, time: 20.2859\n",
      "it: 166/174, loss: 0.6611, time: 19.9659\n",
      "it: 167/174, loss: 0.6606, time: 19.9545\n",
      "it: 168/174, loss: 0.6603, time: 19.8993\n",
      "it: 169/174, loss: 0.6598, time: 19.9134\n",
      "it: 170/174, loss: 0.6596, time: 20.0317\n",
      "it: 171/174, loss: 0.6591, time: 20.1822\n",
      "it: 172/174, loss: 0.6585, time: 20.1263\n",
      "it: 173/174, loss: 0.6581, time: 20.0547\n",
      "it: 174/174, loss: 0.6579, time: 20.0375\n",
      "it: 1/174, loss: 0.6340, time: 19.1750\n",
      "it: 2/174, loss: 0.6026, time: 19.8661\n",
      "it: 3/174, loss: 0.5900, time: 19.9304\n",
      "it: 4/174, loss: 0.5813, time: 19.9146\n",
      "it: 5/174, loss: 0.5833, time: 19.8101\n",
      "it: 6/174, loss: 0.5890, time: 19.8501\n",
      "it: 7/174, loss: 0.5887, time: 19.7910\n",
      "it: 8/174, loss: 0.5851, time: 19.7614\n",
      "it: 9/174, loss: 0.5806, time: 19.5814\n",
      "it: 10/174, loss: 0.5762, time: 19.6773\n",
      "it: 11/174, loss: 0.5719, time: 19.5637\n",
      "it: 12/174, loss: 0.5691, time: 19.7612\n",
      "it: 13/174, loss: 0.5658, time: 19.9180\n",
      "it: 14/174, loss: 0.5667, time: 19.9844\n",
      "it: 15/174, loss: 0.5632, time: 19.9743\n",
      "it: 16/174, loss: 0.5609, time: 20.0955\n",
      "it: 17/174, loss: 0.5569, time: 19.8437\n",
      "it: 18/174, loss: 0.5503, time: 19.7139\n",
      "it: 19/174, loss: 0.5482, time: 19.7349\n",
      "it: 20/174, loss: 0.5469, time: 20.0080\n",
      "it: 21/174, loss: 0.5453, time: 19.9444\n",
      "it: 22/174, loss: 0.5449, time: 20.0471\n",
      "it: 23/174, loss: 0.5443, time: 20.0286\n",
      "it: 24/174, loss: 0.5428, time: 19.9296\n",
      "it: 25/174, loss: 0.5413, time: 20.0546\n",
      "it: 26/174, loss: 0.5403, time: 19.7850\n",
      "it: 27/174, loss: 0.5383, time: 19.8437\n",
      "it: 28/174, loss: 0.5394, time: 19.7819\n",
      "it: 29/174, loss: 0.5383, time: 19.9243\n",
      "it: 30/174, loss: 0.5358, time: 19.7854\n",
      "it: 31/174, loss: 0.5350, time: 19.9979\n",
      "it: 32/174, loss: 0.5323, time: 19.8585\n",
      "it: 33/174, loss: 0.5327, time: 19.9517\n",
      "it: 34/174, loss: 0.5323, time: 20.0028\n",
      "it: 35/174, loss: 0.5326, time: 19.9541\n",
      "it: 36/174, loss: 0.5309, time: 19.7609\n",
      "it: 37/174, loss: 0.5320, time: 20.0098\n",
      "it: 38/174, loss: 0.5301, time: 20.1051\n",
      "it: 39/174, loss: 0.5296, time: 19.9562\n",
      "it: 40/174, loss: 0.5293, time: 19.9560\n",
      "it: 41/174, loss: 0.5268, time: 20.3622\n",
      "it: 42/174, loss: 0.5246, time: 20.1414\n",
      "it: 43/174, loss: 0.5245, time: 20.0149\n",
      "it: 44/174, loss: 0.5239, time: 19.9096\n",
      "it: 45/174, loss: 0.5232, time: 19.7926\n",
      "it: 46/174, loss: 0.5229, time: 19.7290\n",
      "it: 47/174, loss: 0.5216, time: 19.9200\n",
      "it: 48/174, loss: 0.5230, time: 19.9376\n",
      "it: 49/174, loss: 0.5215, time: 20.0260\n",
      "it: 50/174, loss: 0.5200, time: 20.0094\n",
      "it: 51/174, loss: 0.5187, time: 20.0261\n",
      "it: 52/174, loss: 0.5181, time: 19.8613\n",
      "it: 53/174, loss: 0.5177, time: 19.8517\n",
      "it: 54/174, loss: 0.5156, time: 19.8020\n",
      "it: 55/174, loss: 0.5143, time: 20.0378\n",
      "it: 56/174, loss: 0.5137, time: 19.9319\n",
      "it: 57/174, loss: 0.5137, time: 19.8869\n",
      "it: 58/174, loss: 0.5121, time: 19.9356\n",
      "it: 59/174, loss: 0.5115, time: 20.1428\n",
      "it: 60/174, loss: 0.5107, time: 19.8634\n",
      "it: 61/174, loss: 0.5101, time: 20.0037\n",
      "it: 62/174, loss: 0.5092, time: 19.8901\n",
      "it: 63/174, loss: 0.5079, time: 19.7858\n",
      "it: 64/174, loss: 0.5055, time: 19.8355\n",
      "it: 65/174, loss: 0.5056, time: 19.8860\n",
      "it: 66/174, loss: 0.5050, time: 20.7714\n",
      "it: 67/174, loss: 0.5047, time: 20.0651\n",
      "it: 68/174, loss: 0.5028, time: 20.1328\n",
      "it: 69/174, loss: 0.5030, time: 20.1720\n",
      "it: 70/174, loss: 0.5011, time: 19.4249\n",
      "it: 71/174, loss: 0.5002, time: 20.4519\n",
      "it: 72/174, loss: 0.4985, time: 19.9789\n",
      "it: 73/174, loss: 0.4967, time: 19.2424\n",
      "it: 74/174, loss: 0.4974, time: 20.7960\n",
      "it: 75/174, loss: 0.4953, time: 19.9343\n",
      "it: 76/174, loss: 0.4935, time: 19.3248\n",
      "it: 77/174, loss: 0.4923, time: 20.7125\n",
      "it: 78/174, loss: 0.4902, time: 19.4202\n",
      "it: 79/174, loss: 0.4898, time: 20.6688\n",
      "it: 80/174, loss: 0.4878, time: 19.8048\n",
      "it: 81/174, loss: 0.4867, time: 19.7716\n",
      "it: 82/174, loss: 0.4846, time: 19.9679\n",
      "it: 83/174, loss: 0.4837, time: 19.2231\n",
      "it: 84/174, loss: 0.4828, time: 20.6088\n",
      "it: 85/174, loss: 0.4843, time: 20.1180\n",
      "it: 86/174, loss: 0.4828, time: 20.2595\n",
      "it: 87/174, loss: 0.4827, time: 19.9180\n",
      "it: 88/174, loss: 0.4835, time: 20.0338\n",
      "it: 89/174, loss: 0.4820, time: 19.9012\n",
      "it: 90/174, loss: 0.4818, time: 20.0408\n",
      "it: 91/174, loss: 0.4809, time: 19.9701\n",
      "it: 92/174, loss: 0.4801, time: 20.2378\n",
      "it: 93/174, loss: 0.4789, time: 20.0996\n",
      "it: 94/174, loss: 0.4800, time: 20.2900\n",
      "it: 95/174, loss: 0.4785, time: 20.3501\n",
      "it: 96/174, loss: 0.4795, time: 20.1578\n",
      "it: 97/174, loss: 0.4804, time: 19.8718\n",
      "it: 98/174, loss: 0.4825, time: 19.8564\n",
      "it: 99/174, loss: 0.4820, time: 19.7266\n",
      "it: 100/174, loss: 0.4804, time: 20.0212\n",
      "it: 101/174, loss: 0.4815, time: 20.0466\n",
      "it: 102/174, loss: 0.4819, time: 19.9851\n",
      "it: 103/174, loss: 0.4823, time: 19.9913\n",
      "it: 104/174, loss: 0.4805, time: 20.3350\n",
      "it: 105/174, loss: 0.4791, time: 20.0272\n",
      "it: 106/174, loss: 0.4791, time: 19.9707\n",
      "it: 107/174, loss: 0.4783, time: 19.8858\n",
      "it: 108/174, loss: 0.4784, time: 20.0191\n",
      "it: 109/174, loss: 0.4773, time: 20.0130\n",
      "it: 110/174, loss: 0.4771, time: 20.1654\n",
      "it: 111/174, loss: 0.4773, time: 20.1627\n",
      "it: 112/174, loss: 0.4777, time: 20.0359\n",
      "it: 113/174, loss: 0.4763, time: 20.1837\n",
      "it: 114/174, loss: 0.4766, time: 19.8259\n",
      "it: 115/174, loss: 0.4757, time: 20.3994\n",
      "it: 116/174, loss: 0.4746, time: 21.3152\n",
      "it: 117/174, loss: 0.4763, time: 21.6275\n",
      "it: 118/174, loss: 0.4763, time: 21.4635\n",
      "it: 119/174, loss: 0.4757, time: 23.5315\n",
      "it: 120/174, loss: 0.4745, time: 30.0032\n",
      "it: 121/174, loss: 0.4748, time: 24.0848\n",
      "it: 122/174, loss: 0.4740, time: 20.2681\n",
      "it: 123/174, loss: 0.4742, time: 19.8053\n",
      "it: 124/174, loss: 0.4732, time: 20.1278\n",
      "it: 125/174, loss: 0.4729, time: 20.1227\n",
      "it: 126/174, loss: 0.4737, time: 20.0753\n",
      "it: 127/174, loss: 0.4743, time: 20.0580\n",
      "it: 128/174, loss: 0.4732, time: 20.0410\n",
      "it: 129/174, loss: 0.4725, time: 19.8989\n",
      "it: 130/174, loss: 0.4719, time: 20.1682\n",
      "it: 131/174, loss: 0.4709, time: 19.8816\n",
      "it: 132/174, loss: 0.4710, time: 19.9198\n",
      "it: 133/174, loss: 0.4703, time: 20.0040\n",
      "it: 134/174, loss: 0.4706, time: 19.8921\n",
      "it: 135/174, loss: 0.4699, time: 19.9675\n",
      "it: 136/174, loss: 0.4692, time: 20.1055\n",
      "it: 137/174, loss: 0.4688, time: 19.8872\n",
      "it: 138/174, loss: 0.4684, time: 20.0904\n",
      "it: 139/174, loss: 0.4678, time: 20.0935\n",
      "it: 140/174, loss: 0.4673, time: 20.5483\n",
      "it: 141/174, loss: 0.4662, time: 19.2581\n",
      "it: 142/174, loss: 0.4660, time: 20.5490\n",
      "it: 143/174, loss: 0.4657, time: 20.0095\n",
      "it: 144/174, loss: 0.4646, time: 19.3202\n",
      "it: 145/174, loss: 0.4652, time: 20.7958\n",
      "it: 146/174, loss: 0.4650, time: 19.3000\n",
      "it: 147/174, loss: 0.4649, time: 20.8281\n",
      "it: 148/174, loss: 0.4636, time: 19.8156\n",
      "it: 149/174, loss: 0.4629, time: 20.0836\n",
      "it: 150/174, loss: 0.4631, time: 19.9606\n",
      "it: 151/174, loss: 0.4619, time: 19.0739\n",
      "it: 152/174, loss: 0.4615, time: 20.6408\n",
      "it: 153/174, loss: 0.4610, time: 20.0370\n",
      "it: 154/174, loss: 0.4605, time: 19.2730\n",
      "it: 155/174, loss: 0.4608, time: 20.7875\n",
      "it: 156/174, loss: 0.4602, time: 19.9640\n",
      "it: 157/174, loss: 0.4598, time: 19.2164\n",
      "it: 158/174, loss: 0.4590, time: 20.5412\n",
      "it: 159/174, loss: 0.4587, time: 19.3643\n",
      "it: 160/174, loss: 0.4582, time: 20.5623\n",
      "it: 161/174, loss: 0.4572, time: 19.9481\n",
      "it: 162/174, loss: 0.4565, time: 20.0870\n",
      "it: 163/174, loss: 0.4562, time: 20.0889\n",
      "it: 164/174, loss: 0.4553, time: 19.2865\n",
      "it: 165/174, loss: 0.4552, time: 20.7196\n",
      "it: 166/174, loss: 0.4555, time: 19.8553\n",
      "it: 167/174, loss: 0.4549, time: 19.1775\n",
      "it: 168/174, loss: 0.4547, time: 20.5433\n",
      "it: 169/174, loss: 0.4545, time: 19.7123\n",
      "it: 170/174, loss: 0.4550, time: 19.3177\n",
      "it: 171/174, loss: 0.4542, time: 20.5310\n",
      "it: 172/174, loss: 0.4545, time: 19.3099\n",
      "it: 173/174, loss: 0.4548, time: 20.6392\n",
      "it: 174/174, loss: 0.4547, time: 19.8894\n",
      "it: 1/174, loss: 0.3168, time: 19.3361\n",
      "it: 2/174, loss: 0.3974, time: 19.5166\n",
      "it: 3/174, loss: 0.3969, time: 19.6668\n",
      "it: 4/174, loss: 0.3960, time: 19.4817\n",
      "it: 5/174, loss: 0.4018, time: 19.6943\n",
      "it: 6/174, loss: 0.4082, time: 19.7420\n",
      "it: 7/174, loss: 0.4100, time: 19.8069\n",
      "it: 8/174, loss: 0.4055, time: 19.4973\n",
      "it: 9/174, loss: 0.4033, time: 19.6096\n",
      "it: 10/174, loss: 0.4006, time: 19.6965\n",
      "it: 11/174, loss: 0.3927, time: 18.9347\n",
      "it: 12/174, loss: 0.3943, time: 20.2494\n",
      "it: 13/174, loss: 0.3903, time: 19.9242\n",
      "it: 14/174, loss: 0.3843, time: 18.9544\n",
      "it: 15/174, loss: 0.3777, time: 20.4923\n",
      "it: 16/174, loss: 0.3931, time: 19.7802\n",
      "it: 17/174, loss: 0.3890, time: 19.3383\n",
      "it: 18/174, loss: 0.3885, time: 20.3360\n",
      "it: 19/174, loss: 0.3902, time: 18.9936\n",
      "it: 20/174, loss: 0.3980, time: 20.3489\n",
      "it: 21/174, loss: 0.3941, time: 19.7529\n",
      "it: 22/174, loss: 0.3938, time: 19.7703\n",
      "it: 23/174, loss: 0.3929, time: 19.8028\n",
      "it: 24/174, loss: 0.3879, time: 18.9791\n",
      "it: 25/174, loss: 0.3874, time: 20.4336\n",
      "it: 26/174, loss: 0.3851, time: 19.6925\n",
      "it: 27/174, loss: 0.3855, time: 19.1224\n",
      "it: 28/174, loss: 0.3882, time: 20.4184\n",
      "it: 29/174, loss: 0.3873, time: 19.7188\n",
      "it: 30/174, loss: 0.3852, time: 19.1073\n",
      "it: 31/174, loss: 0.3843, time: 20.5143\n",
      "it: 32/174, loss: 0.3892, time: 19.0881\n",
      "it: 33/174, loss: 0.3875, time: 20.5879\n",
      "it: 34/174, loss: 0.3861, time: 19.7412\n",
      "it: 35/174, loss: 0.3857, time: 19.9053\n",
      "it: 36/174, loss: 0.3841, time: 19.7945\n",
      "it: 37/174, loss: 0.3843, time: 19.2086\n",
      "it: 38/174, loss: 0.3883, time: 20.4811\n",
      "it: 39/174, loss: 0.3866, time: 19.8692\n",
      "it: 40/174, loss: 0.3863, time: 19.0529\n",
      "it: 41/174, loss: 0.3838, time: 20.4614\n",
      "it: 42/174, loss: 0.3846, time: 19.9710\n",
      "it: 43/174, loss: 0.3874, time: 19.2404\n",
      "it: 44/174, loss: 0.3909, time: 20.4590\n",
      "it: 45/174, loss: 0.3888, time: 19.0806\n",
      "it: 46/174, loss: 0.3923, time: 20.3163\n",
      "it: 47/174, loss: 0.3898, time: 19.7952\n",
      "it: 48/174, loss: 0.3896, time: 19.8021\n",
      "it: 49/174, loss: 0.3884, time: 20.0027\n",
      "it: 50/174, loss: 0.3863, time: 19.0813\n",
      "it: 51/174, loss: 0.3838, time: 20.7796\n",
      "it: 52/174, loss: 0.3851, time: 20.0180\n",
      "it: 53/174, loss: 0.3834, time: 19.3060\n",
      "it: 54/174, loss: 0.3847, time: 20.4659\n",
      "it: 55/174, loss: 0.3861, time: 19.9225\n",
      "it: 56/174, loss: 0.3871, time: 19.1210\n",
      "it: 57/174, loss: 0.3866, time: 20.6212\n",
      "it: 58/174, loss: 0.3869, time: 19.3573\n",
      "it: 59/174, loss: 0.3862, time: 20.5557\n",
      "it: 60/174, loss: 0.3881, time: 19.9702\n",
      "it: 61/174, loss: 0.3881, time: 19.9236\n",
      "it: 62/174, loss: 0.3865, time: 19.9848\n",
      "it: 63/174, loss: 0.3886, time: 19.1534\n",
      "it: 64/174, loss: 0.3886, time: 20.7065\n",
      "it: 65/174, loss: 0.3907, time: 19.7660\n",
      "it: 66/174, loss: 0.3889, time: 19.2897\n",
      "it: 67/174, loss: 0.3889, time: 20.4616\n",
      "it: 68/174, loss: 0.3904, time: 19.9900\n",
      "it: 69/174, loss: 0.3887, time: 19.2313\n",
      "it: 70/174, loss: 0.3900, time: 20.6165\n",
      "it: 71/174, loss: 0.3893, time: 19.1654\n",
      "it: 72/174, loss: 0.3904, time: 20.3936\n",
      "it: 73/174, loss: 0.3921, time: 19.8640\n",
      "it: 74/174, loss: 0.3924, time: 20.0052\n",
      "it: 75/174, loss: 0.3922, time: 20.1343\n",
      "it: 76/174, loss: 0.3927, time: 19.8723\n",
      "it: 77/174, loss: 0.3936, time: 20.6208\n",
      "it: 78/174, loss: 0.3924, time: 19.9295\n",
      "it: 79/174, loss: 0.3937, time: 19.1605\n",
      "it: 80/174, loss: 0.3942, time: 20.4142\n",
      "it: 81/174, loss: 0.3929, time: 19.7039\n",
      "it: 82/174, loss: 0.3954, time: 19.0103\n",
      "it: 83/174, loss: 0.3935, time: 20.6060\n",
      "it: 84/174, loss: 0.3937, time: 19.2901\n",
      "it: 85/174, loss: 0.3943, time: 20.6169\n",
      "it: 86/174, loss: 0.3937, time: 20.0740\n",
      "it: 87/174, loss: 0.3913, time: 20.0892\n",
      "it: 88/174, loss: 0.3920, time: 20.1999\n",
      "it: 89/174, loss: 0.3934, time: 19.2507\n",
      "it: 90/174, loss: 0.3926, time: 20.5443\n",
      "it: 91/174, loss: 0.3904, time: 19.7956\n",
      "it: 92/174, loss: 0.3899, time: 19.2845\n",
      "it: 93/174, loss: 0.3888, time: 20.6100\n",
      "it: 94/174, loss: 0.3879, time: 20.0235\n",
      "it: 95/174, loss: 0.3876, time: 19.3129\n",
      "it: 96/174, loss: 0.3867, time: 20.7009\n",
      "it: 97/174, loss: 0.3868, time: 19.2271\n",
      "it: 98/174, loss: 0.3854, time: 20.6083\n",
      "it: 99/174, loss: 0.3851, time: 19.7277\n",
      "it: 100/174, loss: 0.3852, time: 20.0191\n",
      "it: 101/174, loss: 0.3861, time: 19.8208\n",
      "it: 102/174, loss: 0.3882, time: 19.3410\n",
      "it: 103/174, loss: 0.3879, time: 20.6794\n",
      "it: 104/174, loss: 0.3887, time: 20.1061\n",
      "it: 105/174, loss: 0.3885, time: 19.3732\n",
      "it: 106/174, loss: 0.3916, time: 20.7800\n",
      "it: 107/174, loss: 0.3909, time: 19.8691\n",
      "it: 108/174, loss: 0.3899, time: 19.1475\n",
      "it: 109/174, loss: 0.3901, time: 20.5972\n",
      "it: 110/174, loss: 0.3908, time: 19.2025\n",
      "it: 111/174, loss: 0.3914, time: 20.7771\n",
      "it: 112/174, loss: 0.3892, time: 20.0725\n",
      "it: 113/174, loss: 0.3888, time: 20.1272\n",
      "it: 114/174, loss: 0.3884, time: 20.3223\n",
      "it: 115/174, loss: 0.3871, time: 19.2178\n",
      "it: 116/174, loss: 0.3876, time: 20.3767\n",
      "it: 117/174, loss: 0.3875, time: 19.7952\n",
      "it: 118/174, loss: 0.3872, time: 19.1369\n",
      "it: 119/174, loss: 0.3867, time: 20.7004\n",
      "it: 120/174, loss: 0.3858, time: 19.9579\n",
      "it: 121/174, loss: 0.3860, time: 19.3293\n",
      "it: 122/174, loss: 0.3867, time: 20.5308\n",
      "it: 123/174, loss: 0.3883, time: 19.4075\n",
      "it: 124/174, loss: 0.3873, time: 20.5002\n",
      "it: 125/174, loss: 0.3859, time: 19.7792\n",
      "it: 126/174, loss: 0.3860, time: 19.9863\n",
      "it: 127/174, loss: 0.3871, time: 20.0167\n",
      "it: 128/174, loss: 0.3866, time: 19.2910\n",
      "it: 129/174, loss: 0.3862, time: 20.7707\n",
      "it: 130/174, loss: 0.3870, time: 19.9634\n",
      "it: 131/174, loss: 0.3860, time: 19.4101\n",
      "it: 132/174, loss: 0.3854, time: 20.7153\n",
      "it: 133/174, loss: 0.3850, time: 19.8512\n",
      "it: 134/174, loss: 0.3843, time: 19.0382\n",
      "it: 135/174, loss: 0.3849, time: 20.3780\n",
      "it: 136/174, loss: 0.3847, time: 19.0096\n",
      "it: 137/174, loss: 0.3855, time: 20.7024\n",
      "it: 138/174, loss: 0.3857, time: 19.8821\n",
      "it: 139/174, loss: 0.3854, time: 20.2670\n",
      "it: 140/174, loss: 0.3855, time: 20.0954\n",
      "it: 141/174, loss: 0.3843, time: 19.5564\n",
      "it: 142/174, loss: 0.3846, time: 20.6188\n",
      "it: 143/174, loss: 0.3841, time: 19.7467\n",
      "it: 144/174, loss: 0.3840, time: 19.2121\n",
      "it: 145/174, loss: 0.3849, time: 20.6801\n",
      "it: 146/174, loss: 0.3840, time: 20.1150\n",
      "it: 147/174, loss: 0.3831, time: 20.0544\n",
      "it: 148/174, loss: 0.3833, time: 20.1377\n",
      "it: 149/174, loss: 0.3828, time: 20.0878\n",
      "it: 150/174, loss: 0.3823, time: 20.1225\n",
      "it: 151/174, loss: 0.3818, time: 19.8433\n",
      "it: 152/174, loss: 0.3816, time: 19.6505\n",
      "it: 153/174, loss: 0.3812, time: 20.0452\n",
      "it: 154/174, loss: 0.3811, time: 19.9048\n",
      "it: 155/174, loss: 0.3805, time: 19.8894\n",
      "it: 156/174, loss: 0.3804, time: 20.0621\n",
      "it: 157/174, loss: 0.3798, time: 20.1073\n",
      "it: 158/174, loss: 0.3800, time: 20.4719\n",
      "it: 159/174, loss: 0.3796, time: 20.1591\n",
      "it: 160/174, loss: 0.3794, time: 19.9536\n",
      "it: 161/174, loss: 0.3793, time: 19.8579\n",
      "it: 162/174, loss: 0.3793, time: 20.0235\n",
      "it: 163/174, loss: 0.3792, time: 19.9771\n",
      "it: 164/174, loss: 0.3778, time: 20.0434\n",
      "it: 165/174, loss: 0.3784, time: 20.1137\n",
      "it: 166/174, loss: 0.3772, time: 20.1229\n",
      "it: 167/174, loss: 0.3768, time: 20.0399\n",
      "it: 168/174, loss: 0.3764, time: 20.0221\n",
      "it: 169/174, loss: 0.3758, time: 19.8274\n",
      "it: 170/174, loss: 0.3754, time: 19.8424\n",
      "it: 171/174, loss: 0.3750, time: 19.8249\n",
      "it: 172/174, loss: 0.3744, time: 19.9343\n",
      "it: 173/174, loss: 0.3749, time: 20.1421\n",
      "it: 174/174, loss: 0.3752, time: 20.2613\n",
      "it: 1/174, loss: 0.2231, time: 19.4845\n",
      "it: 2/174, loss: 0.2749, time: 19.4491\n",
      "it: 3/174, loss: 0.2437, time: 19.4113\n",
      "it: 4/174, loss: 0.2381, time: 19.5348\n",
      "it: 5/174, loss: 0.2375, time: 19.5547\n",
      "it: 6/174, loss: 0.2496, time: 19.5478\n",
      "it: 7/174, loss: 0.2676, time: 19.8415\n",
      "it: 8/174, loss: 0.2876, time: 19.6887\n",
      "it: 9/174, loss: 0.3002, time: 18.9944\n",
      "it: 10/174, loss: 0.2987, time: 20.1912\n",
      "it: 11/174, loss: 0.2876, time: 19.5561\n",
      "it: 12/174, loss: 0.2928, time: 18.8966\n",
      "it: 13/174, loss: 0.3036, time: 20.2909\n",
      "it: 14/174, loss: 0.2948, time: 19.4059\n",
      "it: 15/174, loss: 0.2955, time: 19.1498\n",
      "it: 16/174, loss: 0.3058, time: 20.2415\n",
      "it: 17/174, loss: 0.3036, time: 19.0908\n",
      "it: 18/174, loss: 0.3005, time: 20.4629\n",
      "it: 19/174, loss: 0.2993, time: 19.8593\n",
      "it: 20/174, loss: 0.2983, time: 19.6804\n",
      "it: 21/174, loss: 0.3047, time: 19.7991\n",
      "it: 22/174, loss: 0.3055, time: 19.0316\n",
      "it: 23/174, loss: 0.3202, time: 20.3876\n",
      "it: 24/174, loss: 0.3201, time: 19.7216\n",
      "it: 25/174, loss: 0.3242, time: 19.9212\n",
      "it: 26/174, loss: 0.3193, time: 20.2482\n",
      "it: 27/174, loss: 0.3165, time: 19.9019\n",
      "it: 28/174, loss: 0.3135, time: 20.1316\n",
      "it: 29/174, loss: 0.3160, time: 19.7467\n",
      "it: 30/174, loss: 0.3172, time: 19.6790\n",
      "it: 31/174, loss: 0.3199, time: 19.5692\n",
      "it: 32/174, loss: 0.3169, time: 19.6619\n",
      "it: 33/174, loss: 0.3136, time: 19.9419\n",
      "it: 34/174, loss: 0.3138, time: 20.0847\n",
      "it: 35/174, loss: 0.3120, time: 19.9961\n",
      "it: 36/174, loss: 0.3197, time: 19.8797\n",
      "it: 37/174, loss: 0.3165, time: 20.0088\n",
      "it: 38/174, loss: 0.3163, time: 19.8376\n",
      "it: 39/174, loss: 0.3139, time: 19.8286\n",
      "it: 40/174, loss: 0.3133, time: 19.8896\n",
      "it: 41/174, loss: 0.3115, time: 19.8668\n",
      "it: 42/174, loss: 0.3100, time: 19.8217\n",
      "it: 43/174, loss: 0.3113, time: 20.1124\n",
      "it: 44/174, loss: 0.3126, time: 19.7980\n",
      "it: 45/174, loss: 0.3095, time: 19.9945\n",
      "it: 46/174, loss: 0.3095, time: 19.7174\n",
      "it: 47/174, loss: 0.3093, time: 19.7065\n",
      "it: 48/174, loss: 0.3079, time: 19.7778\n",
      "it: 49/174, loss: 0.3095, time: 19.6714\n",
      "it: 50/174, loss: 0.3125, time: 19.6830\n",
      "it: 51/174, loss: 0.3147, time: 19.7716\n",
      "it: 52/174, loss: 0.3133, time: 19.7298\n",
      "it: 53/174, loss: 0.3134, time: 19.8636\n",
      "it: 54/174, loss: 0.3093, time: 20.0022\n",
      "it: 55/174, loss: 0.3084, time: 19.8326\n",
      "it: 56/174, loss: 0.3103, time: 19.7341\n",
      "it: 57/174, loss: 0.3122, time: 19.8640\n",
      "it: 58/174, loss: 0.3159, time: 19.8301\n",
      "it: 59/174, loss: 0.3175, time: 19.8973\n",
      "it: 60/174, loss: 0.3156, time: 19.7678\n",
      "it: 61/174, loss: 0.3148, time: 19.9096\n",
      "it: 62/174, loss: 0.3178, time: 19.9787\n",
      "it: 63/174, loss: 0.3180, time: 19.9672\n",
      "it: 64/174, loss: 0.3187, time: 19.8972\n",
      "it: 65/174, loss: 0.3177, time: 19.6843\n",
      "it: 66/174, loss: 0.3177, time: 19.7346\n",
      "it: 67/174, loss: 0.3150, time: 19.7093\n",
      "it: 68/174, loss: 0.3151, time: 19.8357\n",
      "it: 69/174, loss: 0.3178, time: 19.8683\n",
      "it: 70/174, loss: 0.3187, time: 19.9393\n",
      "it: 71/174, loss: 0.3188, time: 19.9011\n",
      "it: 72/174, loss: 0.3188, time: 19.8676\n",
      "it: 73/174, loss: 0.3189, time: 19.9243\n",
      "it: 74/174, loss: 0.3190, time: 19.9477\n",
      "it: 75/174, loss: 0.3205, time: 19.8225\n",
      "it: 76/174, loss: 0.3182, time: 19.9291\n",
      "it: 77/174, loss: 0.3193, time: 19.8658\n",
      "it: 78/174, loss: 0.3217, time: 19.9525\n",
      "it: 79/174, loss: 0.3214, time: 19.8504\n",
      "it: 80/174, loss: 0.3223, time: 19.9783\n",
      "it: 81/174, loss: 0.3227, time: 20.0219\n",
      "it: 82/174, loss: 0.3224, time: 19.9121\n",
      "it: 83/174, loss: 0.3209, time: 19.7272\n",
      "it: 84/174, loss: 0.3214, time: 19.6949\n",
      "it: 85/174, loss: 0.3207, time: 19.7379\n",
      "it: 86/174, loss: 0.3200, time: 20.0047\n",
      "it: 87/174, loss: 0.3233, time: 19.8670\n",
      "it: 88/174, loss: 0.3247, time: 19.8706\n",
      "it: 89/174, loss: 0.3259, time: 19.8609\n",
      "it: 90/174, loss: 0.3257, time: 20.0344\n",
      "it: 91/174, loss: 0.3263, time: 19.8981\n",
      "it: 92/174, loss: 0.3258, time: 19.8407\n",
      "it: 93/174, loss: 0.3274, time: 19.8740\n",
      "it: 94/174, loss: 0.3283, time: 20.0207\n",
      "it: 95/174, loss: 0.3280, time: 19.8417\n",
      "it: 96/174, loss: 0.3280, time: 19.9284\n",
      "it: 97/174, loss: 0.3275, time: 19.9641\n",
      "it: 98/174, loss: 0.3283, time: 19.9077\n",
      "it: 99/174, loss: 0.3296, time: 19.8477\n",
      "it: 100/174, loss: 0.3304, time: 19.9298\n",
      "it: 101/174, loss: 0.3308, time: 19.6639\n",
      "it: 102/174, loss: 0.3301, time: 19.8372\n",
      "it: 103/174, loss: 0.3294, time: 19.8651\n",
      "it: 104/174, loss: 0.3286, time: 19.9547\n",
      "it: 105/174, loss: 0.3285, time: 19.9394\n",
      "it: 106/174, loss: 0.3277, time: 20.0000\n",
      "it: 107/174, loss: 0.3280, time: 19.8473\n",
      "it: 108/174, loss: 0.3280, time: 19.9758\n",
      "it: 109/174, loss: 0.3314, time: 19.8277\n",
      "it: 110/174, loss: 0.3326, time: 19.7581\n",
      "it: 111/174, loss: 0.3318, time: 19.8477\n",
      "it: 112/174, loss: 0.3314, time: 20.0248\n",
      "it: 113/174, loss: 0.3315, time: 19.9120\n",
      "it: 114/174, loss: 0.3306, time: 20.1027\n",
      "it: 115/174, loss: 0.3310, time: 20.1117\n",
      "it: 116/174, loss: 0.3304, time: 20.3664\n",
      "it: 117/174, loss: 0.3301, time: 20.0863\n",
      "it: 118/174, loss: 0.3309, time: 19.7997\n",
      "it: 119/174, loss: 0.3315, time: 19.8167\n",
      "it: 120/174, loss: 0.3312, time: 19.8279\n",
      "it: 121/174, loss: 0.3312, time: 19.7822\n",
      "it: 122/174, loss: 0.3336, time: 19.9910\n",
      "it: 123/174, loss: 0.3332, time: 19.9584\n",
      "it: 124/174, loss: 0.3336, time: 20.0191\n",
      "it: 125/174, loss: 0.3336, time: 19.9950\n",
      "it: 126/174, loss: 0.3328, time: 19.8553\n",
      "it: 127/174, loss: 0.3330, time: 19.9254\n",
      "it: 128/174, loss: 0.3316, time: 19.7480\n",
      "it: 129/174, loss: 0.3309, time: 19.9850\n",
      "it: 130/174, loss: 0.3306, time: 19.9459\n",
      "it: 131/174, loss: 0.3303, time: 19.9342\n",
      "it: 132/174, loss: 0.3302, time: 19.8850\n",
      "it: 133/174, loss: 0.3308, time: 19.9808\n",
      "it: 134/174, loss: 0.3310, time: 19.8960\n",
      "it: 135/174, loss: 0.3309, time: 19.9294\n",
      "it: 136/174, loss: 0.3301, time: 19.7478\n",
      "it: 137/174, loss: 0.3301, time: 19.7767\n",
      "it: 138/174, loss: 0.3302, time: 19.7103\n",
      "it: 139/174, loss: 0.3303, time: 19.9487\n",
      "it: 140/174, loss: 0.3315, time: 19.9407\n",
      "it: 141/174, loss: 0.3319, time: 19.9886\n",
      "it: 142/174, loss: 0.3317, time: 19.9696\n",
      "it: 143/174, loss: 0.3329, time: 19.9983\n",
      "it: 144/174, loss: 0.3320, time: 19.7985\n",
      "it: 145/174, loss: 0.3339, time: 19.8501\n",
      "it: 146/174, loss: 0.3336, time: 19.8360\n",
      "it: 147/174, loss: 0.3339, time: 19.8405\n",
      "it: 148/174, loss: 0.3336, time: 19.9576\n",
      "it: 149/174, loss: 0.3335, time: 20.6553\n",
      "it: 150/174, loss: 0.3334, time: 19.7478\n",
      "it: 151/174, loss: 0.3338, time: 20.1259\n",
      "it: 152/174, loss: 0.3335, time: 19.9623\n",
      "it: 153/174, loss: 0.3326, time: 19.2446\n",
      "it: 154/174, loss: 0.3319, time: 20.3742\n",
      "it: 155/174, loss: 0.3324, time: 19.8508\n",
      "it: 156/174, loss: 0.3333, time: 18.9915\n",
      "it: 157/174, loss: 0.3330, time: 20.6088\n",
      "it: 158/174, loss: 0.3330, time: 20.0507\n",
      "it: 159/174, loss: 0.3331, time: 19.2783\n",
      "it: 160/174, loss: 0.3351, time: 20.5973\n",
      "it: 161/174, loss: 0.3353, time: 19.2345\n",
      "it: 162/174, loss: 0.3355, time: 20.7414\n",
      "it: 163/174, loss: 0.3352, time: 19.7783\n",
      "it: 164/174, loss: 0.3357, time: 19.9993\n",
      "it: 165/174, loss: 0.3363, time: 20.0877\n",
      "it: 166/174, loss: 0.3362, time: 19.2341\n",
      "it: 167/174, loss: 0.3355, time: 20.6624\n",
      "it: 168/174, loss: 0.3353, time: 19.8744\n",
      "it: 169/174, loss: 0.3352, time: 19.2057\n",
      "it: 170/174, loss: 0.3350, time: 20.5830\n",
      "it: 171/174, loss: 0.3353, time: 19.9361\n",
      "it: 172/174, loss: 0.3351, time: 19.0665\n",
      "it: 173/174, loss: 0.3352, time: 20.4407\n",
      "it: 174/174, loss: 0.3346, time: 19.0445\n",
      "it: 1/174, loss: 0.2563, time: 19.2718\n",
      "it: 2/174, loss: 0.1737, time: 19.5494\n",
      "it: 3/174, loss: 0.1880, time: 19.8794\n",
      "it: 4/174, loss: 0.1751, time: 19.9520\n",
      "it: 5/174, loss: 0.1670, time: 19.6595\n",
      "it: 6/174, loss: 0.1835, time: 18.8010\n",
      "it: 7/174, loss: 0.2085, time: 20.1811\n",
      "it: 8/174, loss: 0.2283, time: 19.5158\n",
      "it: 9/174, loss: 0.2237, time: 18.9929\n",
      "it: 10/174, loss: 0.2263, time: 20.2538\n",
      "it: 11/174, loss: 0.2371, time: 19.0129\n",
      "it: 12/174, loss: 0.2385, time: 20.5771\n",
      "it: 13/174, loss: 0.2358, time: 20.1567\n",
      "it: 14/174, loss: 0.2350, time: 19.6026\n",
      "it: 15/174, loss: 0.2301, time: 19.5597\n",
      "it: 16/174, loss: 0.2312, time: 18.9175\n",
      "it: 17/174, loss: 0.2541, time: 20.1842\n",
      "it: 18/174, loss: 0.2553, time: 19.7435\n",
      "it: 19/174, loss: 0.2567, time: 19.0983\n",
      "it: 20/174, loss: 0.2651, time: 20.3075\n",
      "it: 21/174, loss: 0.2655, time: 19.7430\n",
      "it: 22/174, loss: 0.2735, time: 19.2517\n",
      "it: 23/174, loss: 0.2693, time: 20.3063\n",
      "it: 24/174, loss: 0.2653, time: 18.9196\n",
      "it: 25/174, loss: 0.2623, time: 20.3292\n",
      "it: 26/174, loss: 0.2633, time: 19.7391\n",
      "it: 27/174, loss: 0.2619, time: 19.7258\n",
      "it: 28/174, loss: 0.2664, time: 20.0412\n",
      "it: 29/174, loss: 0.2655, time: 19.1993\n",
      "it: 30/174, loss: 0.2679, time: 20.4646\n",
      "it: 31/174, loss: 0.2706, time: 19.8347\n",
      "it: 32/174, loss: 0.2716, time: 19.0288\n",
      "it: 33/174, loss: 0.2706, time: 20.2645\n",
      "it: 34/174, loss: 0.2741, time: 19.7644\n",
      "it: 35/174, loss: 0.2746, time: 19.0512\n",
      "it: 36/174, loss: 0.2714, time: 20.4469\n",
      "it: 37/174, loss: 0.2720, time: 19.0622\n",
      "it: 38/174, loss: 0.2687, time: 20.7511\n",
      "it: 39/174, loss: 0.2666, time: 19.9853\n",
      "it: 40/174, loss: 0.2629, time: 20.0419\n",
      "it: 41/174, loss: 0.2611, time: 19.6365\n",
      "it: 42/174, loss: 0.2630, time: 19.1735\n",
      "it: 43/174, loss: 0.2620, time: 20.4903\n",
      "it: 44/174, loss: 0.2655, time: 19.9506\n",
      "it: 45/174, loss: 0.2665, time: 19.0437\n",
      "it: 46/174, loss: 0.2666, time: 20.4580\n",
      "it: 47/174, loss: 0.2647, time: 19.7595\n",
      "it: 48/174, loss: 0.2707, time: 19.1059\n",
      "it: 49/174, loss: 0.2726, time: 20.4260\n",
      "it: 50/174, loss: 0.2699, time: 19.1729\n",
      "it: 51/174, loss: 0.2676, time: 20.2640\n",
      "it: 52/174, loss: 0.2668, time: 19.6400\n",
      "it: 53/174, loss: 0.2685, time: 19.9097\n",
      "it: 54/174, loss: 0.2696, time: 20.1267\n",
      "it: 55/174, loss: 0.2704, time: 19.4149\n",
      "it: 56/174, loss: 0.2709, time: 20.5872\n",
      "it: 57/174, loss: 0.2683, time: 20.4081\n",
      "it: 58/174, loss: 0.2666, time: 19.3731\n",
      "it: 59/174, loss: 0.2691, time: 20.5538\n",
      "it: 60/174, loss: 0.2699, time: 19.8162\n",
      "it: 61/174, loss: 0.2726, time: 19.2607\n",
      "it: 62/174, loss: 0.2717, time: 20.6072\n",
      "it: 63/174, loss: 0.2726, time: 19.4267\n",
      "it: 64/174, loss: 0.2719, time: 20.6961\n",
      "it: 65/174, loss: 0.2702, time: 20.0876\n",
      "it: 66/174, loss: 0.2698, time: 20.0005\n",
      "it: 67/174, loss: 0.2726, time: 19.8770\n",
      "it: 68/174, loss: 0.2720, time: 18.9338\n",
      "it: 69/174, loss: 0.2724, time: 20.3462\n",
      "it: 70/174, loss: 0.2719, time: 19.6832\n",
      "it: 71/174, loss: 0.2749, time: 19.0735\n",
      "it: 72/174, loss: 0.2751, time: 20.3669\n",
      "it: 73/174, loss: 0.2753, time: 19.9652\n",
      "it: 74/174, loss: 0.2745, time: 19.4707\n",
      "it: 75/174, loss: 0.2745, time: 20.8282\n",
      "it: 76/174, loss: 0.2746, time: 19.3072\n",
      "it: 77/174, loss: 0.2740, time: 20.4603\n",
      "it: 78/174, loss: 0.2750, time: 19.7985\n",
      "it: 79/174, loss: 0.2752, time: 19.9675\n",
      "it: 80/174, loss: 0.2741, time: 20.0184\n",
      "it: 81/174, loss: 0.2765, time: 19.1838\n",
      "it: 82/174, loss: 0.2756, time: 20.3653\n",
      "it: 83/174, loss: 0.2747, time: 19.8873\n",
      "it: 84/174, loss: 0.2764, time: 19.0472\n",
      "it: 85/174, loss: 0.2769, time: 20.4945\n",
      "it: 86/174, loss: 0.2763, time: 19.5195\n",
      "it: 87/174, loss: 0.2767, time: 19.1997\n",
      "it: 88/174, loss: 0.2768, time: 20.3075\n",
      "it: 89/174, loss: 0.2781, time: 19.2863\n",
      "it: 90/174, loss: 0.2799, time: 20.5675\n",
      "it: 91/174, loss: 0.2823, time: 19.9189\n",
      "it: 92/174, loss: 0.2814, time: 19.8909\n",
      "it: 93/174, loss: 0.2804, time: 19.9340\n",
      "it: 94/174, loss: 0.2822, time: 19.1627\n",
      "it: 95/174, loss: 0.2827, time: 20.4668\n",
      "it: 96/174, loss: 0.2822, time: 19.7626\n",
      "it: 97/174, loss: 0.2834, time: 19.3535\n",
      "it: 98/174, loss: 0.2835, time: 20.4973\n",
      "it: 99/174, loss: 0.2855, time: 19.7965\n",
      "it: 100/174, loss: 0.2849, time: 19.1206\n",
      "it: 101/174, loss: 0.2855, time: 20.6278\n",
      "it: 102/174, loss: 0.2852, time: 19.1610\n",
      "it: 103/174, loss: 0.2870, time: 20.4907\n",
      "it: 104/174, loss: 0.2868, time: 19.6781\n",
      "it: 105/174, loss: 0.2861, time: 19.8033\n",
      "it: 106/174, loss: 0.2872, time: 19.7888\n",
      "it: 107/174, loss: 0.2875, time: 19.3231\n",
      "it: 108/174, loss: 0.2859, time: 20.5575\n",
      "it: 109/174, loss: 0.2871, time: 19.9023\n",
      "it: 110/174, loss: 0.2879, time: 19.3094\n",
      "it: 111/174, loss: 0.2892, time: 20.5637\n",
      "it: 112/174, loss: 0.2881, time: 19.8284\n",
      "it: 113/174, loss: 0.2876, time: 19.0725\n",
      "it: 114/174, loss: 0.2890, time: 20.4092\n",
      "it: 115/174, loss: 0.2897, time: 19.3091\n",
      "it: 116/174, loss: 0.2900, time: 20.4955\n",
      "it: 117/174, loss: 0.2899, time: 20.0562\n",
      "it: 118/174, loss: 0.2899, time: 20.1048\n",
      "it: 119/174, loss: 0.2908, time: 20.3831\n",
      "it: 120/174, loss: 0.2915, time: 19.2474\n",
      "it: 121/174, loss: 0.2918, time: 20.5044\n",
      "it: 122/174, loss: 0.2913, time: 19.7182\n",
      "it: 123/174, loss: 0.2916, time: 19.1732\n",
      "it: 124/174, loss: 0.2909, time: 20.4486\n",
      "it: 125/174, loss: 0.2916, time: 19.9505\n",
      "it: 126/174, loss: 0.2923, time: 19.3027\n",
      "it: 127/174, loss: 0.2924, time: 20.6307\n",
      "it: 128/174, loss: 0.2926, time: 19.4335\n",
      "it: 129/174, loss: 0.2925, time: 20.4568\n",
      "it: 130/174, loss: 0.2926, time: 19.8386\n",
      "it: 131/174, loss: 0.2936, time: 19.8528\n",
      "it: 132/174, loss: 0.2930, time: 19.9928\n",
      "it: 133/174, loss: 0.2939, time: 19.3692\n",
      "it: 134/174, loss: 0.2935, time: 20.8228\n",
      "it: 135/174, loss: 0.2924, time: 19.8215\n",
      "it: 136/174, loss: 0.2919, time: 19.1683\n",
      "it: 137/174, loss: 0.2911, time: 20.4717\n",
      "it: 138/174, loss: 0.2920, time: 19.8647\n",
      "it: 139/174, loss: 0.2920, time: 19.0877\n",
      "it: 140/174, loss: 0.2923, time: 20.5218\n",
      "it: 141/174, loss: 0.2925, time: 19.0218\n",
      "it: 142/174, loss: 0.2923, time: 20.4947\n",
      "it: 143/174, loss: 0.2923, time: 19.6879\n",
      "it: 144/174, loss: 0.2921, time: 19.8933\n",
      "it: 145/174, loss: 0.2919, time: 19.8982\n",
      "it: 146/174, loss: 0.2943, time: 19.3133\n",
      "it: 147/174, loss: 0.2940, time: 20.4559\n",
      "it: 148/174, loss: 0.2936, time: 19.4715\n",
      "it: 149/174, loss: 0.2935, time: 19.1640\n",
      "it: 150/174, loss: 0.2960, time: 20.5944\n",
      "it: 151/174, loss: 0.2959, time: 19.7885\n",
      "it: 152/174, loss: 0.2963, time: 19.2726\n",
      "it: 153/174, loss: 0.2966, time: 20.5069\n",
      "it: 154/174, loss: 0.2965, time: 19.2787\n",
      "it: 155/174, loss: 0.2966, time: 20.4867\n",
      "it: 156/174, loss: 0.2958, time: 19.8284\n",
      "it: 157/174, loss: 0.2956, time: 19.7890\n",
      "it: 158/174, loss: 0.2958, time: 19.9209\n",
      "it: 159/174, loss: 0.2946, time: 19.1341\n",
      "it: 160/174, loss: 0.2943, time: 20.6105\n",
      "it: 161/174, loss: 0.2956, time: 19.8544\n",
      "it: 162/174, loss: 0.2950, time: 19.3634\n",
      "it: 163/174, loss: 0.2957, time: 20.7007\n",
      "it: 164/174, loss: 0.2950, time: 20.2551\n",
      "it: 165/174, loss: 0.2944, time: 19.2976\n",
      "it: 166/174, loss: 0.2941, time: 20.3102\n",
      "it: 167/174, loss: 0.2939, time: 19.0852\n",
      "it: 168/174, loss: 0.2943, time: 20.5142\n",
      "it: 169/174, loss: 0.2934, time: 19.8123\n",
      "it: 170/174, loss: 0.2935, time: 19.9866\n",
      "it: 171/174, loss: 0.2935, time: 20.0116\n",
      "it: 172/174, loss: 0.2928, time: 19.3598\n",
      "it: 173/174, loss: 0.2927, time: 20.6643\n",
      "it: 174/174, loss: 0.2927, time: 19.6218\n",
      "it: 1/174, loss: 0.2784, time: 19.2332\n",
      "it: 2/174, loss: 0.1946, time: 19.3620\n",
      "it: 3/174, loss: 0.1778, time: 19.5119\n",
      "it: 4/174, loss: 0.1991, time: 19.5974\n",
      "it: 5/174, loss: 0.2315, time: 19.8367\n",
      "it: 6/174, loss: 0.2203, time: 19.0725\n",
      "it: 7/174, loss: 0.2083, time: 20.4488\n",
      "it: 8/174, loss: 0.2071, time: 19.4628\n",
      "it: 9/174, loss: 0.1975, time: 18.8280\n",
      "it: 10/174, loss: 0.2014, time: 20.2304\n",
      "it: 11/174, loss: 0.2072, time: 19.0097\n",
      "it: 12/174, loss: 0.2172, time: 20.2641\n",
      "it: 13/174, loss: 0.2398, time: 19.8582\n",
      "it: 14/174, loss: 0.2377, time: 19.8543\n",
      "it: 15/174, loss: 0.2278, time: 19.8760\n",
      "it: 16/174, loss: 0.2227, time: 18.9404\n",
      "it: 17/174, loss: 0.2176, time: 20.2743\n",
      "it: 18/174, loss: 0.2187, time: 19.3726\n",
      "it: 19/174, loss: 0.2260, time: 18.8729\n",
      "it: 20/174, loss: 0.2284, time: 20.2669\n",
      "it: 21/174, loss: 0.2286, time: 19.7122\n",
      "it: 22/174, loss: 0.2256, time: 19.0202\n",
      "it: 23/174, loss: 0.2335, time: 20.5000\n",
      "it: 24/174, loss: 0.2395, time: 19.0805\n",
      "it: 25/174, loss: 0.2423, time: 20.3102\n",
      "it: 26/174, loss: 0.2486, time: 19.5042\n",
      "it: 27/174, loss: 0.2461, time: 19.6818\n",
      "it: 28/174, loss: 0.2452, time: 19.8029\n",
      "it: 29/174, loss: 0.2514, time: 19.2128\n",
      "it: 30/174, loss: 0.2498, time: 20.4943\n",
      "it: 31/174, loss: 0.2560, time: 19.7474\n",
      "it: 32/174, loss: 0.2530, time: 19.0976\n",
      "it: 33/174, loss: 0.2552, time: 20.3635\n",
      "it: 34/174, loss: 0.2525, time: 19.7050\n",
      "it: 35/174, loss: 0.2486, time: 19.0422\n",
      "it: 36/174, loss: 0.2447, time: 20.2795\n",
      "it: 37/174, loss: 0.2400, time: 18.9350\n",
      "it: 38/174, loss: 0.2404, time: 20.3799\n",
      "it: 39/174, loss: 0.2472, time: 19.8452\n",
      "it: 40/174, loss: 0.2456, time: 19.8824\n",
      "it: 41/174, loss: 0.2456, time: 19.9300\n",
      "it: 42/174, loss: 0.2419, time: 19.1980\n",
      "it: 43/174, loss: 0.2426, time: 20.5799\n",
      "it: 44/174, loss: 0.2421, time: 19.7374\n",
      "it: 45/174, loss: 0.2456, time: 19.0868\n",
      "it: 46/174, loss: 0.2493, time: 20.4377\n",
      "it: 47/174, loss: 0.2472, time: 19.8518\n",
      "it: 48/174, loss: 0.2477, time: 19.1549\n",
      "it: 49/174, loss: 0.2510, time: 20.4941\n",
      "it: 50/174, loss: 0.2560, time: 19.0794\n",
      "it: 51/174, loss: 0.2578, time: 20.6993\n",
      "it: 52/174, loss: 0.2570, time: 19.8934\n",
      "it: 53/174, loss: 0.2586, time: 19.6421\n",
      "it: 54/174, loss: 0.2585, time: 19.7669\n",
      "it: 55/174, loss: 0.2576, time: 18.9735\n",
      "it: 56/174, loss: 0.2597, time: 20.5214\n",
      "it: 57/174, loss: 0.2582, time: 19.8032\n",
      "it: 58/174, loss: 0.2592, time: 19.0459\n",
      "it: 59/174, loss: 0.2588, time: 20.5804\n",
      "it: 60/174, loss: 0.2571, time: 19.9334\n",
      "it: 61/174, loss: 0.2565, time: 19.3265\n",
      "it: 62/174, loss: 0.2559, time: 20.3557\n",
      "it: 63/174, loss: 0.2596, time: 19.1244\n",
      "it: 64/174, loss: 0.2621, time: 20.3487\n",
      "it: 65/174, loss: 0.2623, time: 19.9175\n",
      "it: 66/174, loss: 0.2614, time: 20.0216\n",
      "it: 67/174, loss: 0.2636, time: 19.8069\n",
      "it: 68/174, loss: 0.2643, time: 19.2271\n",
      "it: 69/174, loss: 0.2644, time: 20.4605\n",
      "it: 70/174, loss: 0.2637, time: 19.7347\n",
      "it: 71/174, loss: 0.2630, time: 19.0038\n",
      "it: 72/174, loss: 0.2620, time: 20.3393\n",
      "it: 73/174, loss: 0.2610, time: 19.5931\n",
      "it: 74/174, loss: 0.2601, time: 19.3073\n",
      "it: 75/174, loss: 0.2602, time: 20.4863\n",
      "it: 76/174, loss: 0.2604, time: 19.2345\n",
      "it: 77/174, loss: 0.2594, time: 20.4977\n",
      "it: 78/174, loss: 0.2582, time: 19.8790\n",
      "it: 79/174, loss: 0.2600, time: 19.8200\n",
      "it: 80/174, loss: 0.2599, time: 19.8154\n",
      "it: 81/174, loss: 0.2611, time: 19.2326\n",
      "it: 82/174, loss: 0.2619, time: 20.5852\n",
      "it: 83/174, loss: 0.2610, time: 19.8366\n",
      "it: 84/174, loss: 0.2604, time: 19.1869\n",
      "it: 85/174, loss: 0.2642, time: 20.5405\n",
      "it: 86/174, loss: 0.2639, time: 19.8512\n",
      "it: 87/174, loss: 0.2615, time: 19.1405\n",
      "it: 88/174, loss: 0.2604, time: 20.5253\n",
      "it: 89/174, loss: 0.2603, time: 19.0667\n",
      "it: 90/174, loss: 0.2589, time: 20.4604\n",
      "it: 91/174, loss: 0.2607, time: 19.6106\n",
      "it: 92/174, loss: 0.2610, time: 20.0421\n",
      "it: 93/174, loss: 0.2602, time: 19.9062\n",
      "it: 94/174, loss: 0.2598, time: 19.2145\n",
      "it: 95/174, loss: 0.2595, time: 20.5356\n",
      "it: 96/174, loss: 0.2590, time: 19.8689\n",
      "it: 97/174, loss: 0.2590, time: 19.1556\n",
      "it: 98/174, loss: 0.2597, time: 20.3775\n",
      "it: 99/174, loss: 0.2604, time: 19.9191\n",
      "it: 100/174, loss: 0.2596, time: 19.2171\n",
      "it: 101/174, loss: 0.2607, time: 20.4006\n",
      "it: 102/174, loss: 0.2596, time: 19.1670\n",
      "it: 103/174, loss: 0.2591, time: 20.3844\n",
      "it: 104/174, loss: 0.2592, time: 19.9930\n",
      "it: 105/174, loss: 0.2596, time: 20.0175\n",
      "it: 106/174, loss: 0.2592, time: 19.8562\n",
      "it: 107/174, loss: 0.2599, time: 18.9679\n",
      "it: 108/174, loss: 0.2612, time: 20.3400\n",
      "it: 109/174, loss: 0.2619, time: 19.8297\n",
      "it: 110/174, loss: 0.2620, time: 19.9108\n",
      "it: 111/174, loss: 0.2618, time: 19.7469\n",
      "it: 112/174, loss: 0.2613, time: 20.0344\n",
      "it: 113/174, loss: 0.2618, time: 19.8497\n",
      "it: 114/174, loss: 0.2609, time: 19.9563\n",
      "it: 115/174, loss: 0.2607, time: 19.8051\n",
      "it: 116/174, loss: 0.2608, time: 19.7230\n",
      "it: 117/174, loss: 0.2609, time: 19.7477\n",
      "it: 118/174, loss: 0.2602, time: 19.8313\n",
      "it: 119/174, loss: 0.2606, time: 19.8025\n",
      "it: 120/174, loss: 0.2599, time: 19.9145\n",
      "it: 121/174, loss: 0.2601, time: 19.9243\n",
      "it: 122/174, loss: 0.2597, time: 19.9170\n",
      "it: 123/174, loss: 0.2589, time: 19.9315\n",
      "it: 124/174, loss: 0.2595, time: 19.7690\n",
      "it: 125/174, loss: 0.2621, time: 19.6762\n",
      "it: 126/174, loss: 0.2610, time: 19.7130\n",
      "it: 127/174, loss: 0.2618, time: 19.6396\n",
      "it: 128/174, loss: 0.2610, time: 20.0495\n",
      "it: 129/174, loss: 0.2609, time: 20.0889\n",
      "it: 130/174, loss: 0.2606, time: 19.9732\n",
      "it: 131/174, loss: 0.2595, time: 19.9425\n",
      "it: 132/174, loss: 0.2588, time: 20.1204\n",
      "it: 133/174, loss: 0.2595, time: 19.8289\n",
      "it: 134/174, loss: 0.2604, time: 19.8320\n",
      "it: 135/174, loss: 0.2606, time: 19.9896\n",
      "it: 136/174, loss: 0.2599, time: 20.0214\n",
      "it: 137/174, loss: 0.2597, time: 19.8181\n",
      "it: 138/174, loss: 0.2593, time: 19.9184\n",
      "it: 139/174, loss: 0.2609, time: 19.7602\n",
      "it: 140/174, loss: 0.2621, time: 19.8837\n",
      "it: 141/174, loss: 0.2614, time: 19.7766\n",
      "it: 142/174, loss: 0.2616, time: 19.6969\n",
      "it: 143/174, loss: 0.2615, time: 19.8626\n",
      "it: 144/174, loss: 0.2631, time: 19.7430\n",
      "it: 145/174, loss: 0.2634, time: 19.9056\n",
      "it: 146/174, loss: 0.2625, time: 19.7891\n",
      "it: 147/174, loss: 0.2618, time: 19.8589\n",
      "it: 148/174, loss: 0.2610, time: 19.7731\n",
      "it: 149/174, loss: 0.2615, time: 20.1049\n",
      "it: 150/174, loss: 0.2621, time: 19.8766\n",
      "it: 151/174, loss: 0.2620, time: 19.8956\n",
      "it: 152/174, loss: 0.2614, time: 19.7972\n",
      "it: 153/174, loss: 0.2623, time: 19.8354\n",
      "it: 154/174, loss: 0.2626, time: 19.8973\n",
      "it: 155/174, loss: 0.2622, time: 20.0671\n",
      "it: 156/174, loss: 0.2623, time: 20.3958\n",
      "it: 157/174, loss: 0.2619, time: 20.0135\n",
      "it: 158/174, loss: 0.2645, time: 19.8887\n",
      "it: 159/174, loss: 0.2660, time: 19.9563\n",
      "it: 160/174, loss: 0.2675, time: 19.6611\n",
      "it: 161/174, loss: 0.2681, time: 19.9218\n",
      "it: 162/174, loss: 0.2680, time: 19.6180\n",
      "it: 163/174, loss: 0.2689, time: 19.9156\n",
      "it: 164/174, loss: 0.2682, time: 19.8636\n",
      "it: 165/174, loss: 0.2681, time: 20.0438\n",
      "it: 166/174, loss: 0.2672, time: 19.8844\n",
      "it: 167/174, loss: 0.2666, time: 19.9967\n",
      "it: 168/174, loss: 0.2668, time: 19.9817\n",
      "it: 169/174, loss: 0.2674, time: 19.8108\n",
      "it: 170/174, loss: 0.2679, time: 19.8753\n",
      "it: 171/174, loss: 0.2677, time: 20.0767\n",
      "it: 172/174, loss: 0.2677, time: 19.8122\n",
      "it: 173/174, loss: 0.2670, time: 19.8985\n",
      "it: 174/174, loss: 0.2676, time: 19.8105\n",
      "it: 1/174, loss: 0.3374, time: 19.3171\n",
      "it: 2/174, loss: 0.2857, time: 19.2951\n",
      "it: 3/174, loss: 0.2336, time: 19.3629\n",
      "it: 4/174, loss: 0.2242, time: 19.3428\n",
      "it: 5/174, loss: 0.2207, time: 19.3723\n",
      "it: 6/174, loss: 0.2619, time: 19.7713\n",
      "it: 7/174, loss: 0.2468, time: 19.5349\n",
      "it: 8/174, loss: 0.2460, time: 19.7490\n",
      "it: 9/174, loss: 0.2472, time: 18.9007\n",
      "it: 10/174, loss: 0.2485, time: 20.3862\n",
      "it: 11/174, loss: 0.2653, time: 19.4055\n",
      "it: 12/174, loss: 0.2704, time: 18.8141\n",
      "it: 13/174, loss: 0.2713, time: 20.2094\n",
      "it: 14/174, loss: 0.2678, time: 19.8410\n",
      "it: 15/174, loss: 0.2576, time: 19.2350\n",
      "it: 16/174, loss: 0.2547, time: 20.5154\n",
      "it: 17/174, loss: 0.2525, time: 19.1489\n",
      "it: 18/174, loss: 0.2524, time: 20.5446\n",
      "it: 19/174, loss: 0.2475, time: 19.6338\n",
      "it: 20/174, loss: 0.2422, time: 19.6486\n",
      "it: 21/174, loss: 0.2408, time: 19.5910\n",
      "it: 22/174, loss: 0.2415, time: 18.9024\n",
      "it: 23/174, loss: 0.2479, time: 20.2404\n",
      "it: 24/174, loss: 0.2476, time: 19.9392\n",
      "it: 25/174, loss: 0.2489, time: 18.9996\n",
      "it: 26/174, loss: 0.2502, time: 20.4610\n",
      "it: 27/174, loss: 0.2463, time: 19.6637\n",
      "it: 28/174, loss: 0.2445, time: 19.1073\n",
      "it: 29/174, loss: 0.2449, time: 20.2897\n",
      "it: 30/174, loss: 0.2458, time: 19.2829\n",
      "it: 31/174, loss: 0.2481, time: 20.3335\n",
      "it: 32/174, loss: 0.2472, time: 19.8040\n",
      "it: 33/174, loss: 0.2464, time: 19.6892\n",
      "it: 34/174, loss: 0.2455, time: 19.8244\n",
      "it: 35/174, loss: 0.2488, time: 19.1268\n",
      "it: 36/174, loss: 0.2466, time: 20.7434\n",
      "it: 37/174, loss: 0.2496, time: 19.8986\n",
      "it: 38/174, loss: 0.2516, time: 19.0215\n",
      "it: 39/174, loss: 0.2521, time: 20.3443\n",
      "it: 40/174, loss: 0.2566, time: 19.5370\n",
      "it: 41/174, loss: 0.2545, time: 19.1501\n",
      "it: 42/174, loss: 0.2532, time: 20.4338\n",
      "it: 43/174, loss: 0.2513, time: 19.2943\n",
      "it: 44/174, loss: 0.2510, time: 20.5719\n",
      "it: 45/174, loss: 0.2474, time: 20.1760\n",
      "it: 46/174, loss: 0.2522, time: 19.9926\n",
      "it: 47/174, loss: 0.2521, time: 19.8661\n",
      "it: 48/174, loss: 0.2506, time: 19.0706\n",
      "it: 49/174, loss: 0.2512, time: 20.5432\n",
      "it: 50/174, loss: 0.2527, time: 19.6789\n",
      "it: 51/174, loss: 0.2524, time: 19.4339\n",
      "it: 52/174, loss: 0.2494, time: 20.3901\n",
      "it: 53/174, loss: 0.2489, time: 19.6740\n",
      "it: 54/174, loss: 0.2508, time: 19.1180\n",
      "it: 55/174, loss: 0.2496, time: 20.5740\n",
      "it: 56/174, loss: 0.2479, time: 19.9785\n",
      "it: 57/174, loss: 0.2510, time: 19.8339\n",
      "it: 58/174, loss: 0.2505, time: 19.7639\n",
      "it: 59/174, loss: 0.2489, time: 19.8521\n",
      "it: 60/174, loss: 0.2493, time: 20.0018\n",
      "it: 61/174, loss: 0.2492, time: 19.9236\n",
      "it: 62/174, loss: 0.2502, time: 19.9325\n",
      "it: 63/174, loss: 0.2494, time: 20.0195\n",
      "it: 64/174, loss: 0.2495, time: 19.9692\n",
      "it: 65/174, loss: 0.2475, time: 19.8404\n",
      "it: 66/174, loss: 0.2490, time: 19.7562\n",
      "it: 67/174, loss: 0.2476, time: 20.0347\n",
      "it: 68/174, loss: 0.2461, time: 19.7939\n",
      "it: 69/174, loss: 0.2481, time: 19.9885\n",
      "it: 70/174, loss: 0.2488, time: 20.0202\n",
      "it: 71/174, loss: 0.2493, time: 19.9651\n",
      "it: 72/174, loss: 0.2515, time: 19.8847\n",
      "it: 73/174, loss: 0.2500, time: 19.8391\n",
      "it: 74/174, loss: 0.2505, time: 19.7730\n",
      "it: 75/174, loss: 0.2516, time: 19.9416\n",
      "it: 76/174, loss: 0.2529, time: 19.7675\n",
      "it: 77/174, loss: 0.2523, time: 19.8949\n",
      "it: 78/174, loss: 0.2509, time: 19.8300\n",
      "it: 79/174, loss: 0.2507, time: 19.9017\n",
      "it: 80/174, loss: 0.2502, time: 19.8391\n",
      "it: 81/174, loss: 0.2511, time: 19.8806\n",
      "it: 82/174, loss: 0.2504, time: 20.0198\n",
      "it: 83/174, loss: 0.2505, time: 19.6431\n",
      "it: 84/174, loss: 0.2499, time: 19.7237\n",
      "it: 85/174, loss: 0.2510, time: 19.8727\n",
      "it: 86/174, loss: 0.2495, time: 19.9260\n",
      "it: 87/174, loss: 0.2503, time: 19.8315\n",
      "it: 88/174, loss: 0.2510, time: 19.8648\n",
      "it: 89/174, loss: 0.2536, time: 19.8219\n",
      "it: 90/174, loss: 0.2546, time: 19.9858\n",
      "it: 91/174, loss: 0.2536, time: 19.7039\n",
      "it: 92/174, loss: 0.2554, time: 19.7321\n",
      "it: 93/174, loss: 0.2548, time: 19.6620\n",
      "it: 94/174, loss: 0.2545, time: 20.3400\n",
      "it: 95/174, loss: 0.2533, time: 19.8252\n",
      "it: 96/174, loss: 0.2528, time: 20.0738\n",
      "it: 97/174, loss: 0.2528, time: 19.8984\n",
      "it: 98/174, loss: 0.2527, time: 19.2320\n",
      "it: 99/174, loss: 0.2530, time: 20.5515\n",
      "it: 100/174, loss: 0.2521, time: 19.8874\n",
      "it: 101/174, loss: 0.2536, time: 18.9467\n",
      "it: 102/174, loss: 0.2523, time: 20.5980\n",
      "it: 103/174, loss: 0.2527, time: 19.7073\n",
      "it: 104/174, loss: 0.2520, time: 19.2382\n",
      "it: 105/174, loss: 0.2513, time: 20.5362\n",
      "it: 106/174, loss: 0.2521, time: 19.1789\n",
      "it: 107/174, loss: 0.2535, time: 20.5218\n",
      "it: 108/174, loss: 0.2531, time: 19.7731\n",
      "it: 109/174, loss: 0.2515, time: 19.7085\n",
      "it: 110/174, loss: 0.2518, time: 19.8698\n",
      "it: 111/174, loss: 0.2523, time: 19.0758\n",
      "it: 112/174, loss: 0.2526, time: 20.5155\n",
      "it: 113/174, loss: 0.2517, time: 19.8931\n",
      "it: 114/174, loss: 0.2514, time: 19.2204\n",
      "it: 115/174, loss: 0.2508, time: 20.5794\n",
      "it: 116/174, loss: 0.2515, time: 19.9025\n",
      "it: 117/174, loss: 0.2510, time: 19.1568\n",
      "it: 118/174, loss: 0.2503, time: 20.6280\n",
      "it: 119/174, loss: 0.2503, time: 19.0382\n",
      "it: 120/174, loss: 0.2500, time: 20.5381\n",
      "it: 121/174, loss: 0.2507, time: 20.2817\n",
      "it: 122/174, loss: 0.2506, time: 20.2656\n",
      "it: 123/174, loss: 0.2500, time: 20.1249\n",
      "it: 124/174, loss: 0.2497, time: 19.3884\n",
      "it: 125/174, loss: 0.2494, time: 20.6747\n",
      "it: 126/174, loss: 0.2499, time: 20.1878\n",
      "it: 127/174, loss: 0.2511, time: 19.2682\n",
      "it: 128/174, loss: 0.2504, time: 20.3679\n",
      "it: 129/174, loss: 0.2500, time: 19.6946\n",
      "it: 130/174, loss: 0.2502, time: 19.1483\n",
      "it: 131/174, loss: 0.2500, time: 20.5170\n",
      "it: 132/174, loss: 0.2495, time: 19.2436\n",
      "it: 133/174, loss: 0.2492, time: 20.5364\n",
      "it: 134/174, loss: 0.2495, time: 19.9258\n",
      "it: 135/174, loss: 0.2486, time: 20.0059\n",
      "it: 136/174, loss: 0.2480, time: 19.8534\n",
      "it: 137/174, loss: 0.2477, time: 19.0838\n",
      "it: 138/174, loss: 0.2472, time: 20.6671\n",
      "it: 139/174, loss: 0.2472, time: 19.8201\n",
      "it: 140/174, loss: 0.2474, time: 19.3868\n",
      "it: 141/174, loss: 0.2472, time: 20.5815\n",
      "it: 142/174, loss: 0.2470, time: 19.9018\n",
      "it: 143/174, loss: 0.2465, time: 19.3018\n",
      "it: 144/174, loss: 0.2455, time: 20.5791\n",
      "it: 145/174, loss: 0.2455, time: 19.0807\n",
      "it: 146/174, loss: 0.2451, time: 20.3328\n",
      "it: 147/174, loss: 0.2449, time: 19.6530\n",
      "it: 148/174, loss: 0.2453, time: 21.0964\n",
      "it: 149/174, loss: 0.2457, time: 21.1538\n",
      "it: 150/174, loss: 0.2457, time: 20.1888\n",
      "it: 151/174, loss: 0.2456, time: 20.6970\n",
      "it: 152/174, loss: 0.2462, time: 19.8186\n",
      "it: 153/174, loss: 0.2461, time: 19.3415\n",
      "it: 154/174, loss: 0.2473, time: 20.4759\n",
      "it: 155/174, loss: 0.2463, time: 19.5927\n",
      "it: 156/174, loss: 0.2462, time: 19.1407\n",
      "it: 157/174, loss: 0.2460, time: 20.7585\n",
      "it: 158/174, loss: 0.2454, time: 19.3789\n",
      "it: 159/174, loss: 0.2446, time: 20.8402\n",
      "it: 160/174, loss: 0.2438, time: 19.9217\n",
      "it: 161/174, loss: 0.2440, time: 20.1165\n",
      "it: 162/174, loss: 0.2438, time: 20.3250\n",
      "it: 163/174, loss: 0.2452, time: 19.1790\n",
      "it: 164/174, loss: 0.2451, time: 20.4878\n",
      "it: 165/174, loss: 0.2449, time: 19.6706\n",
      "it: 166/174, loss: 0.2451, time: 19.4586\n",
      "it: 167/174, loss: 0.2444, time: 20.7937\n",
      "it: 168/174, loss: 0.2440, time: 20.0343\n",
      "it: 169/174, loss: 0.2437, time: 20.0378\n",
      "it: 170/174, loss: 0.2447, time: 19.9922\n",
      "it: 171/174, loss: 0.2446, time: 19.8641\n",
      "it: 172/174, loss: 0.2447, time: 19.9109\n",
      "it: 173/174, loss: 0.2445, time: 20.7788\n",
      "it: 174/174, loss: 0.2446, time: 21.3054\n",
      "it: 1/174, loss: 0.4024, time: 19.1854\n",
      "it: 2/174, loss: 0.3471, time: 19.4241\n",
      "it: 3/174, loss: 0.3874, time: 19.3440\n",
      "it: 4/174, loss: 0.3345, time: 19.4832\n",
      "it: 5/174, loss: 0.2996, time: 19.4731\n",
      "it: 6/174, loss: 0.3034, time: 19.5071\n",
      "it: 7/174, loss: 0.2858, time: 19.3490\n",
      "it: 8/174, loss: 0.2711, time: 19.2224\n",
      "it: 9/174, loss: 0.2514, time: 19.3841\n",
      "it: 10/174, loss: 0.2429, time: 19.4432\n",
      "it: 11/174, loss: 0.2658, time: 19.5809\n",
      "it: 12/174, loss: 0.2677, time: 19.6634\n",
      "it: 13/174, loss: 0.2612, time: 19.5250\n",
      "it: 14/174, loss: 0.2568, time: 19.3401\n",
      "it: 15/174, loss: 0.2550, time: 19.5055\n",
      "it: 16/174, loss: 0.2510, time: 19.4672\n",
      "it: 17/174, loss: 0.2443, time: 19.5485\n",
      "it: 18/174, loss: 0.2446, time: 19.6203\n",
      "it: 19/174, loss: 0.2482, time: 19.4811\n",
      "it: 20/174, loss: 0.2431, time: 19.6668\n",
      "it: 21/174, loss: 0.2484, time: 19.6870\n",
      "it: 22/174, loss: 0.2422, time: 19.6890\n",
      "it: 23/174, loss: 0.2406, time: 19.7914\n",
      "it: 24/174, loss: 0.2452, time: 19.4101\n",
      "it: 25/174, loss: 0.2409, time: 19.4322\n",
      "it: 26/174, loss: 0.2446, time: 19.3705\n",
      "it: 27/174, loss: 0.2456, time: 19.6396\n",
      "it: 28/174, loss: 0.2432, time: 19.4928\n",
      "it: 29/174, loss: 0.2396, time: 19.6978\n",
      "it: 30/174, loss: 0.2392, time: 19.5416\n",
      "it: 31/174, loss: 0.2430, time: 19.6880\n",
      "it: 32/174, loss: 0.2467, time: 19.3583\n",
      "it: 33/174, loss: 0.2476, time: 19.3806\n",
      "it: 34/174, loss: 0.2475, time: 19.5765\n",
      "it: 35/174, loss: 0.2468, time: 20.1641\n",
      "it: 36/174, loss: 0.2441, time: 18.9461\n",
      "it: 37/174, loss: 0.2419, time: 20.3626\n",
      "it: 38/174, loss: 0.2404, time: 19.6916\n",
      "it: 39/174, loss: 0.2403, time: 19.0612\n",
      "it: 40/174, loss: 0.2394, time: 20.2926\n",
      "it: 41/174, loss: 0.2377, time: 19.5662\n",
      "it: 42/174, loss: 0.2369, time: 18.8966\n",
      "it: 43/174, loss: 0.2366, time: 20.1867\n",
      "it: 44/174, loss: 0.2329, time: 18.6715\n",
      "it: 45/174, loss: 0.2335, time: 20.1789\n",
      "it: 46/174, loss: 0.2310, time: 19.4447\n",
      "it: 47/174, loss: 0.2328, time: 20.0103\n",
      "it: 48/174, loss: 0.2303, time: 19.7931\n",
      "it: 49/174, loss: 0.2295, time: 18.8479\n",
      "it: 50/174, loss: 0.2295, time: 20.1862\n",
      "it: 51/174, loss: 0.2302, time: 19.5124\n",
      "it: 52/174, loss: 0.2288, time: 18.9424\n",
      "it: 53/174, loss: 0.2285, time: 20.4469\n",
      "it: 54/174, loss: 0.2260, time: 19.4707\n",
      "it: 55/174, loss: 0.2245, time: 19.0582\n",
      "it: 56/174, loss: 0.2236, time: 20.3298\n",
      "it: 57/174, loss: 0.2236, time: 19.0663\n",
      "it: 58/174, loss: 0.2221, time: 20.3274\n",
      "it: 59/174, loss: 0.2230, time: 19.6637\n",
      "it: 60/174, loss: 0.2232, time: 19.6256\n",
      "it: 61/174, loss: 0.2226, time: 19.7841\n",
      "it: 62/174, loss: 0.2228, time: 18.8086\n",
      "it: 63/174, loss: 0.2222, time: 20.2784\n",
      "it: 64/174, loss: 0.2206, time: 19.6637\n",
      "it: 65/174, loss: 0.2204, time: 19.5881\n",
      "it: 66/174, loss: 0.2200, time: 19.7502\n",
      "it: 67/174, loss: 0.2191, time: 19.6860\n",
      "it: 68/174, loss: 0.2186, time: 19.4338\n",
      "it: 69/174, loss: 0.2184, time: 19.3943\n",
      "it: 70/174, loss: 0.2198, time: 19.6222\n",
      "it: 71/174, loss: 0.2186, time: 19.6527\n",
      "it: 72/174, loss: 0.2179, time: 19.6390\n",
      "it: 73/174, loss: 0.2169, time: 19.8618\n",
      "it: 74/174, loss: 0.2177, time: 19.5043\n",
      "it: 75/174, loss: 0.2183, time: 19.7047\n",
      "it: 76/174, loss: 0.2207, time: 19.5934\n",
      "it: 77/174, loss: 0.2212, time: 19.6456\n",
      "it: 78/174, loss: 0.2212, time: 19.4386\n",
      "it: 79/174, loss: 0.2201, time: 19.6151\n",
      "it: 80/174, loss: 0.2207, time: 19.4908\n",
      "it: 81/174, loss: 0.2196, time: 19.3763\n",
      "it: 82/174, loss: 0.2198, time: 19.6767\n",
      "it: 83/174, loss: 0.2189, time: 19.5269\n",
      "it: 84/174, loss: 0.2187, time: 19.7295\n",
      "it: 85/174, loss: 0.2191, time: 19.6464\n",
      "it: 86/174, loss: 0.2194, time: 19.5860\n",
      "it: 87/174, loss: 0.2223, time: 19.5569\n",
      "it: 88/174, loss: 0.2218, time: 19.3762\n",
      "it: 89/174, loss: 0.2206, time: 19.6109\n",
      "it: 90/174, loss: 0.2216, time: 19.5559\n",
      "it: 91/174, loss: 0.2220, time: 19.6138\n",
      "it: 92/174, loss: 0.2221, time: 19.7442\n",
      "it: 93/174, loss: 0.2226, time: 20.0550\n",
      "it: 94/174, loss: 0.2223, time: 19.7116\n",
      "it: 95/174, loss: 0.2221, time: 19.7752\n",
      "it: 96/174, loss: 0.2206, time: 19.6398\n",
      "it: 97/174, loss: 0.2206, time: 19.3206\n",
      "it: 98/174, loss: 0.2207, time: 19.7802\n",
      "it: 99/174, loss: 0.2207, time: 19.5549\n",
      "it: 100/174, loss: 0.2194, time: 19.7233\n",
      "it: 101/174, loss: 0.2208, time: 20.4035\n",
      "it: 102/174, loss: 0.2205, time: 19.6357\n",
      "it: 103/174, loss: 0.2193, time: 19.6311\n",
      "it: 104/174, loss: 0.2183, time: 19.6450\n",
      "it: 105/174, loss: 0.2179, time: 18.8522\n",
      "it: 106/174, loss: 0.2177, time: 20.2872\n",
      "it: 107/174, loss: 0.2169, time: 19.6725\n",
      "it: 108/174, loss: 0.2166, time: 18.9712\n",
      "it: 109/174, loss: 0.2150, time: 20.4492\n",
      "it: 110/174, loss: 0.2147, time: 19.5786\n",
      "it: 111/174, loss: 0.2146, time: 19.0878\n",
      "it: 112/174, loss: 0.2137, time: 20.4702\n",
      "it: 113/174, loss: 0.2147, time: 18.8697\n",
      "it: 114/174, loss: 0.2157, time: 20.3262\n",
      "it: 115/174, loss: 0.2166, time: 19.5565\n",
      "it: 116/174, loss: 0.2160, time: 19.7300\n",
      "it: 117/174, loss: 0.2154, time: 19.8374\n",
      "it: 118/174, loss: 0.2147, time: 19.8520\n",
      "it: 119/174, loss: 0.2150, time: 19.7451\n",
      "it: 120/174, loss: 0.2165, time: 19.6645\n",
      "it: 121/174, loss: 0.2178, time: 19.7648\n",
      "it: 122/174, loss: 0.2179, time: 19.5795\n",
      "it: 123/174, loss: 0.2177, time: 19.6363\n",
      "it: 124/174, loss: 0.2179, time: 19.7468\n",
      "it: 125/174, loss: 0.2183, time: 19.8254\n",
      "it: 126/174, loss: 0.2184, time: 19.9536\n",
      "it: 127/174, loss: 0.2180, time: 19.7572\n",
      "it: 128/174, loss: 0.2195, time: 19.7478\n",
      "it: 129/174, loss: 0.2186, time: 19.7740\n",
      "it: 130/174, loss: 0.2202, time: 19.9019\n",
      "it: 131/174, loss: 0.2213, time: 19.7357\n",
      "it: 132/174, loss: 0.2211, time: 19.7461\n",
      "it: 133/174, loss: 0.2209, time: 19.6137\n",
      "it: 134/174, loss: 0.2214, time: 19.5453\n",
      "it: 135/174, loss: 0.2210, time: 19.5235\n",
      "it: 136/174, loss: 0.2204, time: 19.6293\n",
      "it: 137/174, loss: 0.2205, time: 19.7868\n",
      "it: 138/174, loss: 0.2203, time: 19.8944\n",
      "it: 139/174, loss: 0.2205, time: 19.8062\n",
      "it: 140/174, loss: 0.2197, time: 19.6390\n",
      "it: 141/174, loss: 0.2197, time: 19.6600\n",
      "it: 142/174, loss: 0.2216, time: 19.6051\n",
      "it: 143/174, loss: 0.2225, time: 19.7495\n",
      "it: 144/174, loss: 0.2231, time: 19.8455\n",
      "it: 145/174, loss: 0.2231, time: 19.7320\n",
      "it: 146/174, loss: 0.2235, time: 19.7548\n",
      "it: 147/174, loss: 0.2237, time: 19.6060\n",
      "it: 148/174, loss: 0.2245, time: 19.7284\n",
      "it: 149/174, loss: 0.2241, time: 19.8237\n",
      "it: 150/174, loss: 0.2241, time: 19.5719\n",
      "it: 151/174, loss: 0.2235, time: 19.5677\n",
      "it: 152/174, loss: 0.2234, time: 19.5810\n",
      "it: 153/174, loss: 0.2235, time: 19.8463\n",
      "it: 154/174, loss: 0.2236, time: 19.6584\n",
      "it: 155/174, loss: 0.2235, time: 19.7734\n",
      "it: 156/174, loss: 0.2240, time: 19.7270\n",
      "it: 157/174, loss: 0.2240, time: 19.6956\n",
      "it: 158/174, loss: 0.2236, time: 19.6853\n",
      "it: 159/174, loss: 0.2235, time: 19.5828\n",
      "it: 160/174, loss: 0.2236, time: 19.6190\n",
      "it: 161/174, loss: 0.2236, time: 19.5874\n",
      "it: 162/174, loss: 0.2243, time: 19.8403\n",
      "it: 163/174, loss: 0.2238, time: 19.7240\n",
      "it: 164/174, loss: 0.2251, time: 19.8180\n",
      "it: 165/174, loss: 0.2253, time: 19.6740\n",
      "it: 166/174, loss: 0.2253, time: 19.6932\n",
      "it: 167/174, loss: 0.2257, time: 19.8453\n",
      "it: 168/174, loss: 0.2257, time: 19.6148\n",
      "it: 169/174, loss: 0.2256, time: 19.7366\n",
      "it: 170/174, loss: 0.2267, time: 19.6089\n",
      "it: 171/174, loss: 0.2267, time: 19.7318\n",
      "it: 172/174, loss: 0.2292, time: 19.6564\n",
      "it: 173/174, loss: 0.2295, time: 19.7698\n",
      "it: 174/174, loss: 0.2296, time: 19.8563\n",
      "it: 1/174, loss: 0.4969, time: 18.8512\n",
      "it: 2/174, loss: 0.4221, time: 19.1960\n",
      "it: 3/174, loss: 0.3922, time: 19.2312\n",
      "it: 4/174, loss: 0.3393, time: 19.3304\n",
      "it: 5/174, loss: 0.2949, time: 19.3758\n",
      "it: 6/174, loss: 0.2767, time: 19.5015\n",
      "it: 7/174, loss: 0.2677, time: 19.3784\n",
      "it: 8/174, loss: 0.2466, time: 19.5893\n",
      "it: 9/174, loss: 0.2305, time: 19.6030\n",
      "it: 10/174, loss: 0.2355, time: 19.5700\n",
      "it: 11/174, loss: 0.2280, time: 19.4386\n",
      "it: 12/174, loss: 0.2246, time: 19.2901\n",
      "it: 13/174, loss: 0.2405, time: 19.3682\n",
      "it: 14/174, loss: 0.2304, time: 19.2960\n",
      "it: 15/174, loss: 0.2324, time: 19.5615\n",
      "it: 16/174, loss: 0.2322, time: 19.5246\n",
      "it: 17/174, loss: 0.2299, time: 19.4279\n",
      "it: 18/174, loss: 0.2246, time: 19.5555\n",
      "it: 19/174, loss: 0.2276, time: 19.6567\n",
      "it: 20/174, loss: 0.2238, time: 19.4917\n",
      "it: 21/174, loss: 0.2218, time: 19.3809\n",
      "it: 22/174, loss: 0.2285, time: 19.5321\n",
      "it: 23/174, loss: 0.2324, time: 19.5977\n",
      "it: 24/174, loss: 0.2311, time: 19.6375\n",
      "it: 25/174, loss: 0.2334, time: 19.9354\n",
      "it: 26/174, loss: 0.2288, time: 19.6075\n",
      "it: 27/174, loss: 0.2278, time: 19.6301\n",
      "it: 28/174, loss: 0.2245, time: 19.5809\n",
      "it: 29/174, loss: 0.2192, time: 19.5434\n",
      "it: 30/174, loss: 0.2187, time: 19.4269\n",
      "it: 31/174, loss: 0.2185, time: 19.4411\n",
      "it: 32/174, loss: 0.2183, time: 19.3625\n",
      "it: 33/174, loss: 0.2196, time: 19.4332\n",
      "it: 34/174, loss: 0.2161, time: 19.7400\n",
      "it: 35/174, loss: 0.2119, time: 19.7243\n",
      "it: 36/174, loss: 0.2145, time: 19.7530\n",
      "it: 37/174, loss: 0.2158, time: 19.5357\n",
      "it: 38/174, loss: 0.2154, time: 19.6401\n",
      "it: 39/174, loss: 0.2149, time: 19.3625\n",
      "it: 40/174, loss: 0.2192, time: 19.4892\n",
      "it: 41/174, loss: 0.2158, time: 19.6897\n",
      "it: 42/174, loss: 0.2149, time: 19.6182\n",
      "it: 43/174, loss: 0.2141, time: 19.6081\n",
      "it: 44/174, loss: 0.2166, time: 19.6026\n",
      "it: 45/174, loss: 0.2157, time: 19.7948\n",
      "it: 46/174, loss: 0.2144, time: 19.7012\n",
      "it: 47/174, loss: 0.2141, time: 19.6196\n",
      "it: 48/174, loss: 0.2143, time: 19.5304\n",
      "it: 49/174, loss: 0.2166, time: 19.4349\n",
      "it: 50/174, loss: 0.2168, time: 19.5537\n",
      "it: 51/174, loss: 0.2157, time: 19.4980\n",
      "it: 52/174, loss: 0.2163, time: 19.7625\n",
      "it: 53/174, loss: 0.2204, time: 19.7930\n",
      "it: 54/174, loss: 0.2201, time: 19.8094\n",
      "it: 55/174, loss: 0.2188, time: 19.5641\n",
      "it: 56/174, loss: 0.2194, time: 19.6748\n",
      "it: 57/174, loss: 0.2276, time: 19.6111\n",
      "it: 58/174, loss: 0.2290, time: 19.7128\n",
      "it: 59/174, loss: 0.2292, time: 19.7268\n",
      "it: 60/174, loss: 0.2303, time: 19.6650\n",
      "it: 61/174, loss: 0.2295, time: 19.7669\n",
      "it: 62/174, loss: 0.2333, time: 19.5118\n",
      "it: 63/174, loss: 0.2326, time: 19.6040\n",
      "it: 64/174, loss: 0.2330, time: 20.3574\n",
      "it: 65/174, loss: 0.2352, time: 19.7038\n",
      "it: 66/174, loss: 0.2346, time: 19.6468\n",
      "it: 67/174, loss: 0.2341, time: 19.4429\n",
      "it: 68/174, loss: 0.2343, time: 19.6146\n",
      "it: 69/174, loss: 0.2352, time: 19.5125\n",
      "it: 70/174, loss: 0.2339, time: 19.7934\n",
      "it: 71/174, loss: 0.2335, time: 20.0000\n",
      "it: 72/174, loss: 0.2349, time: 19.5702\n",
      "it: 73/174, loss: 0.2346, time: 19.5736\n",
      "it: 74/174, loss: 0.2360, time: 19.3768\n",
      "it: 75/174, loss: 0.2351, time: 19.6509\n",
      "it: 76/174, loss: 0.2343, time: 19.5948\n",
      "it: 77/174, loss: 0.2354, time: 19.7542\n",
      "it: 78/174, loss: 0.2344, time: 19.7057\n",
      "it: 79/174, loss: 0.2349, time: 19.7028\n",
      "it: 80/174, loss: 0.2329, time: 19.8403\n",
      "it: 81/174, loss: 0.2321, time: 19.6998\n",
      "it: 82/174, loss: 0.2312, time: 19.7568\n",
      "it: 83/174, loss: 0.2314, time: 19.5792\n",
      "it: 84/174, loss: 0.2317, time: 19.5353\n",
      "it: 85/174, loss: 0.2306, time: 19.4360\n",
      "it: 86/174, loss: 0.2316, time: 19.5939\n",
      "it: 87/174, loss: 0.2308, time: 19.6769\n",
      "it: 88/174, loss: 0.2297, time: 19.7572\n",
      "it: 89/174, loss: 0.2296, time: 19.8634\n",
      "it: 90/174, loss: 0.2278, time: 19.6979\n",
      "it: 91/174, loss: 0.2295, time: 19.6521\n",
      "it: 92/174, loss: 0.2290, time: 19.5553\n",
      "it: 93/174, loss: 0.2292, time: 19.5321\n",
      "it: 94/174, loss: 0.2287, time: 19.7099\n",
      "it: 95/174, loss: 0.2276, time: 19.8254\n",
      "it: 96/174, loss: 0.2293, time: 19.8675\n",
      "it: 97/174, loss: 0.2289, time: 19.6266\n",
      "it: 98/174, loss: 0.2278, time: 19.7368\n",
      "it: 99/174, loss: 0.2267, time: 19.7137\n",
      "it: 100/174, loss: 0.2268, time: 19.7462\n",
      "it: 101/174, loss: 0.2257, time: 19.6963\n",
      "it: 102/174, loss: 0.2264, time: 19.4847\n",
      "it: 103/174, loss: 0.2253, time: 19.6200\n",
      "it: 104/174, loss: 0.2257, time: 19.4889\n",
      "it: 105/174, loss: 0.2259, time: 19.7150\n",
      "it: 106/174, loss: 0.2264, time: 19.7601\n",
      "it: 107/174, loss: 0.2251, time: 19.8782\n",
      "it: 108/174, loss: 0.2251, time: 19.9161\n",
      "it: 109/174, loss: 0.2248, time: 19.5703\n",
      "it: 110/174, loss: 0.2255, time: 19.5114\n",
      "it: 111/174, loss: 0.2243, time: 19.8828\n",
      "it: 112/174, loss: 0.2240, time: 19.7474\n",
      "it: 113/174, loss: 0.2232, time: 19.6267\n",
      "it: 114/174, loss: 0.2227, time: 19.8686\n",
      "it: 115/174, loss: 0.2224, time: 19.7258\n",
      "it: 116/174, loss: 0.2227, time: 19.8710\n",
      "it: 117/174, loss: 0.2222, time: 19.7666\n",
      "it: 118/174, loss: 0.2221, time: 19.8100\n",
      "it: 119/174, loss: 0.2226, time: 19.7403\n",
      "it: 120/174, loss: 0.2222, time: 19.5799\n",
      "it: 121/174, loss: 0.2223, time: 19.7245\n",
      "it: 122/174, loss: 0.2228, time: 19.7438\n",
      "it: 123/174, loss: 0.2220, time: 19.8503\n",
      "it: 124/174, loss: 0.2220, time: 19.8903\n",
      "it: 125/174, loss: 0.2218, time: 19.8653\n",
      "it: 126/174, loss: 0.2208, time: 19.8916\n",
      "it: 127/174, loss: 0.2201, time: 19.8079\n",
      "it: 128/174, loss: 0.2219, time: 19.7547\n",
      "it: 129/174, loss: 0.2216, time: 19.6756\n",
      "it: 130/174, loss: 0.2219, time: 19.9748\n",
      "it: 131/174, loss: 0.2231, time: 20.0410\n",
      "it: 132/174, loss: 0.2223, time: 19.8355\n",
      "it: 133/174, loss: 0.2214, time: 19.9611\n",
      "it: 134/174, loss: 0.2213, time: 19.9189\n",
      "it: 135/174, loss: 0.2209, time: 19.8363\n",
      "it: 136/174, loss: 0.2218, time: 19.8911\n",
      "it: 137/174, loss: 0.2208, time: 19.7518\n",
      "it: 138/174, loss: 0.2209, time: 19.7339\n",
      "it: 139/174, loss: 0.2216, time: 19.6995\n",
      "it: 140/174, loss: 0.2217, time: 19.8215\n",
      "it: 141/174, loss: 0.2220, time: 19.7871\n",
      "it: 142/174, loss: 0.2226, time: 19.8669\n",
      "it: 143/174, loss: 0.2225, time: 19.9386\n",
      "it: 144/174, loss: 0.2233, time: 19.8258\n",
      "it: 145/174, loss: 0.2235, time: 19.6242\n",
      "it: 146/174, loss: 0.2235, time: 19.6584\n",
      "it: 147/174, loss: 0.2240, time: 19.6959\n",
      "it: 148/174, loss: 0.2242, time: 19.6602\n",
      "it: 149/174, loss: 0.2238, time: 19.9319\n",
      "it: 150/174, loss: 0.2240, time: 19.9617\n",
      "it: 151/174, loss: 0.2246, time: 19.7656\n",
      "it: 152/174, loss: 0.2239, time: 19.8798\n",
      "it: 153/174, loss: 0.2250, time: 19.7781\n",
      "it: 154/174, loss: 0.2260, time: 19.8007\n",
      "it: 155/174, loss: 0.2261, time: 19.7172\n",
      "it: 156/174, loss: 0.2254, time: 19.9215\n",
      "it: 157/174, loss: 0.2247, time: 20.5626\n",
      "it: 158/174, loss: 0.2238, time: 19.8240\n",
      "it: 159/174, loss: 0.2243, time: 19.9070\n",
      "it: 160/174, loss: 0.2238, time: 19.8849\n",
      "it: 161/174, loss: 0.2244, time: 19.9859\n",
      "it: 162/174, loss: 0.2248, time: 19.7871\n",
      "it: 163/174, loss: 0.2240, time: 19.8011\n",
      "it: 164/174, loss: 0.2240, time: 19.6604\n",
      "it: 165/174, loss: 0.2239, time: 20.2529\n",
      "it: 166/174, loss: 0.2246, time: 20.1570\n",
      "it: 167/174, loss: 0.2239, time: 20.0019\n",
      "it: 168/174, loss: 0.2232, time: 20.1147\n",
      "it: 169/174, loss: 0.2227, time: 20.0074\n",
      "it: 170/174, loss: 0.2227, time: 20.0187\n",
      "it: 171/174, loss: 0.2228, time: 20.0480\n",
      "it: 172/174, loss: 0.2224, time: 19.9775\n",
      "it: 173/174, loss: 0.2217, time: 20.0165\n",
      "it: 174/174, loss: 0.2212, time: 19.9334\n",
      "it: 1/174, loss: 0.1889, time: 19.5055\n",
      "it: 2/174, loss: 0.2047, time: 19.8786\n",
      "it: 3/174, loss: 0.2678, time: 19.1129\n",
      "it: 4/174, loss: 0.2226, time: 19.6678\n",
      "it: 5/174, loss: 0.2547, time: 19.7371\n",
      "it: 6/174, loss: 0.2399, time: 19.8209\n",
      "it: 7/174, loss: 0.2230, time: 19.7794\n",
      "it: 8/174, loss: 0.2284, time: 19.6390\n",
      "it: 9/174, loss: 0.2290, time: 19.7778\n",
      "it: 10/174, loss: 0.2242, time: 19.8561\n",
      "it: 11/174, loss: 0.2202, time: 19.6972\n",
      "it: 12/174, loss: 0.2228, time: 19.9080\n",
      "it: 13/174, loss: 0.2186, time: 19.7778\n",
      "it: 14/174, loss: 0.2321, time: 19.9008\n",
      "it: 15/174, loss: 0.2342, time: 19.8024\n",
      "it: 16/174, loss: 0.2332, time: 19.9051\n",
      "it: 17/174, loss: 0.2334, time: 20.0450\n",
      "it: 18/174, loss: 0.2304, time: 19.8251\n",
      "it: 19/174, loss: 0.2265, time: 19.9501\n",
      "it: 20/174, loss: 0.2238, time: 19.9567\n",
      "it: 21/174, loss: 0.2264, time: 19.9982\n",
      "it: 22/174, loss: 0.2222, time: 19.8541\n",
      "it: 23/174, loss: 0.2190, time: 19.9311\n",
      "it: 24/174, loss: 0.2236, time: 19.9801\n",
      "it: 25/174, loss: 0.2240, time: 19.9050\n",
      "it: 26/174, loss: 0.2195, time: 19.9001\n",
      "it: 27/174, loss: 0.2165, time: 20.1078\n",
      "it: 28/174, loss: 0.2152, time: 20.0053\n",
      "it: 29/174, loss: 0.2112, time: 19.9396\n",
      "it: 30/174, loss: 0.2115, time: 20.1900\n",
      "it: 31/174, loss: 0.2095, time: 20.0272\n",
      "it: 32/174, loss: 0.2113, time: 20.0152\n",
      "it: 33/174, loss: 0.2096, time: 19.9749\n",
      "it: 34/174, loss: 0.2102, time: 20.1262\n",
      "it: 35/174, loss: 0.2075, time: 20.1091\n",
      "it: 36/174, loss: 0.2070, time: 20.1183\n",
      "it: 37/174, loss: 0.2058, time: 20.1049\n",
      "it: 38/174, loss: 0.2020, time: 19.9928\n",
      "it: 39/174, loss: 0.2007, time: 20.0924\n",
      "it: 40/174, loss: 0.1980, time: 19.9635\n",
      "it: 41/174, loss: 0.1942, time: 20.4843\n",
      "it: 42/174, loss: 0.1952, time: 20.3552\n",
      "it: 43/174, loss: 0.1937, time: 20.2285\n",
      "it: 44/174, loss: 0.1931, time: 19.9814\n",
      "it: 45/174, loss: 0.1963, time: 20.1424\n",
      "it: 46/174, loss: 0.1992, time: 20.2541\n",
      "it: 47/174, loss: 0.2003, time: 20.0785\n",
      "it: 48/174, loss: 0.1997, time: 20.1212\n",
      "it: 49/174, loss: 0.2014, time: 20.1366\n",
      "it: 50/174, loss: 0.1987, time: 20.1712\n",
      "it: 51/174, loss: 0.2001, time: 20.0312\n",
      "it: 52/174, loss: 0.2006, time: 20.1413\n",
      "it: 53/174, loss: 0.1999, time: 20.1434\n",
      "it: 54/174, loss: 0.2039, time: 20.2753\n",
      "it: 55/174, loss: 0.2028, time: 20.1243\n",
      "it: 56/174, loss: 0.2026, time: 20.0205\n",
      "it: 57/174, loss: 0.2063, time: 20.1565\n",
      "it: 58/174, loss: 0.2045, time: 20.1271\n",
      "it: 59/174, loss: 0.2044, time: 20.0385\n",
      "it: 60/174, loss: 0.2041, time: 20.1103\n",
      "it: 61/174, loss: 0.2040, time: 20.2510\n",
      "it: 62/174, loss: 0.2068, time: 20.1407\n",
      "it: 63/174, loss: 0.2072, time: 20.0076\n",
      "it: 64/174, loss: 0.2067, time: 20.3392\n",
      "it: 65/174, loss: 0.2062, time: 20.5047\n",
      "it: 66/174, loss: 0.2084, time: 20.0779\n",
      "it: 67/174, loss: 0.2101, time: 20.0951\n",
      "it: 68/174, loss: 0.2127, time: 20.0693\n",
      "it: 69/174, loss: 0.2126, time: 20.0907\n",
      "it: 70/174, loss: 0.2208, time: 20.1969\n",
      "it: 71/174, loss: 0.2199, time: 20.5703\n",
      "it: 72/174, loss: 0.2192, time: 20.0850\n",
      "it: 73/174, loss: 0.2193, time: 20.0476\n",
      "it: 74/174, loss: 0.2207, time: 20.0429\n",
      "it: 75/174, loss: 0.2195, time: 20.1854\n",
      "it: 76/174, loss: 0.2203, time: 20.0908\n",
      "it: 77/174, loss: 0.2206, time: 20.1360\n",
      "it: 78/174, loss: 0.2213, time: 20.0106\n",
      "it: 79/174, loss: 0.2215, time: 20.2738\n",
      "it: 80/174, loss: 0.2200, time: 20.2498\n",
      "it: 81/174, loss: 0.2194, time: 20.2801\n",
      "it: 82/174, loss: 0.2189, time: 20.0779\n",
      "it: 83/174, loss: 0.2190, time: 19.9435\n",
      "it: 84/174, loss: 0.2190, time: 20.1648\n",
      "it: 85/174, loss: 0.2197, time: 20.0811\n",
      "it: 86/174, loss: 0.2238, time: 20.0410\n",
      "it: 87/174, loss: 0.2231, time: 20.0136\n",
      "it: 88/174, loss: 0.2227, time: 20.0283\n",
      "it: 89/174, loss: 0.2235, time: 20.2581\n",
      "it: 90/174, loss: 0.2235, time: 20.0647\n",
      "it: 91/174, loss: 0.2241, time: 20.2430\n",
      "it: 92/174, loss: 0.2227, time: 20.0364\n",
      "it: 93/174, loss: 0.2230, time: 20.1400\n",
      "it: 94/174, loss: 0.2240, time: 20.2790\n",
      "it: 95/174, loss: 0.2228, time: 20.5039\n",
      "it: 96/174, loss: 0.2252, time: 20.2707\n",
      "it: 97/174, loss: 0.2248, time: 20.1098\n",
      "it: 98/174, loss: 0.2263, time: 20.1976\n",
      "it: 99/174, loss: 0.2268, time: 20.0012\n",
      "it: 100/174, loss: 0.2275, time: 20.2565\n",
      "it: 101/174, loss: 0.2269, time: 20.0685\n",
      "it: 102/174, loss: 0.2273, time: 20.1552\n",
      "it: 103/174, loss: 0.2295, time: 20.2448\n",
      "it: 104/174, loss: 0.2307, time: 20.1119\n",
      "it: 105/174, loss: 0.2291, time: 20.0829\n",
      "it: 106/174, loss: 0.2287, time: 20.1030\n",
      "it: 107/174, loss: 0.2283, time: 20.0808\n",
      "it: 108/174, loss: 0.2277, time: 19.9364\n",
      "it: 109/174, loss: 0.2281, time: 20.2504\n",
      "it: 110/174, loss: 0.2281, time: 20.0441\n",
      "it: 111/174, loss: 0.2272, time: 20.0495\n",
      "it: 112/174, loss: 0.2268, time: 19.9375\n",
      "it: 113/174, loss: 0.2264, time: 20.1072\n",
      "it: 114/174, loss: 0.2261, time: 20.0847\n",
      "it: 115/174, loss: 0.2257, time: 19.9442\n",
      "it: 116/174, loss: 0.2248, time: 20.1290\n",
      "it: 117/174, loss: 0.2238, time: 19.9688\n",
      "it: 118/174, loss: 0.2235, time: 20.1522\n",
      "it: 119/174, loss: 0.2237, time: 19.9982\n",
      "it: 120/174, loss: 0.2234, time: 20.0858\n",
      "it: 121/174, loss: 0.2233, time: 20.1366\n",
      "it: 122/174, loss: 0.2220, time: 20.1282\n",
      "it: 123/174, loss: 0.2216, time: 20.1971\n",
      "it: 124/174, loss: 0.2225, time: 19.9813\n",
      "it: 125/174, loss: 0.2212, time: 20.1737\n",
      "it: 126/174, loss: 0.2215, time: 20.2938\n",
      "it: 127/174, loss: 0.2231, time: 20.0847\n",
      "it: 128/174, loss: 0.2259, time: 19.9726\n",
      "it: 129/174, loss: 0.2259, time: 20.2673\n",
      "it: 130/174, loss: 0.2267, time: 20.3051\n",
      "it: 131/174, loss: 0.2265, time: 19.9398\n",
      "it: 132/174, loss: 0.2265, time: 20.1057\n",
      "it: 133/174, loss: 0.2256, time: 20.1318\n",
      "it: 134/174, loss: 0.2267, time: 19.9540\n",
      "it: 135/174, loss: 0.2268, time: 20.1712\n",
      "it: 136/174, loss: 0.2273, time: 20.0739\n",
      "it: 137/174, loss: 0.2262, time: 20.0584\n",
      "it: 138/174, loss: 0.2271, time: 20.0058\n",
      "it: 139/174, loss: 0.2274, time: 20.3020\n",
      "it: 140/174, loss: 0.2275, time: 20.0930\n",
      "it: 141/174, loss: 0.2289, time: 19.9991\n",
      "it: 142/174, loss: 0.2286, time: 20.1199\n",
      "it: 143/174, loss: 0.2286, time: 20.1844\n",
      "it: 144/174, loss: 0.2279, time: 20.0526\n",
      "it: 145/174, loss: 0.2289, time: 20.1388\n",
      "it: 146/174, loss: 0.2287, time: 20.0875\n",
      "it: 147/174, loss: 0.2304, time: 19.9863\n",
      "it: 148/174, loss: 0.2301, time: 20.1549\n",
      "it: 149/174, loss: 0.2302, time: 19.9979\n",
      "it: 150/174, loss: 0.2302, time: 20.1951\n",
      "it: 151/174, loss: 0.2295, time: 20.1428\n",
      "it: 152/174, loss: 0.2291, time: 20.2281\n",
      "it: 153/174, loss: 0.2294, time: 20.0354\n",
      "it: 154/174, loss: 0.2296, time: 20.3858\n",
      "it: 155/174, loss: 0.2294, time: 20.2150\n",
      "it: 156/174, loss: 0.2307, time: 20.1350\n",
      "it: 157/174, loss: 0.2307, time: 20.0602\n",
      "it: 158/174, loss: 0.2306, time: 20.1235\n",
      "it: 159/174, loss: 0.2314, time: 20.1643\n",
      "it: 160/174, loss: 0.2316, time: 19.9456\n",
      "it: 161/174, loss: 0.2315, time: 20.0854\n",
      "it: 162/174, loss: 0.2322, time: 19.9873\n",
      "it: 163/174, loss: 0.2322, time: 19.9127\n",
      "it: 164/174, loss: 0.2320, time: 19.9990\n",
      "it: 165/174, loss: 0.2313, time: 20.1042\n",
      "it: 166/174, loss: 0.2319, time: 19.9365\n",
      "it: 167/174, loss: 0.2311, time: 20.0328\n",
      "it: 168/174, loss: 0.2308, time: 20.0068\n",
      "it: 169/174, loss: 0.2310, time: 19.9849\n",
      "it: 170/174, loss: 0.2319, time: 20.0441\n",
      "it: 171/174, loss: 0.2327, time: 20.2401\n",
      "it: 172/174, loss: 0.2323, time: 20.1733\n",
      "it: 173/174, loss: 0.2329, time: 20.1087\n",
      "it: 174/174, loss: 0.2329, time: 19.9299\n"
     ]
    }
   ],
   "source": [
    "hq_train_losses, \\\n",
    "    hq_test_losses, \\\n",
    "    hq_train_accuracy_arr, \\\n",
    "    hq_test_accuracy_arr = train_for_hqnn(hybrid_quantum_model, criterion=criterion,\n",
    "                              optimizer=hq_optimizer,\n",
    "                              train_dataloader=train_loader,\n",
    "                              test_dataloader=test_loader,\n",
    "                              num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "Jgjxi_GhTVZd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8306686 , 0.80377907, 0.85755814, 0.85465116, 0.86918605,\n",
       "        0.87427326, 0.87209302, 0.84520349, 0.85828488, 0.85610465], requires_grad=True)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq_test_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "Tsr4BWl-PGF1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.64278017, 0.84015805, 0.85093391, 0.86548132, 0.88649425,\n",
       "        0.89727011, 0.90696839, 0.91325431, 0.91648707, 0.91253592], requires_grad=True)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq_train_accuracy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.65786457, 0.45465172, 0.37524405, 0.33457232, 0.29270859,\n",
       "        0.26755933, 0.24458769, 0.2296296 , 0.22123186, 0.23287987], requires_grad=True)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hq_train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(hybrid_quantum_model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF1KSYMGTdHO"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "in_channels = 1\n",
    "out_channels = 3\n",
    "length = 31\n",
    "num_classes = 5\n",
    "kernel_size = 2\n",
    "embedding_dim = 200\n",
    "hidden_dim = 100\n",
    "padding = 1\n",
    "stride = 1\n",
    "num_qubits = 4\n",
    "\n",
    "# Create dummy data & labels\n",
    "train_dummy_data = torch.randint(0, 1000, (batch_size, length))\n",
    "train_dummy_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "test_dummy_data = torch.randn(batch_size, length)\n",
    "test_dummy_labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "# Create a simple Dataset and DataLoader\n",
    "train_dummy_dataset = TensorDataset(train_dummy_data, train_dummy_labels)\n",
    "train_dummy_loader = DataLoader(train_dummy_dataset, batch_size)\n",
    "\n",
    "test_dummy_dataset = TensorDataset(test_dummy_data, test_dummy_labels)\n",
    "test_dummy_loader = DataLoader(test_dummy_dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZjYDvFJLG0z",
    "outputId": "108bfc04-5bdf-43dc-8322-3659a2112648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToyHQModel(\n",
      "  (embedding): Embedding(37569, 200)\n",
      "  (attention): Attention(\n",
      "    (attn): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (v): Linear(in_features=100, out_features=1, bias=False)\n",
      "  )\n",
      "  (lstm): HQLSTM(\n",
      "    (lstm_cells): ModuleList(\n",
      "      (0): HQLSTMCell()\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "LR = 8e-4 #Learning rate\n",
    "\n",
    "toy_hq_model = ToyHQModel(VOCAB_SIZE, embedding_dim, in_channels, out_channels, kernel_size, stride, padding, hidden_dim, num_qubits, num_classes, lstm_layers = 1)\n",
    "\n",
    "toy_hq_model = toy_hq_model\n",
    "# Set up the criterion (loss function)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(toy_hq_model.parameters(), lr=LR, weight_decay = 5e-6)\n",
    "\n",
    "print(toy_hq_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AH2z4g_A5FjA",
    "outputId": "ca9d8fa0-d80c-4a16-b9b9-f4bb0df18bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n",
      "torch.Size([2, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6381, -1.6118, -1.5608, -1.6813, -1.5606],\n",
       "        [-1.6472, -1.6141, -1.5558, -1.6795, -1.5565]], device='cuda:0',\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_hq_model(train_dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rActVEdigY8M",
    "outputId": "05ad61af-0c0d-4c9a-aeb7-d989abf48ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28985, 31)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cps3yt0-eIKQ"
   },
   "outputs": [],
   "source": [
    "# train_dummy_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "# test_dummy_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18tjNlfVeIKR"
   },
   "outputs": [],
   "source": [
    "# BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQRprRrveIKR"
   },
   "outputs": [],
   "source": [
    "# train_dummy_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_dummy_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "06a8e085c5684549b8564f24bd762e4f",
      "3a33592a8c0f4c3e9e42f2d092dd0889",
      "885eeace639140e2a01b16692b64accc",
      "ca36aa40d1e64ad496b3b838462acaa0",
      "62374ed5a97a431c942ba275585d7d57",
      "6281ff12628a44da968a820165064dd1",
      "a32f46d1b39247d69617fd8d08255dde",
      "e437002cee7c48468d608ac3b6419490",
      "d4d7dc520db647d9bd626ee52f8a35bd",
      "27d5e2469ff444539de5d7d4b4ab6278",
      "875eff2116cf45b9b386071a7d8cc1c7"
     ]
    },
    "id": "RBB6oiTGG02t",
    "outputId": "48e1fb56-cbc4-4c18-9836-1a3a5342eb11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a8e085c5684549b8564f24bd762e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-fd0fa7cb832d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_accuracy_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     test_accuracy_arr = train_for_hqnn(toy_hq_model, criterion=criterion,\n\u001b[0m\u001b[1;32m      5\u001b[0m                               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                               \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dummy_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-0036f823edc8>\u001b[0m in \u001b[0;36mtrain_for_hqnn\u001b[0;34m(model, criterion, optimizer, train_dataloader, test_dataloader, num_epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# t.grad = torch.tensor([0., 0., 0.])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# t = t - lr * t.grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, \\\n",
    "    test_losses, \\\n",
    "    train_accuracy_arr, \\\n",
    "    test_accuracy_arr = train_for_hqnn(toy_hq_model, criterion=criterion,\n",
    "                              optimizer=optimizer,\n",
    "                              train_dataloader=train_dummy_loader,\n",
    "                              test_dataloader=test_dummy_loader,\n",
    "                              num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPCx3xe9zXNy"
   },
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FysnlhgaW4TF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "029421ef2fec489b8b379351e00e188a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06a8e085c5684549b8564f24bd762e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3a33592a8c0f4c3e9e42f2d092dd0889",
       "IPY_MODEL_885eeace639140e2a01b16692b64accc",
       "IPY_MODEL_ca36aa40d1e64ad496b3b838462acaa0"
      ],
      "layout": "IPY_MODEL_62374ed5a97a431c942ba275585d7d57"
     }
    },
    "22607ed241a74d4f9dc9d471f18e1690": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9b9d71402eb46b595cfc84dd522acfb",
      "placeholder": "​",
      "style": "IPY_MODEL_8f43314093db461f8f90e91e48dbd0da",
      "value": "  0%"
     }
    },
    "27d5e2469ff444539de5d7d4b4ab6278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a33592a8c0f4c3e9e42f2d092dd0889": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6281ff12628a44da968a820165064dd1",
      "placeholder": "​",
      "style": "IPY_MODEL_a32f46d1b39247d69617fd8d08255dde",
      "value": "  0%"
     }
    },
    "60ed458c82de4401a7eb400c23f1338d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62374ed5a97a431c942ba275585d7d57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6281ff12628a44da968a820165064dd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "661cd90b76c9451f98e0ca29c4b22d44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d702bf265eca4bbbbcb7a358df381c65",
       "IPY_MODEL_fc30a6a4b6064be0b3c4a786a4b64952",
       "IPY_MODEL_df78c6ab42474534a547a0a7ba44b754"
      ],
      "layout": "IPY_MODEL_d49849d9283546c9b34517f327232757"
     }
    },
    "673f3f8df9244b538ad88f1fc82442b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f2f4512c03d4d6d86d51f2f8a3a5fed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_22607ed241a74d4f9dc9d471f18e1690",
       "IPY_MODEL_d5590ae5cbe246c8a3b97a1794582dec",
       "IPY_MODEL_889ece199b2a469388992df645c9977e"
      ],
      "layout": "IPY_MODEL_97def7d09b1a4359bda9237e1a46d28c"
     }
    },
    "8562c9e4bc8443db82350ff5f496958b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "875eff2116cf45b9b386071a7d8cc1c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "885eeace639140e2a01b16692b64accc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e437002cee7c48468d608ac3b6419490",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4d7dc520db647d9bd626ee52f8a35bd",
      "value": 0
     }
    },
    "889ece199b2a469388992df645c9977e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d788e171ee934b0d9275fcb6c88d69c8",
      "placeholder": "​",
      "style": "IPY_MODEL_8ee7f59198f44ac6af6a747b4d7c505a",
      "value": " 0/1 [00:00&lt;?, ?it/s]"
     }
    },
    "8ee7f59198f44ac6af6a747b4d7c505a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f43314093db461f8f90e91e48dbd0da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97def7d09b1a4359bda9237e1a46d28c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98cad26d7f274b71a04d37fca89196bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a32f46d1b39247d69617fd8d08255dde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9b9d71402eb46b595cfc84dd522acfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6fd02082a4342fbb9b9da6449325cfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca36aa40d1e64ad496b3b838462acaa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27d5e2469ff444539de5d7d4b4ab6278",
      "placeholder": "​",
      "style": "IPY_MODEL_875eff2116cf45b9b386071a7d8cc1c7",
      "value": " 0/2 [30:03&lt;?, ?it/s]"
     }
    },
    "d49849d9283546c9b34517f327232757": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4d7dc520db647d9bd626ee52f8a35bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5590ae5cbe246c8a3b97a1794582dec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d623bf979daa40e8af2598e247099b4e",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6fd02082a4342fbb9b9da6449325cfb",
      "value": 0
     }
    },
    "d623bf979daa40e8af2598e247099b4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d702bf265eca4bbbbcb7a358df381c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8562c9e4bc8443db82350ff5f496958b",
      "placeholder": "​",
      "style": "IPY_MODEL_e17bf6667ccd4167af9110a09ba012ac",
      "value": "  0%"
     }
    },
    "d788e171ee934b0d9275fcb6c88d69c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df78c6ab42474534a547a0a7ba44b754": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_029421ef2fec489b8b379351e00e188a",
      "placeholder": "​",
      "style": "IPY_MODEL_98cad26d7f274b71a04d37fca89196bf",
      "value": " 0/10 [00:03&lt;?, ?it/s]"
     }
    },
    "e17bf6667ccd4167af9110a09ba012ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e437002cee7c48468d608ac3b6419490": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc30a6a4b6064be0b3c4a786a4b64952": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60ed458c82de4401a7eb400c23f1338d",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_673f3f8df9244b538ad88f1fc82442b6",
      "value": 0
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
